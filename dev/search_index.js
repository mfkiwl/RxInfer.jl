var documenterSearchIndex = {"docs":
[{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/#examples-rts-vs-bifm-smoothing","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"","category":"section"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"___Credits to Martin de Quincey___","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"This notebook performs Kalman smoothing on a factor graph using message passing, based on the BIFM Kalman smoother. This notebook is based on:","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"F. Wadehn, “State Space Methods with Applications in Biomedical Signal Processing,” ETH Zurich, 2019. Accessed: Jun. 16, 2021. [Online]. Available: https://www.research-collection.ethz.ch/handle/20.500.11850/344762\nH. Loeliger, L. Bruderer, H. Malmberg, F. Wadehn, and N. Zalmai, “On sparsity by NUV-EM, Gaussian message passing, and Kalman smoothing,” in 2016 Information Theory and Applications Workshop (ITA), Jan. 2016, pp. 1–10. doi: 10.1109/ITA.2016.7888168.","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"We perform Kalman smoothing in the linear state space model, represented by:","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"beginaligned\n    Z_k+1 = A Z_k + B U_k \n    Y_k = C Z_k + W_k\nendaligned","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"with observations Y_k, latent states Z_k and inputs U_k. W_k is the observation noise. A in mathrmR^n times n, B in mathrmR^n times m and C in mathrmR^d times n are the transition matrices in the model. Here n, m and d denote the dimensionality of the latent, input and output dimension, respectively.","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"The corresponding probabilistic model can be represented as ","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"beginaligned\n        p(y z u)\n        = p(z_0) prod_k=1^N p(y_k mid z_k) p(z_kmid z_k-1 u_k-1) p(u_k-1) \n        = mathcalN(z_0 mid mu_z_0 Sigma_z_0) left( prod_k=1^N mathcalN(y_k mid C z_k Sigma_W) delta(z_k - (Az_k-1 + Bu_k-1)) mathcalN(u_k-1 mid mu_i_k-1 Sigma_u_k-1) right)\nendaligned","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/#Import-packages","page":"RTS vs BIFM Smoothing","title":"Import packages","text":"","category":"section"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"using RxInfer, Random, LinearAlgebra, BenchmarkTools, PyPlot, ProgressMeter","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/#Data-generation","page":"RTS vs BIFM Smoothing","title":"Data generation","text":"","category":"section"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"function generate_parameters(dim_out::Int64, dim_in::Int64, dim_lat::Int64; seed::Int64 = 123)\n    \n    # define noise levels\n    input_noise  = 500.0\n    output_noise = 50.0\n\n    # create random generator for reproducibility\n    rng = MersenneTwister(seed)\n\n    # generate matrices, input statistics and noise matrices\n    A      = diagm(0.8 .* ones(dim_lat) .+ 0.2 * rand(rng, dim_lat))                                            # size (dim_lat x dim_lat)\n    B      = rand(dim_lat, dim_in)                                                                              # size (dim_lat x dim_in)\n    C      = rand(dim_out, dim_lat)                                                                             # size (dim_out x dim_lat)\n    μu     = rand(dim_in) .* collect(1:dim_in)                                                                  # size (dim_in x 1)\n    Σu     = input_noise  .* collect(Hermitian(randn(rng, dim_in, dim_in) + diagm(10 .+ 10*rand(dim_in))))      # size (dim_in x dim_in)\n    Σy     = output_noise .* collect(Hermitian(randn(rng, dim_out, dim_out) + diagm(10 .+ 10*rand(dim_out))))   # size (dim_out x dim_out)\n    Wu     = cholinv(Σu)\n    Wy     = cholinv(Σy)\n    \n    # return parameters\n    return A, B, C, μu, Σu, Σy, Wu, Wy\n\nend;","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"function generate_data(nr_samples::Int64, A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, μu::Array{Float64,1}, Σu::Array{Float64,2}, Σy::Array{Float64,2}; seed::Int64 = 123)\n        \n    # create random data generator\n    rng = MersenneTwister(seed)\n    \n    # preallocate space for variables\n    z = Vector{Vector{Float64}}(undef, nr_samples)\n    y = Vector{Vector{Float64}}(undef, nr_samples)\n    u = rand(rng, MvNormal(μu, Σu), nr_samples)'\n    \n    # set initial value of latent states\n    z_prev = zeros(size(A,1))\n    \n    # generate data\n    for i in 1:nr_samples\n\n        # generate new latent state\n        z[i] = A * z_prev + B * u[i,:]\n\n        # generate new observation\n        y[i] = C * z[i] + rand(rng, MvNormal(zeros(dim_out), Σy))\n        \n        # generate new observation\n        z_prev .= z[i]\n        \n    end\n    \n    # return generated data\n    return z, y, u\n    \nend","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"# specify settings\nnr_samples = 200\ndim_out = 3\ndim_in = 3\ndim_lat = 25\n\n# generate parameters\nA, B, C, μu, Σu, Σy, Wu, Wy = generate_parameters(dim_out, dim_in, dim_lat);\n            \n# generate data\ndata_z, data_y, data_u = generate_data(nr_samples, A, B, C, μu, Σu, Σy);\n\n# visualise data\nplt.plot(data_y)\nplt.grid()\nplt.xlabel(\"sample\")\nplt.ylabel(\"observations\")\nplt.gcf()","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"(Image: )","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/#Model-specification","page":"RTS vs BIFM Smoothing","title":"Model specification","text":"","category":"section"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"@model function RTS_smoother(nr_samples::Int64, A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, μu::Array{Float64,1}, Wu::Array{Float64,2}, Wy::Array{Float64,2})\n    \n    # fetch dimensionality\n    dim_lat = size(A, 1)\n    dim_out = size(C, 1)\n    \n    # initialize variables\n    z = randomvar(nr_samples)                 # hidden states (random variable)\n    u = randomvar(nr_samples)                 # inputs (random variable)\n    y = datavar(Vector{Float64}, nr_samples)  # outputs (observed variables)\n    \n    # set initial hidden state\n    z_prior ~ MvNormalMeanPrecision(zeros(dim_lat), 1e-5*diagm(ones(dim_lat)))\n    \n    # update last/previous hidden state\n    z_prev = z_prior\n\n    # loop through observations\n    for i in 1:nr_samples\n\n        # specify input as random variable\n        u[i] ~ MvNormalMeanPrecision(μu, Wu)\n        \n        # specify updated hidden state\n        z[i] ~ A * z_prev + B * u[i]\n        \n        # specify observation\n        y[i] ~ MvNormalMeanPrecision(C * z[i], Wy)\n        \n        # update last/previous hidden state\n        z_prev = z[i]\n\n    end\n    \nend","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"@model function BIFM_smoother(nr_samples::Int64, A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, μu::Array{Float64,1}, Wu::Array{Float64,2}, Wy::Array{Float64,2})\n\n    # fetch dimensionality\n    dim_lat = size(A, 1)\n    \n    # initialize variables\n    z  = randomvar(nr_samples)                  # latent states\n    yt = randomvar(nr_samples)                  # latent observations\n    y  = datavar(Vector{Float64}, nr_samples)   # actual observations\n    u  = randomvar(nr_samples)                  # inputs\n    \n    # set priors\n    z_prior ~ MvNormalMeanPrecision(zeros(dim_lat), 1e-5*diagm(ones(dim_lat)))\n    z_tmp   ~ BIFMHelper(z_prior) where { q = MeanField() }\n    \n    # update last/previous hidden state\n    z_prev = z_tmp\n    \n    # loop through observations\n    for i in 1:nr_samples\n\n        # specify input as random variable\n        u[i]   ~ MvNormalMeanPrecision(μu, Wu)\n\n        # specify observation\n        yt[i]  ~ BIFM(u[i], z_prev, z[i]) where { meta = BIFMMeta(A, B, C) }\n        y[i]   ~ MvNormalMeanPrecision(yt[i], Wy)\n        \n        # update last/previous hidden state\n        z_prev = z[i]\n\n    end\n    \n    # set final value\n    z[nr_samples] ~ MvNormalMeanPrecision(zeros(dim_lat), zeros(dim_lat, dim_lat))\n    \nend","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/#Probabilistic-inference","page":"RTS vs BIFM Smoothing","title":"Probabilistic inference","text":"","category":"section"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"function inference_RTS(data_y, A, B, C, μu, Wu, Wy)\n    result = inference(\n        model      = RTS_smoother(length(data_y), A, B, C, μu, Wu, Wy),\n        data       = (y = data_y, ),\n        returnvars = (z = KeepLast(), u = KeepLast())\n    )\n    qs = result.posteriors\n    return (qs[:z], qs[:u])\nend","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"inference_RTS (generic function with 1 method)","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"function inference_BIFM(data_y, A, B, C, μu, Wu, Wy)\n    result = inference(\n        model      = BIFM_smoother(length(data_y), A, B, C, μu, Wu, Wy),\n        data       = (y = data_y, ),\n        returnvars = (z = KeepLast(), u = KeepLast())\n    )\n    qs = result.posteriors\n    return (qs[:z], qs[:u])\nend","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"inference_BIFM (generic function with 1 method)","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/#Experiments-for-200-observations","page":"RTS vs BIFM Smoothing","title":"Experiments for 200 observations","text":"","category":"section"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"z_RTS, u_RTS = inference_RTS(data_y, A, B, C, μu, Wu, Wy)\nz_BIFM, u_BIFM = inference_BIFM(data_y, A, B, C, μu, Wu, Wy);","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"fig, ax = plt.subplots(ncols=2, figsize=(15,5))\nax[1].plot(mean.(z_RTS), c=\"orange\")\nax[1].scatter(repeat(0:nr_samples-1, 1, dim_lat)', hcat(data_z...), c=\"blue\", s=10, alpha=0.1)\nax[2].plot(mean.(z_BIFM), c=\"orange\")\nax[2].scatter(repeat(0:nr_samples-1, 1, dim_lat)', hcat(data_z...), c=\"blue\", s=10, alpha=0.1)\nax[1].grid(), ax[2].grid()\nax[1].set_xlabel(\"sample\"), ax[2].set_xlabel(\"sample\")\nax[1].set_ylabel(\"latent state\"), ax[2].set_ylabel(\"latent state\")\nax[1].set_xlim(0, nr_samples-1), ax[2].set_xlim(0, nr_samples-1)\nax[1].set_title(\"RTS smoother\"), ax[2].set_title(\"BIFM smoother\")\nfig.suptitle(\"Latent state estimation\")\nfig","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"(Image: )","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"fig, ax = plt.subplots(ncols=2, figsize=(15,5))\nax[1].plot(mean.(u_RTS), c=\"orange\")\nax[1].scatter(repeat(0:nr_samples-1, 1, dim_in)', hcat(data_u...), c=\"blue\", s=10, alpha=0.1)\nax[2].plot(mean.(u_BIFM), c=\"orange\")\nax[2].scatter(repeat(0:nr_samples-1, 1, dim_in)', hcat(data_u...), c=\"blue\", s=10, alpha=0.1)\nax[1].grid(), ax[2].grid()\nax[1].set_xlabel(\"sample\"), ax[2].set_xlabel(\"sample\")\nax[1].set_ylabel(\"input\"), ax[2].set_ylabel(\"input\")\nax[1].set_xlim(0, nr_samples-1), ax[2].set_xlim(0, nr_samples-1)\nax[1].set_title(\"RTS smoother\"), ax[2].set_title(\"BIFM smoother\")\nfig.suptitle(\"Input estimation\")\nfig","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"(Image: )","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"fig, ax = plt.subplots(ncols=2, figsize=(15,5))\nax[1].plot(mean.(data_y), c=\"orange\")\nax[1].scatter(repeat(0:nr_samples-1, 1, dim_out)', hcat(data_y...), c=\"blue\", s=10, alpha=0.1)\nax[2].plot(mean.(data_y), c=\"orange\")\nax[2].scatter(repeat(0:nr_samples-1, 1, dim_out)', hcat(data_y...), c=\"blue\", s=10, alpha=0.1)\nax[1].grid(), ax[2].grid()\nax[1].set_xlabel(\"sample\"), ax[2].set_xlabel(\"sample\")\nax[1].set_ylabel(\"output state\"), ax[2].set_ylabel(\"output state\")\nax[1].set_xlim(0, nr_samples-1), ax[2].set_xlim(0, nr_samples-1)\nax[1].set_title(\"RTS smoother\"), ax[2].set_title(\"BIFM smoother\")\nfig.suptitle(\"Output estimation\")\nfig","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"(Image: )","category":"page"},{"location":"examples/RTS vs BIFM Smoothing/#Benchmark","page":"RTS vs BIFM Smoothing","title":"Benchmark","text":"","category":"section"},{"location":"examples/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"# This example runs in our documentation pipeline, benchmark executes approximatelly in 20 minutes so we bypass it in the documentation\n# For those who are interested in exact benchmark numbers clone this example and set `run_benchmark = true`\nrun_benchmark = false\n\nif run_benchmark\n    trials_range = 50\n    trials_n = 200\n    trials_RTS  = Array{BenchmarkTools.Trial, 1}(undef, trials_range)\n    trials_BIFM = Array{BenchmarkTools.Trial, 1}(undef, trials_range)\n\n\n    @showprogress for k = 1 : trials_range\n\n        # generate parameters\n        local A, B, C, μu, Σu, Σy, Wu, Wy = generate_parameters(3, 3, k);\n                    \n        # generate data|\n        local data_z, data_y, data_u = generate_data(trials_n, A, B, C, μu, Σu, Σy);\n\n        # run inference\n        trials_RTS[k] = @benchmark inference_RTS($data_y, $A, $B, $C, $μu, $Wu, $Wy)\n        trials_BIFM[k] = @benchmark inference_BIFM($data_y, $A, $B, $C, $μu, $Wu, $Wy)\n\n    end\n\n    m_RTS = [median(trials_RTS[k].times) for k=1:trials_range] ./ 1e9\n    q1_RTS = [quantile(trials_RTS[k].times, 0.25) for k=1:trials_range] ./ 1e9\n    q3_RTS = [quantile(trials_RTS[k].times, 0.75) for k=1:trials_range] ./ 1e9\n    m_BIFM = [median(trials_BIFM[k].times) for k=1:trials_range] ./ 1e9\n    q1_BIFM = [quantile(trials_BIFM[k].times, 0.25) for k=1:trials_range] ./ 1e9\n    q3_BIFM = [quantile(trials_BIFM[k].times, 0.75) for k=1:trials_range] ./ 1e9;\n\n    plt.figure()\n    plt.plot(1:trials_range, m_RTS, color=\"blue\", label=\"mean (RTS)\")\n    plt.fill_between(1:trials_range, q1_RTS, q3_RTS, color=\"blue\", alpha=0.3, label=\"IQ-range (RTS)\")\n    plt.plot(1:trials_range, m_BIFM, color=\"orange\", label=\"mean (BIFM)\")\n    plt.fill_between(1:trials_range, q1_BIFM, q3_BIFM, color=\"orange\", alpha=0.3, label=\"IQ-range (BIFM)\")\n    plt.yscale(\"log\")\n    plt.grid()\n    plt.xlabel(\"latent state dimension\")\n    plt.ylabel(\"duration [sec]\")\n    plt.legend()\n    plt.xlim(1, trials_range)\nend","category":"page"},{"location":"examples/Gaussian Mixture Univariate/","page":"Univariate Gaussian Mixture Model","title":"Univariate Gaussian Mixture Model","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Gaussian Mixture Univariate/#examples-univariate-gaussian-mixture-model","page":"Univariate Gaussian Mixture Model","title":"Univariate Gaussian Mixture Model","text":"","category":"section"},{"location":"examples/Gaussian Mixture Univariate/","page":"Univariate Gaussian Mixture Model","title":"Univariate Gaussian Mixture Model","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Gaussian Mixture Univariate/","page":"Univariate Gaussian Mixture Model","title":"Univariate Gaussian Mixture Model","text":"The univariate Gaussian Mixture Model can be represented:","category":"page"},{"location":"examples/Gaussian Mixture Univariate/","page":"Univariate Gaussian Mixture Model","title":"Univariate Gaussian Mixture Model","text":"beginaligned\n    p(s)   = mathrmBet(salpha_s beta_s) \n    p(m_l) =  mathcalN(m_lmu_l sigma_l)     \n    p(w_l) =  Gamma(w_lalpha_l beta_l) \n    p(z_i) =  mathrmBer(z_is) \n    p(y_i) = prod_l=1^L mathcalNleft(m_l w_lright)^z_i\nendaligned","category":"page"},{"location":"examples/Gaussian Mixture Univariate/","page":"Univariate Gaussian Mixture Model","title":"Univariate Gaussian Mixture Model","text":"using RxInfer, Random, Plots","category":"page"},{"location":"examples/Gaussian Mixture Univariate/","page":"Univariate Gaussian Mixture Model","title":"Univariate Gaussian Mixture Model","text":"@model function gaussian_mixture_model(n)\n    \n    s ~ Beta(1.0, 1.0)\n    \n    m1 ~ Normal(mean = -2.0, var = 1e3)\n    w1 ~ Gamma(shape = 0.01, rate = 0.01)\n    \n    m2 ~ Normal(mean = 2.0, var = 1e3)\n    w2 ~ Gamma(shape = 0.01, rate = 0.01)\n    \n    z = randomvar(n)\n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        z[i] ~ Bernoulli(s)\n        y[i] ~ NormalMixture(z[i], (m1, m2), (w1, w2))\n    end\n    \nend","category":"page"},{"location":"examples/Gaussian Mixture Univariate/","page":"Univariate Gaussian Mixture Model","title":"Univariate Gaussian Mixture Model","text":"n = 50\n\nRandom.seed!(124)\n\nswitch = [ 1/3, 2/3 ]\nz      = rand(Categorical(switch), n)\ny      = Vector{Float64}(undef, n)\n\nμ1 = -10.0\nμ2 = 10.0\nw  = 1.777\n\ndists = [\n    Normal(μ1, sqrt(inv(w))),\n    Normal(μ2, sqrt(inv(w))),\n]\n\nfor i in 1:n\n    global y\n    y[i] = rand(dists[z[i]])\nend","category":"page"},{"location":"examples/Gaussian Mixture Univariate/","page":"Univariate Gaussian Mixture Model","title":"Univariate Gaussian Mixture Model","text":"model = gaussian_mixture_model(length(y))\ndata  = (y = y,)\n\ninitmarginals = (\n    s  = vague(Beta), \n    m1 = NormalMeanVariance(-2.0, 1e3), \n    m2 = NormalMeanVariance(2.0, 1e3), \n    w1 = vague(GammaShapeRate), \n    w2 = vague(GammaShapeRate)\n)\n\nresult = inference(\n    model = model, \n    constraints = MeanField(),\n    data  = data, \n    initmarginals = initmarginals, \n    iterations  = 10, \n    free_energy = true\n)\n\nmswitch = result.posteriors[:s]\nmm1 = result.posteriors[:m1]\nmm2 = result.posteriors[:m2]\nmw1 = result.posteriors[:w1]\nmw2 = result.posteriors[:w2]\nmz  = result.posteriors[:z]\nfe  = result.free_energy;","category":"page"},{"location":"examples/Gaussian Mixture Univariate/","page":"Univariate Gaussian Mixture Model","title":"Univariate Gaussian Mixture Model","text":"mp = plot(mean.(mm1), ribbon = var.(mm1) .|> sqrt, label = \"m1 prediction\")\nmp = plot!(mean.(mm2), ribbon = var.(mm2) .|> sqrt, label = \"m2 prediction\")\nmp = plot!(mp, [ μ1 ], seriestype = :hline, label = \"real m1\")\nmp = plot!(mp, [ μ2 ], seriestype = :hline, label = \"real m2\")\n\nwp = plot(mean.(mw1), ribbon = var.(mw1) .|> sqrt, label = \"w1 prediction\", legend = :bottomleft, ylim = (-1, 3))\nwp = plot!(wp, [ w ], seriestype = :hline, label = \"real w1\")\nwp = plot!(wp, mean.(mw2), ribbon = var.(mw2) .|> sqrt, label = \"w2 prediction\")\nwp = plot!(wp, [ w ], seriestype = :hline, label = \"real w2\")\n\nswp = plot(mean.(mswitch), ribbon = var.(mswitch) .|> sqrt, label = \"Switch prediction\")\n\nswp = plot!(swp, [ switch[1] ], seriestype = :hline, label = \"switch[1]\")\nswp = plot!(swp, [ switch[2] ], seriestype = :hline, label = \"switch[2]\")\n\nfep = plot(fe[2:end], label = \"Free Energy\", legend = :bottomleft)\n\nplot(mp, wp, swp, fep, layout = @layout([ a b; c d ]), size = (800, 400))","category":"page"},{"location":"examples/Gaussian Mixture Univariate/","page":"Univariate Gaussian Mixture Model","title":"Univariate Gaussian Mixture Model","text":"(Image: )","category":"page"},{"location":"examples/Gaussian Mixture Univariate/","page":"Univariate Gaussian Mixture Model","title":"Univariate Gaussian Mixture Model","text":"","category":"page"},{"location":"manuals/inference/postprocess/#user-guide-inference-postprocess","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"","category":"section"},{"location":"manuals/inference/postprocess/","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"Both inference and rxinference allow users to postprocess  the inference result with the postprocess = ... keyword argument. The inference engine  operates on wrapper types to distinguish between marginals and messages. By default  these wrapper types are removed from the inference results if no addons option is present. Together with the enabled addons, however, the wrapper types are preserved in the  inference result output value. Use the options below to change this behaviour:","category":"page"},{"location":"manuals/inference/postprocess/","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"DefaultPostprocess\nUnpackMarginalPostprocess\nNoopPostprocess","category":"page"},{"location":"manuals/inference/postprocess/#RxInfer.DefaultPostprocess","page":"Inference results postprocessing","title":"RxInfer.DefaultPostprocess","text":"DefaultPostprocess picks the most suitable postprocessing step automatically\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/postprocess/#RxInfer.UnpackMarginalPostprocess","page":"Inference results postprocessing","title":"RxInfer.UnpackMarginalPostprocess","text":"This postprocessing step removes the Marginal wrapper type from the result\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/postprocess/#RxInfer.NoopPostprocess","page":"Inference results postprocessing","title":"RxInfer.NoopPostprocess","text":"This postprocessing step does nothing\n\n\n\n\n\n","category":"type"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Probit Model (EP)/#examples-probit-model-(ep)","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"","category":"section"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"RxInfer comes with support for expectation propagation (EP). In this demo we illustrate EP in the context of state-estimation in a linear state-space model that combines a Gaussian state-evolution model with a discrete observation model. Here, the probit function links continuous variable x_t with the discrete variable y_t. The model is defined as:","category":"page"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"beginaligned\n    u = 01 \n    x_0 sim mathcalN(0 100) \n    x_t sim mathcalN(x_t-1+ u 001) \n    y_t sim mathrmBer(Phi(x_t))\nendaligned","category":"page"},{"location":"examples/Probit Model (EP)/#Import-packages","page":"Probit Model (EP)","title":"Import packages","text":"","category":"section"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"using RxInfer, StableRNGs, Random, Plots\nusing StatsFuns: normcdf","category":"page"},{"location":"examples/Probit Model (EP)/#Data-generation","page":"Probit Model (EP)","title":"Data generation","text":"","category":"section"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"function generate_data(nr_samples::Int64; seed = 123)\n    \n    rng = StableRNG(seed)\n    \n    # hyper parameters\n    u = 0.1\n\n    # allocate space for data\n    data_x = zeros(nr_samples + 1)\n    data_y = zeros(nr_samples)\n    \n    # initialize data\n    data_x[1] = -2\n    \n    # generate data\n    for k = 2:nr_samples + 1\n        \n        # calculate new x\n        data_x[k] = data_x[k-1] + u + sqrt(0.01)*randn(rng)\n        \n        # calculate y\n        data_y[k-1] = normcdf(data_x[k]) > rand(rng)\n        \n    end\n    \n    # return data\n    return data_x, data_y\n    \nend;","category":"page"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"n = 40","category":"page"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"40","category":"page"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"data_x, data_y = generate_data(n);","category":"page"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"p = plot(xlabel = \"t\", ylabel = \"x, y\")\np = scatter!(p, data_y, label = \"y\")\np = plot!(p, data_x[2:end], label = \"x\")","category":"page"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"(Image: )","category":"page"},{"location":"examples/Probit Model (EP)/#Model-specification","page":"Probit Model (EP)","title":"Model specification","text":"","category":"section"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"@model function probit_model(nr_samples::Int64)\n    \n    # allocate space for variables\n    x = randomvar(nr_samples + 1)\n    y = datavar(Float64, nr_samples)\n    \n    # specify uninformative prior\n    x[1] ~ Normal(mean = 0.0, precision = 0.01)\n    \n    # create model \n    for k = 2:nr_samples + 1\n        x[k] ~ Normal(mean = x[k - 1] + 0.1, precision = 100)\n        y[k - 1] ~ Probit(x[k]) where {\n            # Probit node by default uses RequireMessage pipeline with vague(NormalMeanPrecision) message as initial value for `in` edge\n            # To change initial value use may specify it manually, like. Changes to the initial message may improve stability in some situations\n            pipeline = RequireMessage(in = NormalMeanPrecision(0, 0.01)) \n        }\n    end\n    \nend;","category":"page"},{"location":"examples/Probit Model (EP)/#Inference","page":"Probit Model (EP)","title":"Inference","text":"","category":"section"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"result = inference(\n    model = probit_model(length(data_y)), \n    data  = (y = data_y, ), \n    iterations = 5, \n    returnvars = (x = KeepLast(),),\n    free_energy  = true\n)","category":"page"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"Inference results:\n  Posteriors       | available for (x)\n  Free Energy:     | Real[23.1779, 15.743, 15.6467, 15.6462, 15.6462]","category":"page"},{"location":"examples/Probit Model (EP)/#Results","page":"Probit Model (EP)","title":"Results","text":"","category":"section"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"mx = result.posteriors[:x]\n\np = plot(xlabel = \"t\", ylabel = \"x, y\", legend = :bottomright)\np = scatter!(p, data_y, label = \"y\")\np = plot!(p, data_x[2:end], label = \"x\", lw = 2)\np = plot!(mean.(mx)[2:end], ribbon = std.(mx)[2:end], fillalpha = 0.2, label=\"x (inferred mean)\")\n\nf = plot(xlabel = \"t\", ylabel = \"BFE\")\nf = plot!(result.free_energy, label = \"Bethe Free Energy\")\n\nplot(p, f, size = (800, 400))","category":"page"},{"location":"examples/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"(Image: )","category":"page"},{"location":"manuals/inference/inference/#user-guide-inference","page":"Static dataset","title":"Automatic inference specification on static datasets","text":"","category":"section"},{"location":"manuals/inference/inference/","page":"Static dataset","title":"Static dataset","text":"RxInfer exports the inference function to quickly run and test you model with static datasets. Note, however, that this function does cover almost all capabilities of the inference engine, but for advanced use cases you may want to resort to the manual inference specification. ","category":"page"},{"location":"manuals/inference/inference/","page":"Static dataset","title":"Static dataset","text":"For running inference on real-time datasets see the Reactive Inference section. For manual inference specification see the Manual Inference section.","category":"page"},{"location":"manuals/inference/inference/","page":"Static dataset","title":"Static dataset","text":"inference\nInferenceResult","category":"page"},{"location":"manuals/inference/inference/#RxInfer.inference","page":"Static dataset","title":"RxInfer.inference","text":"inference(\n    model; \n    data,\n    initmarginals           = nothing,\n    initmessages            = nothing,\n    constraints             = nothing,\n    meta                    = nothing,\n    options                 = nothing,\n    returnvars              = nothing, \n    iterations              = nothing,\n    free_energy             = false,\n    free_energy_diagnostics = BetheFreeEnergyDefaultChecks,\n    showprogress            = false,\n    callbacks               = nothing,\n    addons                  = nothing,\n    postprocess             = DefaultPostprocess()\n)\n\nThis function provides a generic way to perform probabilistic inference in RxInfer.jl. Returns InferenceResult.\n\nArguments\n\nFor more information about some of the arguments, please check below.\n\nmodel: specifies a model generator, required\ndata: NamedTuple or Dict with data, required\ninitmarginals = nothing: NamedTuple or Dict with initial marginals, optional\ninitmessages = nothing: NamedTuple or Dict with initial messages, optional\nconstraints = nothing: constraints specification object, optional, see @constraints\nmeta  = nothing: meta specification object, optional, may be required for some models, see @meta\noptions = nothing: model creation options, optional, see ModelInferenceOptions\nreturnvars = nothing: return structure info, optional, defaults to return everything at each iteration, see below for more information\niterations = nothing: number of iterations, optional, defaults to nothing, the inference engine does not distinguish between variational message passing or Loopy belief propagation or expectation propagation iterations, see below for more information\nfree_energy = false: compute the Bethe free energy, optional, defaults to false. Can be passed a floating point type, e.g. Float64, for better efficiency, but disables automatic differentiation packages, such as ForwardDiff.jl\nfree_energy_diagnostics = BetheFreeEnergyDefaultChecks: free energy diagnostic checks, optional, by default checks for possible NaNs and Infs. nothing disables all checks.\nshowprogress = false: show progress module, optional, defaults to false\ncallbacks = nothing: inference cycle callbacks, optional, see below for more info\naddons = nothing: inject and send extra computation information along messages, see below for more info\npostprocess = DefaultPostprocess(): inference results postprocessing step, optional, see below for more info\nwarn = true: enables/disables warnings\n\nNote on NamedTuples\n\nWhen passing NamedTuple as a value for some argument, make sure you use a trailing comma for NamedTuples with a single entry. The reason is that Julia treats returnvars = (x = KeepLast()) and returnvars = (x = KeepLast(), ) expressions differently. This first expression creates (or overwrites!) new local/global variable named x with contents KeepLast(). The second expression (note trailing comma) creates NamedTuple with x as a key and KeepLast() as a value assigned for this key.\n\nExtended information about some of the arguments\n\nmodel\n\nThe model argument accepts a ModelGenerator as its input. The easiest way to create the ModelGenerator is to use the @model macro.  For example:\n\n@model function coin_toss(some_argument, some_keyword_argument = 3)\n   ...\nend\n\nresult = inference(\n    model = coin_toss(some_argument; some_keyword_argument = 3)\n)\n\nNote: The model keyword argument does not accept a FactorGraphModel instance as a value, as it needs to inject constraints and meta during the inference procedure.\n\ndata\n\nThe data keyword argument must be a NamedTuple (or Dict) where keys (of Symbol type) correspond to all datavars defined in the model specification. For example, if a model defines x = datavar(Float64) the  data field must have an :x key (of Symbol type) which holds a value of type Float64. The values in the data must have the exact same shape as the datavar container. In other words, if a model defines x = datavar(Float64, n) then  data[:x] must provide a container with length n and with elements of type Float64.\n\nNote: The behavior of the data keyword argument is different from that which is used in the rxinference function.\n\ninitmarginals\n\nFor specific types of inference algorithms, such as variational message passing, it might be required to initialize (some of) the marginals before running the inference procedure in order to break the dependency loop. If this is not done, the inference algorithm will not be executed due to the lack of information and message and/or marginals will not be updated. In order to specify these initial marginals, you can use the initmarginals argument, such as\n\ninference(...\n    initmarginals = (\n        # initialize the marginal distribution of x as a vague Normal distribution\n        # if x is a vector, then it simply uses the same value for all elements\n        # However, it is also possible to provide a vector of distributions to set each element individually \n        x = vague(NormalMeanPrecision),  \n    ),\n)\n\nThis argument needs to be a named tuple, i.e. initmarginals = (a = ..., ), or dictionary.\n\ninitmessages\n\nFor specific types of inference algorithms, such as loopy belief propagation or expectation propagation, it might be required to initialize (some of) the messages before running the inference procedure in order to break the dependency loop. If this is not done, the inference algorithm will not be executed due to the lack of information and message and/or marginals will not be updated. In order to specify these initial messages, you can use the initmessages argument, such as\n\ninference(...\n    initmessages = (\n        # initialize the messages distribution of x as a vague Normal distribution\n        # if x is a vector, then it simply uses the same value for all elements\n        # However, it is also possible to provide a vector of distributions to set each element individually \n        x = vague(NormalMeanPrecision),  \n    ),\n)\n\nThis argument needs to be a named tuple, i.e. initmessages = (a = ..., ), or dictionary.\n\noptions\nlimit_stack_depth: limits the stack depth for computing messages, helps with StackOverflowError for large models, but reduces the performance of the inference backend. Accepts integer as an argument that specifies the maximum number of recursive depth. Lower is better for stack overflow error, but worse for performance.\npipeline: changes the default pipeline for each factor node in the graph\nglobal_reactive_scheduler: changes the scheduler of reactive streams, see Rocket.jl for more info, defaults to no scheduler\nreturnvars\n\nreturnvars specifies the variables of interests and the amount of information to return about their posterior updates. \n\nreturnvars accepts a NamedTuple or Dict or return var specification. There are two specifications:\n\nKeepLast: saves the last update for a variable, ignoring any intermediate results during iterations\nKeepEach: saves all updates for a variable for all iterations\n\nNote: if iterations are specified as a number, the inference function tracks and returns every update for each iteration for every random variable in the model (equivalent to KeepEach()). If number of iterations is set to nothing, the inference function saves the 'last' (and the only one) update for every random variable in the model (equivalent to KeepLast()).  Use iterations = 1 to force KeepEach() setting when number of iterations is equal to 1 or set returnvars = KeepEach() manually.\n\nExample: \n\nresult = inference(\n    ...,\n    returnvars = (\n        x = KeepLast(),\n        τ = KeepEach()\n    )\n)\n\nIt is also possible to set either returnvars = KeepLast() or returnvars = KeepEach() that acts as an alias and sets the given option for all random variables in the model.\n\nExample:\n\nresult = inference(\n    ...,\n    returnvars = KeepLast()\n)\n\niterations\n\nSpecifies the number of variational (or loopy belief propagation) iterations. By default set to nothing, which is equivalent of doing 1 iteration. \n\nfree_energy\n\nThis setting specifies whenever the inference function should return Bethe Free Energy (BFE) values.  Note, however, that it may be not possible to compute BFE values for every model. \n\nAdditionally, the argument may accept a floating point type, instead of a Bool value. Using his option, e.g.Float64, improves performance of Bethe Free Energy computation, but restricts using automatic differentiation packages.\n\nfree_energy_diagnostics\n\nThis settings specifies either a single or a tuple of diagnostic checks for Bethe Free Energy values stream. By default checks for NaNs and Infs. See also BetheFreeEnergyCheckNaNs and BetheFreeEnergyCheckInfs. Pass nothing to disable any checks.\n\ncallbacks\n\nThe inference function has its own lifecycle. The user is free to provide some (or none) of the callbacks to inject some extra logging or other procedures in the inference function, e.g.\n\nresult = inference(\n    ...,\n    callbacks = (\n        on_marginal_update = (model, name, update) -> println(\"$(name) has been updated: $(update)\"),\n        after_inference    = (args...) -> println(\"Inference has been completed\")\n    )\n)\n\nThe callbacks keyword argument accepts a named-tuple of 'name = callback' pairs.  The list of all possible callbacks and their arguments is present below:\n\non_marginal_update:    args: (model::FactorGraphModel, name::Symbol, update)\nbefore_model_creation: args: ()\nafter_model_creation:  args: (model::FactorGraphModel, returnval)\nbefore_inference:      args: (model::FactorGraphModel)\nbefore_iteration:      args: (model::FactorGraphModel, iteration::Int)\nbefore_data_update:    args: (model::FactorGraphModel, data)\nafter_data_update:     args: (model::FactorGraphModel, data)\nafter_iteration:       args: (model::FactorGraphModel, iteration::Int)\nafter_inference:       args: (model::FactorGraphModel)\naddons\n\nThe addons field extends the default message computation rules with some extra information, e.g. computing log-scaling factors of messages or saving debug-information. Accepts a single addon or a tuple of addons. If set, replaces the corresponding setting in the options. Automatically changes the default value of the postprocess argument to NoopPostprocess.\n\npostprocess\n\nThe postprocess keyword argument controls whether the inference results must be modified in some way before exiting the inference function. By default, the inference function uses the DefaultPostprocess strategy, which by default removes the Marginal wrapper type from the results. Change this setting to NoopPostprocess if you would like to keep the Marginal wrapper type, which might be useful in the combination with the addons argument. If the addons argument has been used, automatically changes the default strategy value to NoopPostprocess.\n\nSee also: InferenceResult, rxinference\n\n\n\n\n\n","category":"function"},{"location":"manuals/inference/inference/#RxInfer.InferenceResult","page":"Static dataset","title":"RxInfer.InferenceResult","text":"InferenceResult\n\nThis structure is used as a return value from the inference function. \n\nPublic Fields\n\nposteriors: Dict or NamedTuple of 'random variable' - 'posterior' pairs. See the returnvars argument for inference.\nfree_energy: (optional) An array of Bethe Free Energy values per VMP iteration. See the free_energy argument for inference.\nmodel: FactorGraphModel object reference.\nreturnval: Return value from executed @model.\n\nSee also: inference\n\n\n\n\n\n","category":"type"},{"location":"manuals/model-specification/#user-guide-model-specification","page":"Model specification","title":"Model Specification","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The RxInfer.jl package exports the @model macro for model specification. This @model macro accepts the model specification itself in a form of regular Julia function. For example: ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(model_arguments...; model_keyword_arguments...)\n    # model specification here\n    return ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Model options, model_arguments and model_keyword_arguments are optional and may be omitted:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name()\n    # model specification here\n    return ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The @model macro returns a regular Julia function (in this example model_name()) which can be executed as usual. It returns a so-called model generator object, e.g:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function my_model(model_arguments...)\n    # model specification here\n    # ...\n    return x, y\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"generator = my_model(model_arguments...)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"In order to create an instance of the model object we should use the create_model function:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"model, (x, y) = create_model(generator)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"It is not necessary to return anything from the model, in that case RxInfer.jl will automatically inject return nothing to the end of the model function.","category":"page"},{"location":"manuals/model-specification/#A-full-example-before-diving-in","page":"Model specification","title":"A full example before diving in","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Before presenting the details of the model specification syntax, an example of a probabilistic model is given. Here is an example of a simple state space model with latent random variables x and noisy observations y:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function state_space_model(n_observations, noise_variance)\n\n    c = constvar(1.0)\n    x = randomvar(n_observations)\n    y = datavar(Float64, n_observations)\n\n    x[1] ~ NormalMeanVariance(0.0, 100.0)\n\n    for i in 2:n_observations\n       x[i] ~ x[i - 1] + c\n       y[i] ~ NormalMeanVariance(x[i], noise_var)\n    end\n\n    return x, y\nend","category":"page"},{"location":"manuals/model-specification/#Graph-variables-creation","page":"Model specification","title":"Graph variables creation","text":"","category":"section"},{"location":"manuals/model-specification/#user-guide-model-specification-constant-variables","page":"Model specification","title":"Constants","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Even though any runtime constant passed to a model as a model argument will be automatically converted to a fixed constant, sometimes it might be useful to create constants by hand (e.g. to avoid copying large matrices across the model and to avoid extensive memory allocations).","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"You can create a constant within a model specification macro with constvar() function. For example:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    c = constvar(1.0)\n\n    for i in 2:n\n        x[i] ~ x[i - 1] + c # Reuse the same reference to a constant 1.0\n    end\n    ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nconstvar() function is supposed to be used only within the @model macro.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Additionally you can specify an extra ::ConstVariable type for some of the model arguments. In this case macro automatically converts them to a single constant using constvar() function. E.g.:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(nsamples::Int, c::ConstVariable)\n    ...\n    # no need to call for a constvar() here\n    for i in 2:n\n        x[i] ~ x[i - 1] + c # Reuse the same reference to a constant `c`\n    end\n    ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\n::ConstVariable annotation does not play role in Julia's multiple dispatch. RxInfer.jl removes this annotation and replaces it with ::Any.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-data-variables","page":"Model specification","title":"Data variables","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"It is important to have a mechanism to pass data values to the model. You can create data inputs with datavar() function. As a first argument it accepts a type specification and optional dimensionality (as additional arguments or as a tuple). User can treat datavar()s in the model as both clamped values for priors and observations.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Examples: ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    y = datavar(Float64) # Creates a single data input with `y` as identificator\n    y = datavar(Float64, n) # Returns a vector of  `y_i` data input objects with length `n`\n    y = datavar(Float64, n, m) # Returns a matrix of `y_i_j` data input objects with size `(n, m)`\n    y = datavar(Float64, (n, m)) # It is also possible to use a tuple for dimensionality\n    ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\ndatavar() function is supposed to be used only within the @model macro.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"datavar() call within @model macro supports where { options... } block for extra options specification, e.g:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    y = datavar(Float64, n) where { allow_missing = true }\n    ...\nend","category":"page"},{"location":"manuals/model-specification/#Data-variables-available-options","page":"Model specification","title":"Data variables available options","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"allow_missing = true/false: Specifies if it is possible to pass missing object as an observation. Note however that by default the ReactiveMP inference engine does not expose any message computation rules that involve missings.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-random-variables","page":"Model specification","title":"Random variables","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"There are several ways to create random variables. The first one is an explicit call to randomvar() function. By default it doesn't accept any argument, creates a single random variable in the model and returns it. It is also possible to pass dimensionality arguments to randomvar() function in the same way as for the datavar() function.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Examples: ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    x = randomvar() # Returns a single random variable which can be used later in the model\n    x = randomvar(n) # Returns an vector of random variables with length `n`\n    x = randomvar(n, m) # Returns a matrix of random variables with size `(n, m)`\n    x = randomvar((n, m)) # It is also possible to use a tuple for dimensionality\n    ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nrandomvar() function is supposed to be used only within the @model macro.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"randomvar() call within @model macro supports where { options... } block for extra options specification, e.g:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    y = randomvar() where { prod_constraint = ProdGeneric() }\n    ...\nend","category":"page"},{"location":"manuals/model-specification/#Random-variables-available-options","page":"Model specification","title":"Random variables available options","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"prod_constraint\nprod_strategy\nmarginal_form_constraint\nmarginal_form_check_strategy\nmessages_form_constraint\nmessages_form_check_strategy\npipeline","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The second way to create a random variable is to create a node with the ~ operator. If the random variable has not yet been created before this call, it will be created automatically during the creation of the node. Read more about the ~ operator below.","category":"page"},{"location":"manuals/model-specification/#Node-creation","page":"Model specification","title":"Node creation","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Factor nodes are used to define a relationship between random variables and/or constants and data inputs. A factor node defines a probability distribution over selected random variables. ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nTo quickly check the list of all available factor nodes that can be used in the model specification language call ?make_node or Base.doc(make_node).","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"We model a random variable by a probability distribution using the ~ operator. For example, to create a random variable y which is modeled by a Normal distribution, where its mean and variance are controlled by the random variables m and v respectively, we define","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    m = randomvar()\n    v = randomvar()\n    y ~ NormalMeanVariance(m, v) # Creates a `y` random variable automatically\n    ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Another example, but using a deterministic relation between random variables:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    a = randomvar()\n    b = randomvar()\n    c ~ a + b\n    ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nThe RxInfer.jl package uses the ~ operator for modelling both stochastic and deterministic relationships between random variables.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The @model macro automatically resolves any inner function calls into anonymous extra nodes in case this inner function call is a non-linear transformation. It will also create needed anonymous random variables. But it is important to note that the inference backend will try to optimize inner non-linear deterministic function calls in the case where all arguments are constants or data inputs. For example:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"noise ~ NormalMeanVariance(mean, inv(precision)) # Will create a non-linear `inv` node in case if `precision` is a random variable. Won't create an additional non-linear node in case if `precision` is a constant or data input.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"It is possible to use any functional expression within the ~ operator arguments list. The only one exception is the ref expression (e.g x[i]). All reference expressions within the ~ operator arguments list are left untouched during model parsing. This means that the model parser will not create unnecessary nodes when only simple indexing is involved.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nIt is forbidden to use random variable within square brackets in the model specification.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ NormalMeanVariance(x[i - 1], variance) # While in principle `i - 1` is an inner function call (`-(i, 1)`) model parser will leave it untouched and won't create any anonymous nodes for `ref` expressions.\n\ny ~ NormalMeanVariance(A * x[i - 1], variance) # This example will create a `*` anonymous node (in case if x[i - 1] is a random variable) and leave `x[i - 1]` untouched.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"It is also possible to return a node reference from the ~ operator. Use the following syntax:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"node, y ~ NormalMeanVariance(mean, var)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Having a node reference can be useful in case the user wants to return it from a model and to use it later on to specify initial joint marginal distributions.","category":"page"},{"location":"manuals/model-specification/#Broadcasting-syntax","page":"Model specification","title":"Broadcasting syntax","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nBroadcasting syntax requires at least v2.1.0 of GraphPPL.jl ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"GraphPPL support broadcasting for ~ operator in the exact same way as Julia itself. A user is free to write an expression of the following form:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y = datavar(Float64, n)\ny .~ NormalMeanVariance(0.0, 1.0) # <- i.i.d observations","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"More complex expression are also allowed:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"m ~ NormalMeanPrecision(0.0, 0.0001)\nt ~ Gamma(1.0, 1.0)\n\ny = randomvar(Float64, n)\ny .~ NormalMeanPrecision(m, t)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"A = constvar(...)\nx = randomvar(n)\ny = datavar(Vector{Float64}, n)\n\nw         ~ Wishart(3, diageye(2))\nx[1]      ~ MvNormalMeanPrecision(zeros(2), diageye(2))\nx[2:end] .~ A .* x[1:end-1] # <- State-space model with transition matrix A\ny        .~ MvNormalMeanPrecision(x, w) # <- Observations with unknown precision matrix","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Note, however, that all variables that take part in the broadcasting operation must be defined before either with randomvar or datavar. The exception here is constants that are automatically converted to their constvar equivalent. If you want to prevent broadcasting for some constant (e.g. if you want to add a vector to a multivariate Gaussian distribution) use explicit constvar call:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"# Suppose `x` is a 2-dimensional Gaussian distribution\nz .~ x .+ constvar([ 1, 1 ])\n# Which is equivalent to \nfor i in 1:n\n   z[i] ~ x[i] + constvar([ 1, 1 ])\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Without explicit constvar Julia's broadcasting machinery would instead attempt to unroll for loop in the following way:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"# Without explicit `constvar`\nz .~ x .+ [ 1, 1 ]\n# Which is equivalent to \narray = [1, 1]\nfor i in 1:n\n   z[i] ~ x[i] + array[i] # This is wrong if `x[i]` is supposed to be a multivariate Gaussian \nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Read more about how broadcasting machinery works in Julia in the official documentation.","category":"page"},{"location":"manuals/model-specification/#Node-creation-options","page":"Model specification","title":"Node creation options","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"To pass optional arguments to the node creation constructor the user can use the where { options...  } options specification syntax.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Example:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean)q(y_var)q(y) } # mean-field factorisation over q","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"A list of the available options specific to the ReactiveMP inference engine is presented below.","category":"page"},{"location":"manuals/model-specification/#Factorisation-constraint-option","page":"Model specification","title":"Factorisation constraint option","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"See also Constraints Specification section.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Users can specify a factorisation constraint over the approximate posterior q for variational inference. The general syntax for factorisation constraints over q is the following:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"variable ~ Node(node_arguments...) where { q = RecognitionFactorisationConstraint }","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"where RecognitionFactorisationConstraint can be the following","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"MeanField()","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Automatically specifies a mean-field factorisation","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Example:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = MeanField() }","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"FullFactorisation()","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Automatically specifies a full factorisation","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Example:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = FullFactorisation() }","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"q(μ)q(v)q(out) or q(μ) * q(v) * q(out)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"A user can specify any factorisation he wants as the multiplication of q(interface_names...) factors. As interface names the user can use the interface names of an actual node (read node's documentation), its aliases (if available) or actual random variable names present in the ~ operator expression.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Examples: ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"# Using interface names of a `NormalMeanVariance` node for factorisation constraint. \n# Call `?NormalMeanVariance` to know more about interface names for some node\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ)q(v)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ, v)q(out) }\n\n# Using interface names aliases of a `NormalMeanVariance` node for factorisation constraint. \n# Call `?NormalMeanVariance` to know more about interface names aliases for some node\n# In general aliases correspond to the function names for distribution parameters\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(mean)q(var)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(mean, var)q(out) }\n\n# Using random variables names from `~` operator expression\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean)q(y_var)q(y) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean, y_var)q(y) }\n\n# All methods can be combined easily\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ)q(y_var)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean, v)q(y) }","category":"page"},{"location":"manuals/model-specification/#Metadata-option","page":"Model specification","title":"Metadata option","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Is is possible to pass any extra metadata to a factor node with the meta option. Metadata can be later accessed in message computation rules. See also Meta specification section.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"z ~ f(x, y) where { meta = ... }","category":"page"},{"location":"examples/overview/#examples-overview","page":"Overview","title":"Examples overview","text":"","category":"section"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"This section contains a set of examples for Bayesian Inference with RxInfer package in various probabilistic models.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"note: Note\nAll examples have been pre-generated automatically from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"Coin toss model (Beta-Bernoulli): An example of Bayesian inference in Beta-Bernoulli model with IID observations.\nBayesian Linear Regression: An example of Bayesian linear regression.\nActive Inference Mountain car: This notebooks covers RxInfer usage in the Active Inference setting for the simple mountain car problem.\nAssessing People’s Skills: The demo is inspired by the example from Chapter 2 of Bishop's Model-Based Machine Learning book. We are going to perform an exact inference to assess the skills of a student given the results of the test.\nGaussian Linear Dynamical System: An example of inference procedure for Gaussian Linear Dynamical System with multivariate noisy observations using Belief Propagation (Sum Product) algorithm. Reference: Simo Sarkka, Bayesian Filtering and Smoothing.\nEnsemble Learning of a Hidden Markov Model: An example of structured variational Bayesian inference in Hidden Markov Model with unknown transition and observational matrices.\nAutoregressive Model: An example of variational Bayesian Inference on full graph for Autoregressive model. Reference: Albert Podusenko, Message Passing-Based Inference for Time-Varying Autoregressive Models.\nHierarchical Gaussian Filter: An example of online inference procedure for Hierarchical Gaussian Filter with univariate noisy observations using Variational Message Passing algorithm. Reference: Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter.\nBayesian ARMA model: This notebook shows how Bayesian ARMA (Autoregressive Moving-Average) model can be implemeted in RxInfer.jl\nInfinite Data Stream: This example shows RxInfer capabilities of running inference for infinite time-series data.\nSystem Identification Problem: This example attempts to identify and separate two combined signals.\nUnivariate Gaussian Mixture Model: This example implements variational Bayesian inference in a univariate Gaussian mixture model with mean-field assumption.\nMultivariate Gaussian Mixture Model: This example implements variational Bayesian inference in a multivariate Gaussian mixture model with mean-field assumption.\nGamma Mixture Model: This example implements one of the Gamma mixture experiments outlined in https://biaslab.github.io/publication/mp-based-inference-in-gmm/ .\nUniversal Mixtures: Universal mixture modeling.\nGlobal Parameter Optimisation: This example shows how to use RxInfer.jl automated inference within other optimisation packages such as Optim.jl.\nInvertible neural networks: a tutorial: An example of variational Bayesian Inference with invertible neural networks. Reference: Bart van Erp, Hybrid Inference with Invertible Neural Networks in Factor Graphs.\nConjugate-Computational Variational Message Passing (CVI): This example provides an extensive tutorial for the non-conjugate message-passing based inference by exploiting the local CVI approximation.\nSolve GP regression by SDE: In this notebook, we solve a GP regression problem by using 'Stochastic Differential Equation' (SDE). This method is well described in the dissertation 'Stochastic differential equation methods for spatio-temporal Gaussian process regression.' by Arno Solin and 'Sequential Inference for Latent Temporal Gaussian Process Models' by Jouni Hartikainen.\nNonlinear Smoothing: Noisy Pendulum: In this demo, we will look at a realistic dynamical system with nonlinear state transitions: tracking a noisy single pendulum. We translate a differential equation in state-space model form to a probabilistic model.\nNonlinear Smoothing: Rabbit Population: In this demo, we will look at dynamical systems with nonlinear state transitions. We will start with a one-dimensional problem; the number of rabbits on an island. This problem seems overly simple, but it is a good way to demonstrate the basic pipeline of working with RxInfer.\nNonlinear Virus Spread: In this demo we consider a model for the spead of a virus (not COVID-19!) in a population. We are interested in estimating the reproduction rate from daily observations of the number of infected individuals.\nNonlinear Sensor Fusion: Nonlinear object position identification using a sparse set of sensors\nKalman filter with LSTM network driven dynamic: In this demo, we are interested in Bayesian state estimation in Nonlinear State-Space Model using the LSTM.\nHandling Missing Data: This example shows how to extend the set of builtin rules to support missing observations.\nCustom Nonlinear Node: In this example we create a non-conjugate model and use a nonlinear link function between variables. We show how to extend the functionality of RxInfer and to create a custom factor node with arbitrary message passing update rules.\nProbit Model (EP): In this demo we illustrate EP in the context of state-estimation in a linear state-space model that combines a Gaussian state-evolution model with a discrete observation model.\nRTS vs BIFM Smoothing: This example performs BIFM Kalman smoother on a factor graph using message passing and compares it with the RTS implementation.\nAdvanced Tutorial: This notebook covers the fundamentals and advanced usage of the RxInfer.jl package.","category":"page"},{"location":"manuals/inference/manual/#user-guide-manual-inference","page":"Manual inference specification","title":"Manual inference","text":"","category":"section"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"For advanced use cases it is advised to use manual inference specification. ","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"Manual inference specification with RxInfer usually consists of the same simple building blocks and designed in such a way to support both static and real-time infinite datasets:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"Create a model with @model macro and get a references to random variables and data inputs\nSubscribe to random variable posterior marginal updates \nSubscribe to Bethe Free Energy updates (optional)\nFeed model with observations \nUnsubscribe from posterior marginal updates (optional)","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"It is worth to note that Step 5 is optional and in case where observations come from an infinite real-time data stream (e.g. from the internet) it may be justified to never unsubscribe and perform real-time Bayesian inference in a reactive manner as soon as data arrives.","category":"page"},{"location":"manuals/inference/manual/#user-guide-manual-inference-model-creation","page":"Manual inference specification","title":"Model creation","text":"","category":"section"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"During model specification stage user decides on variables of interest in a model and returns (optionally) them using a return ... statement. As an example consider that we have a simple hierarchical model in which the mean of a Normal distribution is represented by another Normal distribution whose mean is modelled by another Normal distribution.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"using RxInfer, Distributions, Random\n\n@model function my_model()\n    m2 ~ NormalMeanVariance(0.0, 1.0)\n    m1 ~ NormalMeanVariance(m2, 1.0)\n\n    y = datavar(Float64)\n    y ~ NormalMeanVariance(m1, 1.0)\n\n    # Return variables of interests, optional\n    return m1, y\nend","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"And later on we may create our model and obtain references for variables of interests:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"model, (m1, y) = create_model(my_model())\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"Alternatively, it is possible to query variables using squared brackets on model object:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"model[:m1] # m1\nmodel[:y]  # y\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"@model macro also return a reference for a factor graph as its first return value. Factor graph object (named model in previous example) contains all information about all factor nodes in a model as well as random variables and data inputs.","category":"page"},{"location":"manuals/inference/manual/#user-guide-manual-inference-marginal-updates","page":"Manual inference specification","title":"Posterior marginal updates","text":"","category":"section"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"The RxInfer inference engine has a reactive API and operates in terms of Observables and Actors. For detailed information about these concepts we refer to Rocket.jl documentation.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"We use getmarginal function from ReactiveMP to get a posterior marginal updates observable:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"m1_posterior_updates = getmarginal(m1)\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"After that we can subscribe on new updates and perform some actions based on new values:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"m1_posterior_subscription = subscribe!(m1_posterior_updates, (new_posterior) -> begin\n    println(\"New posterior for m1: \", new_posterior)\nend)\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"Sometimes it is useful to return an array of random variables from model specification, in this case we may use getmarginals() function that transform an array of observables to an observable of arrays.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"@model function my_model()\n    ...\n    m_n = randomvar(n)\n    ...\n    return m_n, ...\nend\n\nmodel, (m_n, ...) = my_model()\n\nm_n_updates = getmarginals(m_n)","category":"page"},{"location":"manuals/inference/manual/#user-guide-manual-inference-observations","page":"Manual inference specification","title":"Feeding observations","text":"","category":"section"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"By default (without any extra factorisation constraints) model specification implies Belief Propagation message passing update rules. In case of BP algorithm RxInfer package computes an exact Bayesian posteriors with a single message passing iteration. To enforce Belief Propagation message passing update rule for some specific factor node user may use where { q = FullFactorisation() } option. Read more in Model Specification section. To perform a message passing iteration we need to pass some data to all our data inputs that were created with datavar function during model specification.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"To feed an observation for a specific data input we use update! function:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"update!(y, 0.0)\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"As you can see after we passed a single value to our data input we got a posterior marginal update from our subscription and printed it with println function. In case of BP if observations do not change it should not affect posterior marginal results:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"update!(y, 0.0) # Observation didn't change, should result in the same posterior\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"If y is an array of data inputs it is possible to pass an array of observation to update! function:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"for i in 1:length(data)\n    update!(y[i], data[i])\nend\n# is an equivalent of\nupdate!(y, data)","category":"page"},{"location":"manuals/inference/manual/#user-guide-manual-inference-vmp","page":"Manual inference specification","title":"Variational Message Passing","text":"","category":"section"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"Variational message passing (VMP) algorithms are generated much in the same way as the belief propagation algorithm we saw in the previous section. There is a major difference though: for VMP algorithm generation we need to define the factorization properties of our approximate distribution. A common approach is to assume that all random variables of the model factorize with respect to each other. This is known as the mean field assumption. In RxInfer, the specification of such factorization properties is defined during model specification stage using the where { q = ... } syntax or with the @constraints macro (see Constraints specification section for more info about the @constraints macro). Let's take a look at a simple example to see how it is used. In this model we want to learn the mean and precision of a Normal distribution, where the former is modelled with a Normal distribution and the latter with a Gamma.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"using RxInfer, Distributions, Random","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"real_mean      = -4.0\nreal_precision = 0.2\nrng            = MersenneTwister(1234)\n\nn    = 100\ndata = rand(rng, Normal(real_mean, sqrt(inv(real_precision))), n)\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"@model function normal_estimation(n)\n    m ~ NormalMeanVariance(0.0, 10.0)\n    w ~ Gamma(0.1, 10.0)\n\n    y = datavar(Float64, n)\n\n    for i in 1:n\n        y[i] ~ NormalMeanPrecision(m, w) where { q = MeanField() }\n    end\n\n    return m, w, y\nend","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"We create our model as usual, however in order to start VMP inference procedure we need to set initial posterior marginals for all random variables in the model:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"model, (m, w, y) = create_model(normal_estimation(n))\n\n# We use vague initial marginals\nsetmarginal!(m, vague(NormalMeanVariance)) \nsetmarginal!(w, vague(Gamma))\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"To perform a single VMP iteration it is enough to feed all data inputs with some values. To perform multiple VMP iterations we should feed our all data inputs with the same values multiple times:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"m_marginals = []\nw_marginals = []\n\nsubscriptions = subscribe!([\n    (getmarginal(m), (marginal) -> push!(m_marginals, marginal)),\n    (getmarginal(w), (marginal) -> push!(w_marginals, marginal)),\n])\n\nvmp_iterations = 10\n\nfor _ in 1:vmp_iterations\n    update!(y, data)\nend\n\nunsubscribe!(subscriptions)\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"As we process more VMP iterations, our beliefs about the possible values of m and w converge and become more confident.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"using Plots\n\np1    = plot(title = \"'Mean' posterior marginals\")\ngrid1 = -6.0:0.01:4.0\n\nfor iter in [ 1, 2, 10 ]\n\n    estimated = Normal(mean(m_marginals[iter]), std(m_marginals[iter]))\n    e_pdf     = (x) -> pdf(estimated, x)\n\n    plot!(p1, grid1, e_pdf, fill = true, opacity = 0.3, label = \"Estimated mean after $iter VMP iterations\")\n\nend\n\nplot!(p1, [ real_mean ], seriestype = :vline, label = \"Real mean\", color = :red4, opacity = 0.7)","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"p2    = plot(title = \"'Precision' posterior marginals\")\ngrid2 = 0.01:0.001:0.35\n\nfor iter in [ 2, 3, 10 ]\n\n    estimated = Gamma(shape(w_marginals[iter]), scale(w_marginals[iter]))\n    e_pdf     = (x) -> pdf(estimated, x)\n\n    plot!(p2, grid2, e_pdf, fill = true, opacity = 0.3, label = \"Estimated precision after $iter VMP iterations\")\n\nend\n\nplot!(p2, [ real_precision ], seriestype = :vline, label = \"Real precision\", color = :red4, opacity = 0.7)","category":"page"},{"location":"manuals/inference/manual/#user-guide-manual-inference-vmp-bfe","page":"Manual inference specification","title":"Computing Bethe Free Energy","text":"","category":"section"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"VMP inference boils down to finding the member of a family of tractable probability distributions that is closest in KL divergence to an intractable posterior distribution. This is achieved by minimizing a quantity known as Variational Free Energy. RxInfer uses Bethe Free Energy approximation to the real Variational Free Energy. Free energy is particularly useful to test for convergence of the VMP iterative procedure.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"The RxInfer package exports score function for an observable of free energy values:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"fe_observable = score(model, BetheFreeEnergy())\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"# Reset posterior marginals for `m` and `w`\nsetmarginal!(m, vague(NormalMeanVariance))\nsetmarginal!(w, vague(Gamma))\n\nfe_values = []\n\nfe_subscription = subscribe!(fe_observable, (v) -> push!(fe_values, v))\n\nvmp_iterations = 10\n\nfor _ in 1:vmp_iterations\n    update!(y, data)\nend\n\nunsubscribe!(fe_subscription)","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"plot(fe_values, label = \"Bethe Free Energy\", xlabel = \"Iteration #\")","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Assessing People Skills/#examples-assessing-people’s-skills","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"","category":"section"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"This demo demonstrates the capabilities of ReactiveMP.jl to perform inference in the models composed of Bernoulli random variables.","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"The demo is inspired by the example from Chapter 2 of Bishop's Model-Based Machine Learning book. We are going to perform an exact inference to assess the skills of a student given the results of the test.","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Let us assume that our imaginary test is composed of three questions, and each of these questions is associated with test results r, where r in mathbbR 0  r  1","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"The result of the first question will solely depend on the student's attendance. For example, if the student attends the lectures, he will most certainly answer the first question. The result of the second question will depend on a specific skill s_2. However, if the student has attended the lectures, he would still have a good chance of answering the second question. We will model this relationship through disjunction or logical OR. The third question is more difficult to answer, i.e., the student needs to have a particular skill s_3 and he must have good attendance or must have a s_3 Hence, to model this relationship between skills and the third question, we will use conjunction or logical AND.","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"For the sake of the example, we will replace attendance with laziness. The convention is that if a person is not lazy, he attends lectures. This way, the first question can be answered if the student is not lazy. We will use the NOT function to represent this relationship.","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Let us define the generative model: p(l s_2 s_3 r_1 r_2 r_3)=p(l)p(s_2)p(s_3)p(r_1f_1(l))p(r_2f_2(l s_2))p(r_3f_3(l s_2 s_3))","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"The factors p(l) p(s_2) p(s_3) represent Bernoulli prior distributions. ","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"f_1(l) = NOT(l)","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"where NOT(X) triangleq overlineX, ","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"f_2(l s_2) = OR(NOT(l) s_2)","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"where OR(X Y) triangleq X vee Y, ","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"f_3(l s_2 s_3) = AND(OR(NOT(l) s_2) s_3)","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"where AND(X Y) triangleq X land Y","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"An attentive reader may notice that f_2(l s_2) can be rewritten as IMPLY(l s_2), i.e., limplies s_2 ","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Similar to the example from the Model-Based Machine Learning book, our observations are noisy. It means that the likelihood functions should map 0 1 to a real value r in (0 1), denoting the result of the test. We can associate r=0 and r=10 with 0 and 100 correctness of the test, respectively.","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"One way of specifying the likelihood is p(r_if) = begincases r_i  textif f_i = 1 \n1-r_i  textif f_i=0 endcases or p(r_if)=r_if_i+(1-r_i)(1-f_i)","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"It can be shown that given the observation r_i, the backward message from the node p(r_if_i) will be a Bernoulli distribution with parameter r_i, i.e. overleftarrowmu(f_i)proptomathrmBer(r_i).  If we observe r_i=09 it is more \"likely\" that the variable f_i=1.","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Following Bishop, we will call this node function AddNoise","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"using RxInfer, Random","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"# Create AddNoise node\nstruct AddNoise end\n\n@node AddNoise Stochastic [out, in]","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"# Adding update rule for AddNoise node\n@rule AddNoise(:in, Marginalisation) (q_out::PointMass,) = begin     \n    return Bernoulli(mean(q_out))\nend","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"# GraphPPL.jl export `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function skill_model()\n\n    res = datavar(Float64, 3)\n\n    laziness ~ Bernoulli(0.5)\n    skill2 ~ Bernoulli(0.5)\n    skill3 ~ Bernoulli(0.5)\n\n    test2 ~ laziness -> skill2\n    test3 ~ test2 && skill3\n    \n    res[1] ~ AddNoise(¬laziness)\n    res[2] ~ AddNoise(test2)\n    res[3] ~ AddNoise(test3)\n\nend","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Let us assume that a student scoared 70 and 95 at first and second tests respectively. But got only 30 on the third one. ","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"test_results = [0.7, 0.95, 0.3]\n\ninference_result = inference(\n    model = skill_model(),\n    data  = (res = test_results, )\n)","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Inference results:\n  Posteriors       | available for (skill3, test2, test3, skill2, laziness)","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"inference_result.posteriors[:laziness]","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Bernoulli{Float64}(p=0.18704156479217607)","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"inference_result.posteriors[:skill2]","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Bernoulli{Float64}(p=0.5806845965770171)","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"inference_result.posteriors[:skill3]","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Bernoulli{Float64}(p=0.3025672371638141)","category":"page"},{"location":"examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"The results make sense. On the one hand, the student answered the first question correctly, which immediately gives us reason to believe that he is not lazy. He answered the second question pretty well, but this does not mean that the student had the skills to answer this question (attendance,i.e., lack of laziness, could help). To answer the third question, it was necessary to answer the second and have additional skills (#3). Unfortunately, the student's answer was weak, so our confidence about skill #3 was shattered.","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/#examples-multivariate-gaussian-mixture-model","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"","category":"section"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"In this example we are going to perform an automated Variational Bayesian Inference for multivariate Gaussian Mixture Model that can be represented as following:","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"beginaligned\n    p(mathbfs)   = mathrmDir(mathbfsboldsymbolalpha) \n    p(mathbfm_l) =  mathcalN(m_lboldsymbolmu_l boldsymbolSigma_l)     \n    p(mathbfW_l) =  mathcalW(mathbfW_lmathbfV_l nu_l) \n    p(mathbfz_i) =  mathrmCat(mathbfz_imathbfs) \n    p(mathbfy_i) = prod_l=1^L mathcalNleft(mathbfm_l mathbfW_lright)^mathbfz_i\nendaligned","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"ENV[\"GKS_ENCODING\"] = \"utf8\"; # Fix for Plots.jl","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"using RxInfer, Plots, Random, LinearAlgebra, BenchmarkTools","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"Random.seed!(123)\n\nL         = 50.0\nnmixtures = 6\nn_samples = 500\n\nprobvec = ones(nmixtures)\nprobvec = probvec ./ sum(probvec)\n\nswitch = Categorical(probvec)\n\nprintln(\"Switch distribution: \", Distributions.params(switch))\n\ngaussians = map(1:nmixtures) do index\n    angle      = 2π / nmixtures * (index - 1)\n    basis_v    = L * [ 1.0, 0.0 ]\n    rotationm  = [ cos(angle) -sin(angle); sin(angle) cos(angle) ]\n    mean       = rotationm * basis_v \n    covariance = Matrix(Hermitian(rotationm * [ 10.0 0.0; 0.0 20.0 ] * transpose(rotationm)))\n    return MvNormal(mean, covariance)\nend\n\nz = rand(switch, n_samples)\ny = Vector{Vector{Float64}}(undef, n_samples)\n\nfor i in 1:n_samples\n    y[i] = rand(gaussians[z[i]])\nend","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"Switch distribution: ([0.16666666666666666, 0.16666666666666666, 0.16666666\n666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666],)","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"sdim(n) = (a) -> map(d -> d[n], a)","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"sdim (generic function with 1 method)","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"let \n    local p = plot(xlim = (-1.5L, 1.5L), ylim = (-1.5L, 1.5L))\n\n    for (index, gaussian) in enumerate(gaussians)\n        p = contour!(p, range(-2L, 2L, step = 0.25), range(-2L, 2L, step = 0.25), (x, y) -> pdf(gaussian, [ x, y ]), levels = 3, colorbar = false)\n    end\n\n    p = scatter!(y |> sdim(1), y |> sdim(2), ms = 2, alpha = 0.4)\n\n    plot(p, size = (600, 400), legend=false)\nend","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"(Image: )","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"@model function gaussian_mixture_model(rng, nmixtures, n, priors_mean, priors_cov)\n    \n    z = randomvar(n)\n    m = randomvar(nmixtures)\n    w = randomvar(nmixtures)\n    \n    for i in 1:nmixtures        \n        m[i] ~ MvNormal(μ = priors_mean[i], Σ = priors_cov[i])\n        w[i] ~ Wishart(3, [ 1e2 0.0; 0.0 1e2 ])\n    end\n    \n    s ~ Dirichlet(ones(nmixtures))\n\n    y = datavar(Vector{Float64}, n)\n    \n    means = tuple(m...)\n    precs = tuple(w...)\n    \n    for i in 1:n\n        z[i] ~ Categorical(s) \n        y[i] ~ NormalMixture(z[i], means, precs)\n    end\n    \nend","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"rng = MersenneTwister(11)\n\nbasis_v       = [ 1.0, 0.0 ]\napproximate_L = 50rand(rng)\npriors_mean   = []\n\nfor i in 1:nmixtures\n    approximate_angle_prior = ((2π + rand(rng)) / nmixtures) * (i - 1)\n    approximate_basis_v  = approximate_L / 2 * (basis_v .+ rand(rng, 2))\n    approximate_rotation = [ cos(approximate_angle_prior) -sin(approximate_angle_prior); sin(approximate_angle_prior) cos(approximate_angle_prior) ]\n    push!(priors_mean,  approximate_rotation * approximate_basis_v)\nend\n\npriors_cov = [ [ 1e2 0.0; 0.0 1e2 ] for _ in 1:nmixtures ]\n\ninitmarginals = (\n    s = vague(Dirichlet, nmixtures), \n    m = [ MvNormalMeanCovariance(prior[1], prior[2]) for prior in zip(priors_mean, priors_cov) ], \n    w = Wishart(3, [ 1e2 0.0; 0.0 1e2 ])\n)\n\nresult = inference(\n    model = gaussian_mixture_model(rng, nmixtures, length(y), priors_mean, priors_cov), \n    data  = (y = y,), \n    constraints   = MeanField(),\n    initmarginals = initmarginals, \n    iterations  = 20, \n    free_energy = true\n)","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"Inference results:\n  Posteriors       | available for (m, w, s, z)\n  Free Energy:     | Real[4511.95, 4095.63, 4018.06, 4002.74, 3977.26, 3951\n.16, 3931.94, 3912.63, 3871.22, 3813.31, 3812.07, 3812.07, 3812.07, 3812.07\n, 3812.07, 3812.07, 3812.07, 3812.07, 3812.07, 3812.07]","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"s = result.posteriors[:s]\nz = result.posteriors[:z]\nm = result.posteriors[:m]\nw = result.posteriors[:w]\nfe = result.free_energy;","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"let \n    local pe = plot(xlim = (-1.5L, 1.5L), ylim = (-1.5L, 1.5L))\n    local rp = scatter(y |> sdim(1), y |> sdim(2), xlim = (-1.5L, 1.5L), ylim = (-1.5L, 1.5L), legend=false, title=\"Generated\", ms = 2)\n    local pfe = plot(fe[2:end], label = \"Free Energy\")\n\n    e_means = m[end]\n    e_precs = w[end]\n\n    for (e_m, e_w) in zip(e_means, e_precs)\n        gaussian = MvNormal(mean(e_m), Matrix(Hermitian(mean(inv, e_w))))\n        pe = contour!(pe, range(-2L, 2L, step = 0.25), range(-2L, 2L, step = 0.25), (x, y) -> pdf(gaussian, [ x, y ]), title=\"Inference result\", legend=false, levels = 7, colorbar = false)\n    end\n\n    plot(rp, pe, pfe, layout = @layout([ a b; c ]))\nend","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"(Image: )","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"@benchmark inference(\n    model = gaussian_mixture_model($rng, $nmixtures, length($y), $priors_mean, $priors_cov), \n    data = (y = $y,), \n    initmarginals = $initmarginals, \n    constraints = MeanField(),\n    iterations = 15, \n    free_energy = true\n)","category":"page"},{"location":"examples/Gaussian Mixtures Multivariate/","page":"Multivariate Gaussian Mixture Model","title":"Multivariate Gaussian Mixture Model","text":"BenchmarkTools.Trial: 5 samples with 1 evaluation.\n Range (min … max):  986.951 ms …   1.195 s  ┊ GC (min … max): 12.90% … 27.\n88%\n Time  (median):        1.030 s              ┊ GC (median):    12.36%\n Time  (mean ± σ):      1.068 s ± 84.791 ms  ┊ GC (mean ± σ):  17.18% ±  7.\n18%\n\n  █       █   █                       █                      █  \n  █▁▁▁▁▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  987 ms          Histogram: frequency by time          1.19 s <\n\n Memory estimate: 346.13 MiB, allocs estimate: 3656770.","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Handling Missing Data/#examples-handling-missing-data","page":"Handling Missing Data","title":"Handling Missing Data","text":"","category":"section"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"Let us assume that the following model generates the data","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"beginaligned\n    x_t sim mathcalNleft(x_t-1 10right) \n    y_t sim mathcalNleft(x_t P right) \nendaligned","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"with prior x_0 sim mathcalN(m_x_0 v_x_0). Suppose that our measurement device fails to acquire data from time to time.  In this case, instead of scalar observation haty_t in mathrmR we sometimes will catch missing observations.","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"using RxInfer, Plots","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"@model function smoothing(n, x0)\n    \n    P ~ Gamma(shape = 0.001, scale = 0.001)\n    x_prior ~ Normal(mean = mean(x0), var = var(x0)) \n\n    x = randomvar(n)\n    y = datavar(Float64, n) where { allow_missing = true }\n\n    x_prev = x_prior\n\n    for i in 1:n\n        x[i] ~ Normal(mean = x_prev, precision = 1.0)\n        y[i] ~ Normal(mean = x[i], precision = P)\n        \n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"We need to manually extend the set of rules to support ::Missing values","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Missing) = missing\n@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Missing, q_τ::Any) = missing\n\n@rule NormalMeanPrecision(:τ, Marginalisation) (q_out::Any, q_μ::Missing) = missing\n@rule NormalMeanPrecision(:τ, Marginalisation) (q_out::Missing, q_μ::Any) = missing\n\n@rule typeof(+)(:in1, Marginalisation) (m_out::Missing, m_in2::Any) = missing\n@rule typeof(+)(:in1, Marginalisation) (m_out::Any, m_in2::Missing) = missing","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"P = 1.0\nn = 250\n\nreal_signal     = map(e -> sin(0.05 * e), collect(1:n))\nnoisy_data      = real_signal + rand(Normal(0.0, sqrt(P)), n);\nmissing_indices = 100:125\nmissing_data    = similar(noisy_data, Union{Float64, Missing}, )\n\ncopyto!(missing_data, noisy_data)\n\nfor index in missing_indices\n    missing_data[index] = missing\nend","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"constraints = @constraints begin\n    q(x, P) = q(x)q(P)\nend","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"Constraints:\n  marginals form:\n  messages form:\n  factorisation:\n    q(x, P) = q(x)q(P)\nOptions:\n  warn = true","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"x0_prior = NormalMeanVariance(0.0, 1000.0)\n\nresult = inference(\n    model = smoothing(n, x0_prior), \n    data  = (y = missing_data,), \n    constraints = constraints,\n    initmarginals = (P = Gamma(0.001, 0.001), ),\n    returnvars = (x = KeepLast(),),\n    iterations = 20\n);","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"plot(real_signal, label = \"Noisy signal\", legend = :bottomright)\nscatter!(missing_indices, real_signal[missing_indices], ms = 2, opacity = 0.75, label = \"Missing region\")\nplot!(mean.(result.posteriors[:x]), ribbon = var.(result.posteriors[:x]), label = \"Estimated hidden state\")","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"(Image: )","category":"page"},{"location":"examples/Handling Missing Data/","page":"Handling Missing Data","title":"Handling Missing Data","text":"","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Universal Mixtures/#examples-universal-mixtures","page":"Universal Mixtures","title":"Universal Mixtures","text":"","category":"section"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"using RxInfer, Distributions, Random, Plots","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"John and Jane are having a coin toss competition. Before they start, they both have the feeling that something is not right. The coin is unbalanced and favors one side over the other. However, both John and Jane do not know which side is being favored. John thinks that the coin favors heads and Jane thinks tails. Coincidentally, both John and Jane have a strong mathematics background and express their gut feeling in terms of a prior distribution over the coin parameter theta, which represents the probability of the coin landing on heads. Based on their gut feeling and the support of thetain01 they come up with the prior beliefs:","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"prior beliefs\")\nplot!(rθ, (x) -> pdf(Beta(7.0, 2.0), x), fillalpha=0.3, fillrange = 0, label=\"P(θ) John\", c=1)\nplot!(rθ, (x) -> pdf(Beta(2.0, 7.0), x), fillalpha=0.3, fillrange = 0, label=\"p(θ) Jane\", c=3,)","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"John and Jane really want to clear the odds and decide to perform a lengthy experiment. They toss the unbalanced coin 10 times, because their favorite TV show is cancelled anyway and therefore they have plenty of time. ","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"true_coin = Bernoulli(0.25)\nnr_throws = 10\ndataset = Int.(rand(MersenneTwister(42), true_coin, nr_throws))\nnr_heads, nr_tails = sum(dataset), nr_throws-sum(dataset)\nprintln(\"experimental outcome: \\n - heads: \", nr_heads, \"\\n - tails: \", nr_tails);","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"experimental outcome: \n - heads: 3\n - tails: 7","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"For updating their prior beliefs about the parameter theta, they will perform probabilistic inference in the model based on Bayes' rule. Luckily everything is tractable and therefore they can resort to exact inference. They decide to outsource these tedious computations using RxInfer.jl and specify the following models:","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_john(nr_throws)\n\n    # specify experimental outcomes\n    y = datavar(Int64, nr_throws)\n\n    # specify John's prior model over θ\n    θ ~ Beta(7.0, 2.0)\n\n    # create likelihood models\n    for i in 1:nr_throws\n        y[i] ~ Bernoulli(θ)\n    end\n\n    return y, θ\n    \nend","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_jane(nr_throws)\n\n    # specify experimental outcomes\n    y = datavar(Int64, nr_throws)\n\n    # specify Jane's prior model over θ\n    θ ~ Beta(2.0, 7.0)\n\n    # create likelihood models\n    for i in 1:nr_throws\n        y[i] ~ Bernoulli(θ)\n    end\n\n    return y, θ\n    \nend","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Now it is time to figure out whose prior belief was the best and who was actually right. They perform probabilistic inference automatically and compute the Bethe free energy to compare eachothers models.","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_john = inference(\n    model = beta_model_john(nr_throws), \n    data  = (y = dataset, ),\n    free_energy = true,\n)","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (θ)\n  Free Energy:     | Real[8.96366]","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_jane = inference(\n    model = beta_model_jane(nr_throws), \n    data  = (y = dataset, ),\n    free_energy = true\n)","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (θ)\n  Free Energy:     | Real[6.63988]","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"From these results, they agree that Jane her gut feeling was right all along, as her Bethe free energy is lower. Nonetheless, after the 10 throws, they now have a better idea about the underlying theta parameter. They formulate this through the posterior distributions:","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"posterior beliefs\")\nplot!(rθ, (x) -> pdf(result_john.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y) John\", c=1)\nplot!(rθ, (x) -> pdf(result_jane.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"p(θ|y) Jane\", c=3,)","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"What John and Jane did not know, was that Mary, their neighbour, was overhearing their conversation. She was also curious, but could not see the coin. She did not really know how to formulate a prior distribution over theta, so instead she combined both John and Jane their prior beliefs. She had the feeling that John his assessment was more correct, as he was often going to the casino. As a result, she mixed the prior beliefs of John and Jane with proportions 0.7 and 0.3, respecively. Her prior distribution over theta now became","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"prior belief\")\nplot!(rθ, (x) -> pdf(MixtureModel([Beta(7.0, 2.0), Beta(2.0, 7.0)], Bernoulli(0.7)), x), fillalpha=0.3, fillrange = 0, label=\"P(θ) Mary\", c=1)\nplot!(rθ, (x) -> 0.7*pdf(Beta(7.0, 2.0), x), c=3, label=\"\")\nplot!(rθ, (x) -> 0.3*pdf(Beta(2.0, 7.0), x), c=3, label=\"\")","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"She was also interested in the results and used the new Switch node and addons in ReactiveMP.jl. She specified her model as follows and performed inference in this model:","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_mary(nr_throws)\n\n    # specify experimental outcomes\n    y = datavar(Int64, nr_throws)\n\n    # specify John's and Jane's prior models over θ\n    θ_john ~ Beta(7.0, 2.0)\n    θ_jane ~ Beta(2.0, 7.0)\n\n    # specify initial guess as to who is right\n    john_is_right ~ Bernoulli(0.7)\n\n    # specify mixture prior Distribution\n    θ ~ Switch(john_is_right, (θ_john, θ_jane))\n\n    # create likelihood models\n    for i in 1:nr_throws\n        y[i] ~ Bernoulli(θ)\n    end\n\n    return y, θ\n    \nend","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_mary = inference(\n    model = beta_model_mary(nr_throws), \n    data  = (y = dataset, ),\n    returnvars = (θ = KeepLast(), θ_john = KeepLast(), θ_jane = KeepLast(), john_is_right = KeepLast()),\n    addons = AddonLogScale(),\n    postprocess = UnpackMarginalPostprocess(),\n)","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (john_is_right, θ_john, θ, θ_jane)","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Mary was happy, with her mixture prior, she beat John in terms of performance. However, it was not the best decision to think that John was right. In fact, after the experiment there was only a minor possibility remaining that John was right. Her posterior distribution over theta also changed, and as expected the estimate from Jane was more prominent.","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"posterior belief\")\nplot!(rθ, (x) -> pdf(result_mary.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y) Mary\", c=1)\nplot!(rθ, (x) -> probvec(result_mary.posteriors[:θ].prior)[1]*pdf(result_mary.posteriors[:θ].components[1], x), label=\"\", c=3)\nplot!(rθ, (x) -> probvec(result_mary.posteriors[:θ].prior)[2]*pdf(result_mary.posteriors[:θ].components[2], x), label=\"\", c=3)","category":"page"},{"location":"examples/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Advanced Tutorial/#examples-advanced-tutorial","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"using RxInfer, Plots","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This notebook covers the fundamentals and advanced usage of the RxInfer.jl package.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This tutorial is also available in the documentation.","category":"page"},{"location":"examples/Advanced Tutorial/#General-model-specification-syntax","page":"Advanced Tutorial","title":"General model specification syntax","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We use the @model macro from the RxInfer.jl package to create a probabilistic model p(s y) and we also specify extra constraints on the variational family of distributions mathcalQ, used for approximating intractable posterior distributions. Below there is a simple example of the general syntax for model specification. In this tutorial we do not cover all possible ways to create models or advanced features of RxInfer.jl.  Instead we refer the interested reader to the documentation for a more rigorous explanation and illustrative examples.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# the `@model` macro accepts a regular Julia function\n@model function test_model1(s_mean, s_precision)\n    \n    # We use the `randomvar` function to create \n    # a random variable in our model\n    s = randomvar()\n    \n    # the `tilde` operator creates a functional dependency\n    # between variables in our model and can be read as \n    # `sampled from` or `is modeled by`\n    s ~ Normal(mean = s_mean, precision = s_precision)\n    \n    # We use the `datavar` function to create \n    # observed data variables in our models\n    # We also need to specify the type of our data \n    # In this example it is `Float64`\n    y = datavar(Float64)\n    \n    y ~ Normal(mean = s, precision = 1.0)\n    \n    # It is possible to return something from the model specification (including variables and nodes)\n    return \"Hello world\"\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The @model macro creates a function with the same name and with the same set of input arguments as the original function (test_model1(s_mean, s_precision) in this example). The return value is modified in such a way to contain a reference to the model object as the first value and to the user specified variables in the form of a tuple as the second value.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"modelgenerator = test_model1(0.0, 1.0)\n\nmodel, returnval = create_model(modelgenerator)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(FactorGraphModel(), \"Hello world\")","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The benefits of using model generator as a way to create a model is that it allows to change inference constraints and meta specification for nodes. We will talk about factorisation and form constraints and meta specification later on in this demo.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl returns a factor graph-based representation of a model. We can examine this factor graph structure with the help of some utility functions such as: ","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getnodes(): returns an array of factor nodes in a corresponding factor graph\ngetrandom(): returns an array of random variables in the model\ngetdata(): returns an array of data inputs in the model\ngetconstant(): returns an array of constant values in the model","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getnodes(model)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"FactorNodesCollection(nodes: 2)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getrandom(model) .|> name","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"1-element Vector{Symbol}:\n :s","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getdata(model) .|> name","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"1-element Vector{Symbol}:\n :y","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getconstant(model) .|> getconst","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"3-element Vector{Float64}:\n 0.0\n 1.0\n 1.0","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use control flow statements such as if or for blocks in the model specification function. In general, any valid snippet of Julia code can be used inside the @model block. As an example consider the following (valid!) model:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model2(n)\n    \n    if n <= 1\n        error(\"`n` argument must be greater than one.\")\n    end\n    \n    # `randomvar(n)` creates a dense sequence of \n    # random variables\n    s = randomvar(n)\n    \n    # `datavar(Float64, n)` creates a dense sequence of \n    # observed data variables of type `Float64`\n    y = datavar(Float64, n)\n    \n    s[1] ~ Normal(mean = 0.0, precision = 0.1)\n    y[1] ~ Normal(mean = s[1], precision = 1.0)\n    \n    for i in 2:n\n        s[i] ~ Normal(mean = s[i - 1], precision = 1.0)\n        y[i] ~ Normal(mean = s[i], precision = 1.0)\n    end\n    \nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, _ = create_model(test_model2(10));","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# An amount of factor nodes in generated Factor Graph\ngetnodes(model) |> length","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"20","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# An amount of random variables\ngetrandom(model) |> length","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"10","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# An amount of data inputs\ngetdata(model) |> length","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"10","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# An amount of constant values\ngetconstant(model) |> length","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"21","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use complex expressions inside the functional dependency expressions","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"y ~ NormalMeanPrecision(2.0 * (s + 1.0), 1.0)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The ~ operator automatically creates a random variable if none was created before with the same name and throws an error if this name already exists","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# s = randomvar() here is optional\n# `~` creates random variables automatically\ns ~ NormalMeanPrecision(0.0, 1.0)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"An example model which will throw an error:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function error_model1()\n    s = 1.0\n    s ~ NormalMeanPrecision(0.0, 1.0)\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"By default the RxInfer.jl package creates new references for constants (literals like 0.0 or 1.0) in a model. In some situations this may not be efficient, especially when these constants represent large matrices. RxInfer.jl will by default create new copies of some constant (e.g. matrix) in a model every time it uses it. However it is possible to use constvar() function to create and reuse similar constants in the model specification syntax as","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Creates constant reference in a model with a prespecified value\nc = constvar(0.0)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"An example:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model5(dim::Int, n::Int, A::Matrix, P::Matrix, Q::Matrix)\n    \n    s = randomvar(n)\n    \n    y = datavar(Vector{Float64}, n)\n    \n    # Here we create constant references\n    # for constant matrices in our model \n    # to make inference more memory efficient\n    cA = constvar(A)\n    cP = constvar(P)\n    cQ = constvar(Q)\n    \n    s[1] ~ MvNormal(mean = zeros(dim), covariance = cP)\n    y[1] ~ MvNormal(mean = s[1], covariance = cQ)\n    \n    for i in 2:n\n        s[i] ~ MvNormal(mean = cA * s[i - 1], covariance = cP)\n        y[i] ~ MvNormal(mean = s[i], covariance = cQ)\n    end\n    \nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The ~ expression can also return a reference to a newly created node in a corresponding factor graph for convenience in later usage:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model()\n\n    # In this example `ynode` refers to the corresponding \n    # `GaussianMeanVariance` node created in the factor graph\n    ynode, y ~ GaussianMeanVariance(0.0, 1.0)\n    \n    return ynode, y\nend","category":"page"},{"location":"examples/Advanced Tutorial/#Probabilistic-inference-in-RxInfer.jl","page":"Advanced Tutorial","title":"Probabilistic inference in RxInfer.jl","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl uses the Rocket.jl package API for inference routines. Rocket.jl is a reactive programming extension for Julia that is higly inspired by RxJS and similar libraries from the Rx ecosystem. It consists of observables, actors, subscriptions and operators. For more information and rigorous examples see Rocket.jl github page.","category":"page"},{"location":"examples/Advanced Tutorial/#Observables","page":"Advanced Tutorial","title":"Observables","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Observables are lazy push-based collections and they deliver their values over time.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Timer that emits a new value every second and has an initial one second delay \nobservable = timer(300, 300)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerObservable(300, 300)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"A subscription allows us to subscribe on future values of some observable, and actors specify what to do with these new values:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"actor = (value) -> println(value)\nsubscription1 = subscribe!(observable, actor)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerSubscription()","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# We always need to unsubscribe from some observables\nunsubscribe!(subscription1)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# We can modify our observables\nmodified = observable |> filter(d -> rem(d, 2) === 1) |> map(Int, d -> d ^ 2)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"ProxyObservable(Int64, MapProxy(Int64))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscription2 = subscribe!(modified, (value) -> println(value))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerSubscription()","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"unsubscribe!(subscription2)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function coin_toss_model(n)\n\n    # `datavar` creates data 'inputs' in our model\n    # We will pass data later on to these inputs\n    # In this example we create a sequence of inputs that accepts Float64\n    y = datavar(Float64, n)\n    \n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(2.0, 7.0)\n    \n    # We assume that the outcome of each coin flip \n    # is modeled by a Bernoulli distribution\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)\n    end\n    \n    # We return references to our data inputs and θ parameter\n    # We will use these references later on during the inference step\n    return y, θ\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We can call the inference function to run inference in such model:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"p = 0.75 # Bias of a coin\n\ndataset = float.(rand(Bernoulli(p), 500));\n\nresult = inference(\n    model = coin_toss_model(length(dataset)),\n    data  = (y = dataset, )\n)\n\nprintln(\"Inferred bias: \", mean_var(result.posteriors[:θ]))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inferred bias: (0.75049115913556, 0.00036716505724494823)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We can see that the inferred bias is quite close to the actual value we used in the dataset generation.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The RxInfer.jl package's API is more flexible (and reactive!) and can return posterior marginal distributions in our specified model in the form of an observable. It is possible to subscribe on its future updates, but for convenience RxInfer.jl only caches the last obtained values of all marginals in a model. To get a reference for the posterior marginal of some random variable in a model RxInfer.jl exports two functions: ","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getmarginal(x): for a single random variable x\ngetmarginals(xs): for a dense sequence of random variables sx","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Let's see how it works in practice. Here we create a simple coin toss model. We assume that observations are governed by the Bernoulli distribution with unknown bias parameter θ. To have a fully Bayesian treatment of this problem we endow θ with the Beta prior.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"_, (y, θ) = create_model(coin_toss_model(length(dataset)));","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# As soon as we have a new value for the marginal posterior over the `θ` variable\n# we simply print the first two statistics of it\nθ_subscription = subscribe!(getmarginal(θ), (marginal) -> println(\"New update: mean(θ) = \", mean(marginal), \", std(θ) = \", std(marginal)));","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To pass data to our model we use update! function","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"update!(y, dataset)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"New update: mean(θ) = 0.75049115913556, std(θ) = 0.01916155153543022","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# It is necessary to always unsubscribe from running observables\nunsubscribe!(θ_subscription)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# The ReactiveMP.jl inference backend is lazy and does not compute posterior marginals if no-one is listening for them\n# At this moment we have already unsubscribed from the new posterior updates so this `update!` does nothing\nupdate!(y, dataset)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Rocket.jl provides some useful built-in actors for obtaining posterior marginals especially with static datasets.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# the `keep` actor simply keeps all incoming updates in an internal storage, ordered\nθvalues = keep(Marginal)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"KeepActor{Marginal}(Marginal[])","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# `getmarginal` always emits last cached value as its first value\nsubscribe!(getmarginal(θ) |> take(1), θvalues);","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(θvalues)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"1-element Vector{Marginal}:\n Marginal(Beta{Float64}(α=382.0, β=127.0))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscribe!(getmarginal(θ) |> take(1), θvalues);","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(θvalues)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"2-element Vector{Marginal}:\n Marginal(Beta{Float64}(α=382.0, β=127.0))\n Marginal(Beta{Float64}(α=382.0, β=127.0))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# the `buffer` actor keeps very last incoming update in an internal storage and can also store \n# an array of updates for a sequence of random variables\nθbuffer = buffer(Marginal, 1)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"BufferActor{Marginal, Vector{Marginal}}(Marginal[#undef])","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(θbuffer)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"1-element Vector{Marginal}:\n Marginal(Beta{Float64}(α=382.0, β=127.0))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(θbuffer)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"1-element Vector{Marginal}:\n Marginal(Beta{Float64}(α=382.0, β=127.0))","category":"page"},{"location":"examples/Advanced Tutorial/#Reactive-Online-Inference","page":"Advanced Tutorial","title":"Reactive Online Inference","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl naturally supports reactive streams of data and it is possible to run reactive inference with some external datasource.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function online_coin_toss_model()\n    \n    # We create datavars for the prior \n    # over `θ` variable\n    θ_a = datavar(Float64)\n    θ_b = datavar(Float64)\n    \n    θ ~ Beta(θ_a, θ_b)\n    \n    y = datavar(Float64)\n    y ~ Bernoulli(θ)\n\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"autoupdates = @autoupdates begin \n    θ_a, θ_b = params(q(θ))\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(θ_a,θ_b = params(q(θ)),)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"rxresult = rxinference(\n    model = online_coin_toss_model(),\n    data  = (y = dataset, ),\n    autoupdates = autoupdates,\n    historyvars = (θ = KeepLast(), ),\n    keephistory = length(dataset),\n    initmarginals = (\n        θ = vague(Beta),\n    ),\n    autostart = true\n);","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"animation = @animate for i in 1:length(dataset)\n    plot(mean.(rxresult.history[:θ][1:i]), ribbon = std.(rxresult.history[:θ][1:i]), title = \"Online coin bias inference\", label = \"Inferred bias\", legend = :bottomright)\n    hline!([ p ], label = \"Real bias\", size = (600, 200))\nend\n\ngif(animation, fps = 30)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Plots.AnimatedGif(\"/tmp/jl_tYLYHybkFC.gif\")","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we used static dataset and the history field of the reactive inference result, but the rxinference function also supports any real-time reactive stream and can run indefinitely.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, RxInfer is not limited to only the sum-product algorithm but it also supports variational message passing with Constrained Bethe Free Energy Minimisation.","category":"page"},{"location":"examples/Advanced Tutorial/#Variational-inference","page":"Advanced Tutorial","title":"Variational inference","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"On a very high-level, ReactiveMP.jl is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we approximate our exact posterior marginal distribution by some family of distributions q in mathcalQ. Often this involves assuming some factorization over q. For this purpose the @model macro supports optional where { ... } clauses for every ~ expression in a model specification.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model6_with_manual_constraints(n)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        # Here we assume a mean-field assumption on our \n        # variational family of distributions locally for the current node\n        y[i] ~ Normal(mean = μ, precision = τ) where { q = q(y[i])q(μ)q(τ) }\n    end\n\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we specified an extra constraints for q_a for Bethe factorisation:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nq(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)\nendaligned","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"There are several options to specify the mean-field factorisation constraint. ","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) } # With names from model specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out)q(mean)q(precision) } # With names from node specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = MeanField() } # With alias name","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use local structured factorisation:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i], μ)q(τ) } # With names from model specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out, mean)q(precision) } # With names from node specification","category":"page"},{"location":"examples/Advanced Tutorial/#RxInfer.jl-constraints-macro","page":"Advanced Tutorial","title":"RxInfer.jl constraints macro","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl package exports @constraints macro to simplify factorisation and form constraints specification. Read more about @constraints macro in the corresponding documentation section, here we show a simple example of the same factorisation constraints specification, but with @constraints macro:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"constraints6 = @constraints begin\n     q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Constraints:\n  marginals form:\n  messages form:\n  factorisation:\n    q(μ, τ) = q(μ)q(τ)\nOptions:\n  warn = true","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Note: where blocks have higher priority over constraints specification","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model6(n)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        # Here we assume a mean-field assumption on our \n        # variational family of distributions locally for the current node\n        y[i] ~ Normal(mean = μ, precision = τ)\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"examples/Advanced Tutorial/#Inference","page":"Advanced Tutorial","title":"Inference","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To run inference in this model we again need to create a synthetic dataset:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000);","category":"page"},{"location":"examples/Advanced Tutorial/#inference-function","page":"Advanced Tutorial","title":"inference function","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In order to simplify model and inference testing, RxInfer.jl exports pre-written inference function, that is aimed for simple use cases with static datasets:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Use ?inference to quickly check the documentation for the inference function.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"result = inference(\n    model         = test_model6(length(dataset)),\n    data          = (y = dataset, ),\n    constraints   = constraints6, \n    initmarginals = (μ = vague(NormalMeanPrecision), τ = vague(GammaShapeRate)),\n    returnvars    = (μ = KeepLast(), τ = KeepLast()),\n    iterations    = 10,\n    free_energy   = true,\n    showprogress  = true\n)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inference results:\n  Posteriors       | available for (μ, τ)\n  Free Energy:     | Real[14763.3, 3275.55, 668.849, 628.866, 628.866, 628.\n866, 628.866, 628.866, 628.866, 628.866]","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"μ: mean = \", mean(result.posteriors[:μ]), \", std = \", std(result.posteriors[:μ]))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ: mean = -2.9903274323217657, std = 0.014240810935351763","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"τ: mean = \", mean(result.posteriors[:τ]), \", std = \", std(result.posteriors[:τ]))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"τ: mean = 4.930939544297789, std = 0.2202981321867016","category":"page"},{"location":"examples/Advanced Tutorial/#Manual-inference","page":"Advanced Tutorial","title":"Manual inference","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"For advanced use cases it is advised to write inference functions manually as it provides more flexibility, here is an example of manual inference specification:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, (μ, τ, y) = create_model(test_model6(length(dataset)), constraints = constraints6);","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"For variational inference we also usually need to set initial marginals for our inference procedure. For that purpose ReactiveMP inference engine export the setmarginal! function:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"setmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, vague(GammaShapeRate))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ_values = keep(Marginal)\nτ_values = keep(Marginal)\n\nμ_subscription = subscribe!(getmarginal(μ), μ_values)\nτ_subscription = subscribe!(getmarginal(τ), τ_values)\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(μ_values)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"10-element Vector{Marginal}:\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-2.99631416370503e-9, w=0\n.010000001002000566))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-27.452579491266256, w=9.\n190440750573261))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-9609.015978977592, w=321\n3.3693090725324))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14737.292184910251, w=49\n28.320571707699))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14745.14583974313, w=493\n0.946919184187))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14745.153681874091, w=49\n30.949541677919))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14745.153689700555, w=49\n30.9495442951475))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14745.153689708388, w=49\n30.949544297851))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14745.153689708377, w=49\n30.949544297851))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14745.153689708375, w=49\n30.949544297851))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(τ_values)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"10-element Vector{Marginal}:\n Marginal(GammaShapeRate{Float64}(a=501.0, b=5.0000000000457225e14))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=54572.54325928958))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=155.91160272226125))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=101.65755439117922))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=101.6034088878374))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=101.60335485060882))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=101.60335479667961))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=101.60335479662562))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=101.60335479662567))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=101.6033547966257))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"μ: mean = \", mean(last(μ_values)), \", std = \", std(last(μ_values)))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ: mean = -2.9903274323217657, std = 0.014240810935351763","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"τ: mean = \", mean(last(τ_values)), \", std = \", std(last(τ_values)))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"τ: mean = 4.930939544297789, std = 0.2202981321867016","category":"page"},{"location":"examples/Advanced Tutorial/#Form-constraints","page":"Advanced Tutorial","title":"Form constraints","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In order to support form constraints, the randomvar() function also supports a where { ... } clause with some optional arguments. One of these arguments is form_constraint that allows us to specify a form constraint to the random variables in our model. Another one is prod_constraint that allows to specify an additional constraints during computation of product of two colliding messages. For example we can perform the EM algorithm if we assign a point mass contraint on some variables in our model.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"<img style=\"display: block;   margin-left: auto;   margin-right: auto;   width: 50%;\" src=\"./pics/posterior.png\" />","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model7_with_manual_constraints(n)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    \n    # In case of form constraints `randomvar()` call is necessary\n    μ = randomvar() where { marginal_form_constraint = PointMassFormConstraint() }\n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ Normal(mean = μ, precision = τ) where { q = q(y[i])q(μ)q(τ) }\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"As in the previous example we can use @constraints macro to achieve the same goal with a nicer syntax:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"constraints7 = @constraints begin \n    q(μ) :: PointMass\n    \n    q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Constraints:\n  marginals form:\n    q(μ) :: PointMassFormConstraint() [ prod_constraint = ProdGeneric(fallb\nack = ProdAnalytical()) ]\n  messages form:\n  factorisation:\n    q(μ, τ) = q(μ)q(τ)\nOptions:\n  warn = true","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we specified an extra constraints for q_i for Bethe factorisation:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nq(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)\nendaligned","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model7(n)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    \n    # In case of form constraints `randomvar()` call is necessary\n    μ = randomvar()\n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ Normal(mean = μ, precision = τ)\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, (μ, τ, y) = create_model(test_model7(length(dataset)), constraints = constraints7);","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"setmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, PointMass(1.0))\n\nμ_values = keep(Marginal)\nτ_values = keep(Marginal)\n\nμ_subscription = subscribe!(getmarginal(μ), μ_values)\nτ_subscription = subscribe!(getmarginal(τ), τ_values)\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(μ_values) |> last","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Marginal(PointMass{Float64}(-2.9903274383741154))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(τ_values) |> last","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Marginal(GammaShapeRate{Float64}(a=501.0, b=101.50195444854074))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"By default RxInfer tries to compute an analytical product of two colliding messages and throws an error if no analytical solution is known. However, it is possible to fall back to a generic product that does not require an analytical solution to be known. In this case the inference backend will simply propagate the product of two messages in a form of a tuple. It is not possible to use such a tuple-product during an inference and in this case it is mandatory to use some form constraint to approximate this product.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ = randomvar() where { \n    prod_constraint = ProdGeneric(),\n    form_constraint = SampleListFormConstraint() \n}","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Sometimes it is useful to preserve a specific parametrisation of the resulting product later on in an inference procedure. The ReactiveMP inference engine exports a special prod_constraint called ProdPreserveType especially for that purpose:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ = randomvar() where { prod_constraint = ProdPreserveType(NormalWeightedMeanPrecision) }","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Note: @constraints macro specifies required prod_constraint automatically.","category":"page"},{"location":"examples/Advanced Tutorial/#Free-Energy","page":"Advanced Tutorial","title":"Free Energy","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"During variational inference the RxInfer optimises a special functional called the Bethe Free Energy functional. It is possible to obtain its values for all VMP iterations with the score function.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, (μ, τ, y) = create_model(test_model6(length(dataset)), constraints = constraints6);","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"bfe_observable = score(model, Float64, BetheFreeEnergy())","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"ProxyObservable(Float64, MapProxy(Tuple{ReactiveMP.CountingReal{Float64}, R\neactiveMP.CountingReal{Float64}}))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"bfe_subscription = subscribe!(bfe_observable, (fe) -> println(\"Current BFE value: \", fe));","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Reset the model with vague marginals\nsetmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, vague(GammaShapeRate))\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Current BFE value: 641.3838634439944\nCurrent BFE value: 628.8663684978947\nCurrent BFE value: 628.8663682489773\nCurrent BFE value: 628.8663682489769\nCurrent BFE value: 628.8663682489764\nCurrent BFE value: 628.8663682489764\nCurrent BFE value: 628.866368248976\nCurrent BFE value: 628.8663682489755\nCurrent BFE value: 628.8663682489755\nCurrent BFE value: 628.8663682489755","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# It always necessary to unsubscribe and release computer resources\nunsubscribe!([ μ_subscription, τ_subscription, bfe_subscription ])","category":"page"},{"location":"examples/Advanced Tutorial/#Meta-data-specification","page":"Advanced Tutorial","title":"Meta data specification","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"During model specification some functional dependencies may accept an optional meta object in the where { ... } clause. The purpose of the meta object is to adjust, modify or supply some extra information to the inference backend during the computations of the messages. The meta object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example the `meta` object for the autoregressive `AR` node specifies the variate type of \n# the autoregressive process and its order. In addition it specifies that the message computation rules should\n# respect accuracy over speed with the `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations\n# by cost of possible numerical instabilities during an inference procedure\ns[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Multivariate, order, ARsafe()) }\n...\ns[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Univariate, order, ARunsafe()) }","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Another example with GaussianControlledVariance, or simply GCV [see Hierarchical Gaussian Filter], node:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` \n# method with `21` sigma points for approximation of non-lineariety between hierarchy layers\nxt ~ GCV(xt_min, zt, real_k, real_w) where { q = q(xt, xt_min)q(zt)q(κ)q(ω), meta = GCVMetadata(GaussHermiteCubature(21)) }","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The Meta object is useful to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc.","category":"page"},{"location":"examples/Advanced Tutorial/#RxInfer.jl-@meta-macro","page":"Advanced Tutorial","title":"RxInfer.jl @meta macro","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Users can use @meta macro from the RxInfer.jl package to achieve the same goal. Read more about @meta macro in the corresponding documentation section. Here is a simple example of the same meta specification:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@meta begin \n     AR(s, θ, γ) -> ARMeta(Multivariate, 5, ARsafe())\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Meta specification:\n  AR(s, θ, γ) -> ARMeta{Multivariate, ARsafe}(5, ARsafe())\nOptions:\n  warn = true","category":"page"},{"location":"examples/Advanced Tutorial/#Creating-custom-nodes-and-message-computation-rules","page":"Advanced Tutorial","title":"Creating custom nodes and message computation rules","text":"","category":"section"},{"location":"examples/Advanced Tutorial/#Custom-nodes","page":"Advanced Tutorial","title":"Custom nodes","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To create a custom functional form and to make it available during model specification the ReactiveMP inference engine exports the @node macro:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification\n@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]\n\n# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error\nstruct NormalMeanVariance end \n\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# It is also possible to use function objects as a node functional form\nfunction dot end\n\n# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them \n# out = dot(x, a)\n@node typeof(dot) Deterministic [ out, x, a ]","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Note: Deterministic nodes do not support factorisation constraints with the where { q = ... } clause.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"After that it is possible to use the newly created node during model specification:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model()\n    ...\n    y ~ dot(x, a)\n    ...\nend","category":"page"},{"location":"examples/Advanced Tutorial/#Custom-messages-computation-rules","page":"Advanced Tutorial","title":"Custom messages computation rules","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl exports the @rule macro to create custom message computation rules. For example let us create a simple + node to be available for usage in the model specification usage. We refer to A Factor Graph Approach to Signal Modelling , System Identification and Filtering [ Sascha Korl, 2005, page 32 ] for a rigorous explanation of the + node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for + node is the following:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nmu_z = mu_x + mu_y\nV_z = V_x + V_y\nendaligned","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To specify this in RxInfer.jl we use the @node and @rule macros:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@node typeof(+) Deterministic  [ z, x, y ]\n\n@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin\n    x_mean, x_var = mean_var(m_x)\n    y_mean, y_var = mean_var(m_y)\n    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example, for the @rule macro, we specify a type of our functional form: typeof(+). Next, we specify an edge we are going to compute an outbound message for. Marginalisation indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nq(z) = int q(z x y) mathrmdxmathrmdy\nendaligned","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"<div style=\"width:100%\"> <div style=\"width:50%; left:25%; position: relative; padding-top: 50px; padding-bottom: 50px\"> <img style=\"display: block;   margin-left: auto;   margin-right: auto;   width: 50%;\" src=\"./pics/sp.png\" align=\"left\"/>","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"<img style=\"display: block;   margin-left: auto;   margin-right: auto;   width: 50%;\" src=\"./pics/vmp.png\" align=\"left\" /> </div> <div style=\"width:50%\">&nbsp;&nbsp;&nbsp;&nbsp;</div> </div","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"<div style=\"width:100%\"> beginaligned mu(z) = int f(x y z)mu(x)mu(y)mathrmdxmathrmdy endaligned","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nnu(z) = exp int log f(x y z)q(x)q(y)mathrmdxmathrmdy \nendaligned","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"</div>","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The @rule macro supports both cases with special prefixes during rule specification:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"m_ prefix corresponds to the incoming message on a specific edge\nq_ prefix corresponds to the posterior marginal of a specific edge","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Example of a Sum-Product rule with m_ messages used:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin \n    m_out_mean, m_out_cov = mean_cov(m_out)\n    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Example of a Variational rule with Mean-Field assumption with q_ posteriors used:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin \n    return NormalMeanPrecision(mean(q_out), mean(q_τ))\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl also supports structured rules. It is possible to obtain joint marginal over a set of edges:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin\n    m, V = mean_cov(q_out_μ)\n    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))\n    α = convert(typeof(θ), 1.5)\n    return Gamma(α, θ)\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"NOTE: In the @rule specification the messages or marginals arguments must be in order with interfaces specification from @node macro:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Inference backend expects arguments in `@rule` macro to be in the same order\n@node NormalMeanPrecision Stochastic [ out, μ, τ ]","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Any rule always has access to the meta information with hidden the meta::Any variable:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin \n    ...\n    println(meta)\n    ...\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to dispatch on a specific type of a meta object:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin \n    ...\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"or","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin \n    ...\nend","category":"page"},{"location":"examples/Advanced Tutorial/#Customizing-messages-computational-pipeline","page":"Advanced Tutorial","title":"Customizing messages computational pipeline","text":"","category":"section"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In certain situations it might be convenient to customize the default message computational pipeline. RxInfer.jl supports the pipeline keyword in the where { ... } clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"<img style=\"display: block;   margin-left: auto;   margin-right: auto;   width: 30%;\" src=\"./pics/pipeline.png\" width=\"20%\" />","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Logs all outbound messages\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LoggerPipelineStage() }\n# Initialise messages to be vague\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = InitVaguePipelineStage() }\n# In principle, it is possible to approximate outbound messages with Laplace Approximation\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LaplaceApproximation() }","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Let us return to the coin toss model, but this time we want to print flowing messages:","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function coin_toss_model_log(n)\n\n    y = datavar(Float64, n)\n\n    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(\"θ\") }\n\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(\"y[$i]\") }\n    end\n    \n    return y, θ\nend","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"_, (y, θ) = RxInfer.create_model(coin_toss_model_log(5));","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"θ_subscription = subscribe!(getmarginal(θ), (value) -> println(\"New posterior marginal for θ: \", value));","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"[θ][Beta][out]: VariationalMessage()","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"coinflips = float.(rand(Bernoulli(0.5), 5));","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"update!(y, coinflips)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"[y[1]][Bernoulli][p]: VariationalMessage()\n[y[2]][Bernoulli][p]: VariationalMessage()\n[y[3]][Bernoulli][p]: VariationalMessage()\n[y[4]][Bernoulli][p]: VariationalMessage()\n[y[5]][Bernoulli][p]: VariationalMessage()\nNew posterior marginal for θ: Marginal(Beta{Float64}(α=4.0, β=10.0))","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"unsubscribe!(θ_subscription)","category":"page"},{"location":"examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Inference is lazy and does not send messages if no one is listening for them\nupdate!(y, coinflips)","category":"page"},{"location":"manuals/meta-specification/#user-guide-meta-specification","page":"Meta specification","title":"Meta Specification","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Some nodes in the ReactiveMP.jl inference engine accept optional meta structure that may be used to change or customise the inference procedure or the way node computes outbound messages. As an example GCV node accepts the approximation method that will be used to approximate non-conjugate relationships between variables in this node. RxInfer.jl exports @meta macro to specify node-specific meta and contextual information.","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"using RxInfer","category":"page"},{"location":"manuals/meta-specification/#General-syntax","page":"Meta specification","title":"General syntax","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"@meta macro accepts either regular Julia function or a single begin ... end block. For example both are valid:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"\n@meta function create_meta(arg1, arg2)\n    ...\nend\n\nmymeta = @meta begin \n    ...\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"In the first case it returns a function that returns meta upon calling, e.g. ","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"@meta function create_meta(flag)\n    ...\nend\n\nmymeta = create_meta(true)","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"and in the second case it returns constraints directly.","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"mymeta = @meta begin \n    ...\nend","category":"page"},{"location":"manuals/meta-specification/#Options-specification","page":"Meta specification","title":"Options specification","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"@meta macro accepts optional list of options as a first argument and specified as an array of key = value pairs, e.g. ","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"mymeta = @meta [ warn = false ] begin \n   ...\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"List of available options:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"warn::Bool - enables/disables various warnings with an incompatible model/meta specification","category":"page"},{"location":"manuals/meta-specification/#Meta-specification","page":"Meta specification","title":"Meta specification","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"First, let's start with an example:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"meta = @meta begin \n    GCV(x, k, w) -> GCVMetadata(GaussHermiteCubature(20))\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"This meta specification indicates, that for every GCV node in the model with x, k and w as connected variables should use the GCVMetadata(GaussHermiteCubature(20)) meta object.","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"You can have a list of as many meta specification entries as possible for different nodes:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"meta = @meta begin \n    GCV(x1, k1, w1) -> GCVMetadata(GaussHermiteCubature(20))\n    GCV(x2, k2, w3) -> GCVMetadata(GaussHermiteCubature(30))\n    NormalMeanVariance(y, x) -> MyCustomMetaObject(arg1, arg2)\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"To create a model with extra constraints the user may pass an optional meta keyword argument for the create_model function:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"@model function my_model(arguments...)\n   ...\nend\n\nmeta = @meta begin \n    ...\nend\n\nmodel, returnval = create_model(my_model(arguments...); meta = meta)","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Alternatively, it is possible to use constraints directly in the automatic inference and rxinference functions that accepts meta keyword argument. ","category":"page"},{"location":"manuals/inference/overview/#user-guide-inference-execution","page":"Overview","title":"Inference execution","text":"","category":"section"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The RxInfer inference API supports different types of message-passing algorithms (including hybrid algorithms combining several different types):","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Belief Propagation\nVariational Message Passing","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Whereas belief propagation computes exact inference for the random variables of interest, the variational message passing (VMP) in an approximation method that can be applied to a larger range of models.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The inference engine itself isn't aware of different algorithm types and simply does message passing between nodes, however during model specification stage user may specify different factorisation constraints around factor nodes by using where { q = ... } syntax or with the help of the @constraints macro. Different factorisation constraints lead to a different message passing update rules. See more documentation about constraints specification in the corresponding section.","category":"page"},{"location":"manuals/inference/overview/#user-guide-inference-execution-automatic-specification-static","page":"Overview","title":"Automatic inference specification on static datasets","text":"","category":"section"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"RxInfer exports the inference function to quickly run and test you model with static datasets. See more information about the inference function on the separate documentation section. ","category":"page"},{"location":"manuals/inference/overview/#user-guide-inference-execution-automatic-specification-realtime","page":"Overview","title":"Automatic inference specification on real-time datasets","text":"","category":"section"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"RxInfer exports the rxinference function to quickly run and test you model with dynamic and potentially real-time datasets. See more information about the rxinference function on the separate documentation section. ","category":"page"},{"location":"manuals/inference/overview/#user-guide-inference-execution-manual-specification","page":"Overview","title":"Manual inference specification","text":"","category":"section"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"While both inference and rxinference use most of the RxInfer inference engine capabilities in some situations it might be beneficial to write inference code manually. The Manual inference documentation section explains how to write your custom inference routines.","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Bayesian ARMA/#examples-bayesian-arma-model","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"","category":"section"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"This notebook shows how Bayesian ARMA model can be implemeted in RxInfer.jl. For theoretical details on Varitional Inference for ARMA model, we refer the reader to the following paper. The Bayesian ARMA model can be written as follows:","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"beginaligned\ne_t sim mathcalN(0 gamma^-1) quad\ntheta sim mathcalMN(mathbf0 mathbfI) quad\neta sim mathcalMN(mathbf0 mathbfI) \nmathbfh_0 sim mathcalMNleft(beginbmatrix\ne_-1 \ne_-2\nendbmatrix mathbfIright) \nmathbfh_t = mathbfSmathbfh_t-1 + mathbfc e_t-1 \nmathbfx_t = boldsymboltheta^topmathbfx_t-1 + boldsymboleta^topmathbfh_t + e_t \nendaligned","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"where shift matrix mathbfS is","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"beginaligned\nmathbfS = beginpmatrix\n0  0 \n1  0 \nendpmatrix\nendaligned","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"and unit vector mathbfc: ","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"beginaligned\nmathbfc=1 0\nendaligned","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"when MA order is 2","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"In this way, mathbfh_t containing errors e_t can be viewed as hidden state.","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"In short, the Bayesian ARMA model has two intractabilities: (1) induced by the multiplication of two Gaussian RVs, i.e., boldsymboleta^topmathbfh_t, (2) induced by errors e_t that prevents analytical update of precision parameter gamma (this can be easily seen when constructing the Factor Graph, i.e. there is a loop). Both problems can be easily resolved in RxInfer.jl, by creating a hybrid inference algorithm based on Loopy Variational Message Passing.","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"# Load packages\nusing RxInfer, LinearAlgebra, CSV, DataFrames, Plots","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"# Define shift function\nfunction shift(dim)\n    S = Matrix{Float64}(I, dim, dim)\n    for i in dim:-1:2\n           S[i,:] = S[i-1, :]\n    end\n    S[1, :] = zeros(dim)\n    return S\nend","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"shift (generic function with 1 method)","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"@model function ARMA(n, x_prev, h_prior, γ_prior, τ_prior, η_prior, θ_prior, p, q)\n        \n    c = zeros(p); c[1] = 1.0; # AR\n    b = zeros(q); b[1] = 1.0; # MA\n    S = shift(q); # MA\n    \n    # initialize variables\n    h  = randomvar(n-1)\n    e  = randomvar(n)\n    z  = randomvar(n)\n    \n    x  = datavar(Float64, n) where { allow_missing = true }\n    \n    # priors\n    γ  ~ Gamma(shape = shape(γ_prior), rate = rate(γ_prior))\n    η  ~ MvNormal(mean = mean(η_prior), precision = precision(η_prior))\n    θ  ~ MvNormal(mean = mean(θ_prior), precision = precision(θ_prior))\n    τ  ~ Gamma(shape = shape(τ_prior), rate = rate(τ_prior))\n    \n    # initial\n    h_0  ~ MvNormal(mean = mean(h_prior), precision = precision(h_prior))\n    z[1] ~ AR(h_0, η, τ) where { meta = ARMeta(Multivariate, q, ARsafe()) }\n    e[1] ~ Normal(mean = 0.0, precision = γ)\n\n    x[1] ~ dot(b, z[1]) + dot(θ, x_prev[1]) + e[1]\n    \n    h_prev = h_0\n    for t in 1:n-1\n        \n        e[t+1] ~ Normal(mean = 0.0, precision = γ)\n        h[t] ~ S*h_prev + b*e[t]\n        z[t+1] ~ AR(h[t], η, τ) where { meta = ARMeta(Multivariate, q, ARsafe()) }\n        x[t+1] ~ dot(z[t+1], b) + dot(θ, x_prev[t]) + e[t+1]\n        h_prev = h[t]\n    end\nend","category":"page"},{"location":"examples/Bayesian ARMA/#Load-dataset","page":"Bayesian ARMA model","title":"Load dataset","text":"","category":"section"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"To validate our model and inference, we will use American Airlines stock data downloaded from Kaggle","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"Once the dataset is downloaded, we can load it into DataFrame.","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"x_df = CSV.read(\"data/arma/aal_stock.csv\", DataFrame)","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"1259×7 DataFrame\n  Row │ date        open     high     low      close    volume    Name\n      │ Date        Float64  Float64  Float64  Float64  Int64     String3\n──────┼───────────────────────────────────────────────────────────────────\n    1 │ 2013-02-08    15.07    15.12   14.63     14.75   8407500  AAL\n    2 │ 2013-02-11    14.89    15.01   14.26     14.46   8882000  AAL\n    3 │ 2013-02-12    14.45    14.51   14.1      14.27   8126000  AAL\n    4 │ 2013-02-13    14.3     14.94   14.25     14.66  10259500  AAL\n    5 │ 2013-02-14    14.94    14.96   13.16     13.99  31879900  AAL\n    6 │ 2013-02-15    13.93    14.61   13.93     14.5   15628000  AAL\n    7 │ 2013-02-19    14.33    14.56   14.08     14.26  11354400  AAL\n    8 │ 2013-02-20    14.17    14.26   13.15     13.33  14725200  AAL\n  ⋮   │     ⋮          ⋮        ⋮        ⋮        ⋮        ⋮         ⋮\n 1253 │ 2018-01-30    52.45    53.05   52.36     52.59   4741808  AAL\n 1254 │ 2018-01-31    53.08    54.71   53.0      54.32   5962937  AAL\n 1255 │ 2018-02-01    54.0     54.64   53.59     53.88   3623078  AAL\n 1256 │ 2018-02-02    53.49    53.99   52.03     52.1    5109361  AAL\n 1257 │ 2018-02-05    51.99    52.39   49.75     49.76   6878284  AAL\n 1258 │ 2018-02-06    49.32    51.5    48.79     51.18   6782480  AAL\n 1259 │ 2018-02-07    50.91    51.98   50.89     51.4    4845831  AAL\n                                                         1244 rows omitted","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"# we will use \"close\" column\nx_data = filter(!ismissing, x_df[:, 5]);","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"# Plot data\nplot(x_data, xlabel=\"day\", ylabel=\"price\", label=false)","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"(Image: )","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"p_order = 10 # AR\nq_order = 4 # MA","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"4","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"# Training set\ntrain_size = 1000\nx_prev_train = [Float64.(x_data[i+p_order-1:-1:i]) for i in 1:length(x_data)-p_order][1:train_size]\nx_train = Float64.(x_data[p_order+1:end])[1:train_size];","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"# Test set\nx_prev_test = [Float64.(x_data[i+p_order-1:-1:i]) for i in 1:length(x_data)-p_order][train_size+1:end]\nx_test = Float64.(x_data[p_order+1:end])[train_size+1:end];","category":"page"},{"location":"examples/Bayesian ARMA/#Inference","page":"Bayesian ARMA model","title":"Inference","text":"","category":"section"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"# Constraints are needed for performing VMP\narma_constraints = @constraints begin\n    q(z, h_0, h, η, τ, γ,e) = q(h_0)q(z, h)q(η)q(τ)q(γ)q(e)\nend;","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"# This cell defines prior knowledge for model parameters\nh_prior = MvNormalMeanPrecision(zeros(q_order), diageye(q_order))\nγ_prior = GammaShapeRate(1e4, 1.0)\nτ_prior = GammaShapeRate(1e2, 1.0)\nη_prior = MvNormalMeanPrecision(zeros(q_order), diageye(q_order))\nθ_prior = MvNormalMeanPrecision(zeros(p_order), diageye(p_order));","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"# Model's graph has structural loops, hence, it requires pre-initialisation\narma_imarginals = (h_0 = h_prior, h = h_prior, γ = γ_prior, τ = τ_prior, η = η_prior, θ = θ_prior);\narma_imessages  = (h_0 = h_prior, h = h_prior);","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"result = inference(\n    model = ARMA(length(x_train), x_prev_train, h_prior, γ_prior, τ_prior, η_prior, θ_prior, p_order, q_order), \n    data  = (x = x_train, ),\n    initmarginals = arma_imarginals,\n    initmessages  = arma_imessages,\n    constraints   = arma_constraints,\n    iterations    = 50,\n    options       = (limit_stack_depth = 100, ),\n);","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"plot(mean.(result.posteriors[:e][end]), ribbon=std.(result.posteriors[:e][end]))","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"(Image: )","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"# extract posteriors\nh_posterior = result.posteriors[:h][end][end]\nγ_posterior = result.posteriors[:γ][end]\nτ_posterior = result.posteriors[:τ][end]\nη_posterior = result.posteriors[:η][end]\nθ_posterior = result.posteriors[:θ][end];","category":"page"},{"location":"examples/Bayesian ARMA/#Prediction","page":"Bayesian ARMA model","title":"Prediction","text":"","category":"section"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"Here we are going to use our inference results in order to predict the dataset itself","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"# The prediction function is aimed at approximating the predictive posterior distribution\n# It triggers the rules in the generative order (in future, RxInfer.jl will provide this function out of the box)\nfunction prediction(x_prev, h_posterior, γ_posterior, τ_posterior, η_posterior, θ_posterior, p, q)\n    h_out = MvNormalMeanPrecision(mean(h_posterior), precision(h_posterior))\n    ar_out = @call_rule AR(:y, Marginalisation) (m_x=h_out, q_θ=η_posterior, q_γ=τ_posterior, meta=ARMeta(Multivariate, p, ARsafe()))\n    c = zeros(p); c[1] = 1.0\n    b = zeros(q); b[1] = 1.0\n    ar_dot_out = @call_rule typeof(dot)(:out, Marginalisation) (m_in1=PointMass(b), m_in2=ar_out, meta=ReactiveMP.TinyCorrection())\n    θ_out = MvNormalMeanPrecision(mean(θ_posterior), precision(θ_posterior))\n    ma_dot_out = @call_rule typeof(dot)(:out, Marginalisation) (m_in1=PointMass(x_prev), m_in2=θ_out, meta=ReactiveMP.TinyCorrection())\n    e_out = @call_rule NormalMeanPrecision(:out, Marginalisation) (q_μ=PointMass(0.0), q_τ=mean(γ_posterior))\n    ar_ma = @call_rule typeof(+)(:out, Marginalisation) (m_in1=ar_dot_out, m_in2=ma_dot_out)  \n    @call_rule typeof(+)(:out, Marginalisation) (m_in1=ar_ma, m_in2=e_out)  \nend","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"prediction (generic function with 1 method)","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"predictions = []\nfor x_prev in x_prev_test\n    push!(predictions, prediction(x_prev, h_posterior, γ_posterior, τ_posterior, η_posterior, θ_posterior, p_order, q_order))\n    # after every new prediction we can actually \"retrain\" the model to use the power of Bayesian approach\n    # we will skip this part for now\nend","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"plot(x_test, label=\"test data\", legend=:topleft)\nplot!(mean.(predictions)[1:end], ribbon=std.(predictions)[1:end], label=\"predicted\", xlabel=\"day\", ylabel=\"price\")","category":"page"},{"location":"examples/Bayesian ARMA/","page":"Bayesian ARMA model","title":"Bayesian ARMA model","text":"(Image: )","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Coin Toss Model/#examples-coin-toss-model-(beta-bernoulli)","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"","category":"section"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"In this example, we are going to perform an exact inference for a coin toss model that can be represented as:","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"beginaligned\np(theta) = mathrmBeta(thetaa b)\np(y_itheta) = mathrmBer(y_itheta)\nendaligned","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"where y_i in 0 1 is a binary observation induced by Bernoulli likelihood while theta is a Beta prior distribution on the parameter of Bernoulli. We are interested in inferring the posterior distribution of theta.","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"We start with importing all needed packages:","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"using RxInfer, Random","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"Let's generate some synthetic dataset with IID observations from Bernoulli distribution, that represents our coin tosses. We also assume that our coin is biased:","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"rng = MersenneTwister(42)\nn = 500\nθ_real = 0.75\ndistribution = Bernoulli(θ_real)\n\ndataset = float.(rand(rng, Bernoulli(θ_real), n));","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"# GraphPPL.jl export `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function coin_model(n)\n\n    # `datavar` creates data 'inputs' in our model\n    # We will pass data later on to these inputs\n    # In this example we create a sequence of inputs that accepts Float64\n    y = datavar(Float64, n)\n\n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(4.0, 8.0)\n    # or, in this particular case, the `Uniform(0.0, 1.0)` prior also works:\n    # θ ~ Uniform(0.0, 1.0)\n\n    # We assume that outcome of each coin flip is governed by the Bernoulli distribution\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)\n    end\n\nend","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"result = inference(\n    model = coin_model(length(dataset)), \n    data  = (y = dataset, )\n)","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"θestimated = result.posteriors[:θ]","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"Beta{Float64}(α=365.0, β=147.0)","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"using Plots\n\nrθ = range(0, 1, length = 1000)\n\np = plot(title = \"Inference results\")\n\nplot!(rθ, (x) -> pdf(Beta(2.0, 7.0), x), fillalpha=0.3, fillrange = 0, label=\"P(θ)\", c=1,)\nplot!(rθ, (x) -> pdf(θestimated, x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y)\", c=3)\nvline!([θ_real], label=\"Real θ\")","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"(Image: )","category":"page"},{"location":"examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/GPRegression by SSM/#examples-solve-gp-regression-by-sde","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"","category":"section"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"In this notebook, we solve a GP regression problem by using \"Stochastic Differential Equation\" (SDE). This method is well described in the dissertation \"Stochastic differential equation methods for spatio-temporal Gaussian process regression.\" by Arno Solin and \"Sequential Inference for Latent Temporal Gaussian Process Models\" by Jouni Hartikainen. The idea of the method is as follows.","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Suppose a function f(x) follows a zero-mean Gaussian Process beginaligned f(x) sim mathcalGP(0 k(xx)) endaligned","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"When the dimensionality of x is 1, we can consider f(x) as a stochastic process over time, i.e. f(t). For a certain classses of covariance functions, f(t) is a solution to an m-th order linear stochastic differential equation (SDE) beginaligned a_0 f(t) + a_1 fracd f(t)dt + dots + a_m fracd^m f(t)dt^m = w(t)  endaligned","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where w(t) is a zero-mean white noise process with spectral density Q_c. If we define a vector-valued function mathbff(t) = (f(t) ddt f(t)dots d^m-1dt^m-1f(t)), then we can rewrite the above SDE under the companion form","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nfracd mathbff(t)dt = mathbfF mathbff(t) + mathbfL w(t) quad (1)\nendaligned","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where mathbfF and mathbfL are defined based on the choice of covariance functions.  From (1), we have the following state-space model: beginaligned mathbff_k = mathbfA_k-1  mathbff_k-1 + mathbfq_k-1 quad mathbfq_k-1 sim mathcalN(mathbf0 mathbfQ_k-1) quad(2a) \ny_k = mathbfH  mathbff(t_k) + epsilon_k  quad epsilon_k sim mathcalN(0 sigma^2_noise) quad(2b) \nendaligned","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where mathbfA_k = exp(mathbfFDelta t_k), with Delta t_k = t_k+1 - t_k, is called the discrete-time state transition matrix, and mathbfQ_k the process noise covariance matrix. For the computation of mathbfQ_k, we will come back later. According to Arno Solin and Jouni Hartikainen's dissertation, the GP regression problem amounts to the inference problem of the above state-space model, and this can be solved by RTS-smoothing. The state-space model starts from  the initial state f_0 sim mathcalN(mathbf0 mathbfP_0). For stationary covariance function, the SDE has a stationary state f_infty sim mathcalN(mathbf0 mathbfP_infty), where mathbfP_infty is the solution to beginaligned fracdmathbfP_inftydt = mathbfF mathbfP_infty + mathbfP_infty mathbfF^T + mathbfL mathbfQ_c mathbfL^T = 0 quad (mathrmLyapunov  equation) endaligned","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"With this stationary condition, the process noise covariance mathbfQ_k is computed as follows beginaligned mathbfQ_k = mathbfP_infty - mathbfA_k mathbfP_infty mathbfA_k^T  endaligned","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"For one-dimensional problem the SDE representation of the GP is defined by the matrices mathbfF  mathbfL  mathbfQ_c  mathbfP_0 and mathbfH. Once we obtain all the matrices, we can do GP regression by implementing RTS-smoothing on the state-space model (2). In this notebook we will particularly use the Matern class of covariance functions for Gaussian Process.","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"using RxInfer, Random, Distributions, LinearAlgebra, Plots","category":"page"},{"location":"examples/GPRegression by SSM/#Create-state-space-model-for-GP-regression","page":"Solve GP regression by SDE","title":"Create state space model for GP regression","text":"","category":"section"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Here we create a state-space model beginaligned mathbff_k = mathbfA_k-1  mathbff_k-1 + mathbfq_k-1 quad mathbfq_k-1 sim mathcalN(mathbf0 mathbfQ_k-1) \ny_k = mathbfH  mathbff(t_k) + epsilon_k  quad epsilon_k sim mathcalN(0 sigma^2_noise) \nendaligned where y_k is the noisy observation of the function f at time t_k, and sigma^2_noise is the noise variance and assumed to be known.","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"@model function gp_regression(n, P∞, A, Q, H, σ²_noise)\n    f_0 ~ MvNormalMeanCovariance(zeros(length(H)), P∞)\n    f = randomvar(n)\n    y = datavar(Float64, n) where { allow_missing = true }\n    \n    f_prev = f_0\n\n    for i=1:n\n        f[i] ~ MvNormalMeanCovariance(A[i] * f_prev, Q[i])\n        y[i] ~ NormalMeanVariance(dot(H , f[i]), σ²_noise)\n        f_prev = f[i]\n    end\n    return f, y\nend","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Since we only observe the (noisy) function value at some time instances on the timeline, the data variable y contains textitmissing values and this requires us to add update rule for missing messages.","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"@rule MvNormalMeanCovariance(:μ, Marginalisation) (m_out::Missing, q_Σ::PointMass, ) = missing\n\n@rule NormalMeanVariance(:μ, Marginalisation) (q_out::Missing, q_v::PointMass) = missing\n\n@rule typeof(*)(:in, Marginalisation) (m_out::Missing, m_A::PointMass, meta::TinyCorrection) = missing\n\n@rule typeof(dot)(:in2, Marginalisation) (m_out::Missing, m_in1::PointMass, meta::TinyCorrection) = missing","category":"page"},{"location":"examples/GPRegression by SSM/#Generate-data","page":"Solve GP regression by SDE","title":"Generate data","text":"","category":"section"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Random.seed!(10)\nn = 100\nσ²_noise = 0.04;\nt = collect(range(-2, 2, length=n)); #timeline\nf_true = sinc.(t); # true process\nf_noisy = f_true + sqrt(σ²_noise)*randn(n); #noisy process\n\npos = sort(randperm(75)[1:2:75]); \nt_obser = t[pos]; # time where we observe data\n\ny_data = Array{Union{Float64,Missing}}(missing, n)\nfor i in pos \n    y_data[i] = f_noisy[i]\nend\n\nθ = [1., 1.]; # store [l, σ²]\nΔt = [t[1]]; # time difference\nappend!(Δt, t[2:end] - t[1:end-1]);","category":"page"},{"location":"examples/GPRegression by SSM/#Let's-visualize-our-data","page":"Solve GP regression by SDE","title":"Let's visualize our data","text":"","category":"section"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"plot(t, f_true, label=\"True process f(t)\")\nscatter!(t_obser, y_data[pos], label = \"Noisy observations\")\nxlabel!(\"t\")\nylabel!(\"f(t)\")","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"(Image: )","category":"page"},{"location":"examples/GPRegression by SSM/#Covariance-function:-Matern-3/2","page":"Solve GP regression by SDE","title":"Covariance function: Matern-3/2","text":"","category":"section"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"The Matern is a stationary covariance function and defined as follows beginaligned k(tau) = sigma^2 frac2^1-nuGamma(nu) left(fracsqrt2nutaul right)^nu K_nuleft(fracsqrt2nutaul right) endaligned where  beginaligned sigma^2 textthe magnitude scale hyperparameter\nl textthe characteristic length-scale\nnu textthe smoothness hyperparameter\nK_nu() textthe modified Bessel function of the second kind endaligned When we say the Matern-3/2, we mean nu=32. The matrices for the state space model are computed as follows beginaligned mathbfF = beginpmatrix 0  1\n-lambda^2  -2lambda endpmatrix quad quad mathbfL = beginpmatrix 0  1 endpmatrix quad quad mathbfP_infty = beginpmatrix sigma^2  0  0  lambda^2sigma^2 endpmatrix quad quad mathbfH = beginpmatrix 1  0 endpmatrix quad quad Q_c = 4lambda^3sigma^2 endaligned  where lambda = fracsqrt3l  From these matrices we can define mathbfA_k and mathbfQ_k.","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"λ = sqrt(3)/θ[1];\n#### compute matrices for the state-space model ######\nL = [0., 1.];\nH = [1., 0.];\nF = [0. 1.; -λ^2 -2λ]\nP∞ = [θ[2] 0.; 0. (λ^2*θ[2]) ]\nA = [exp(F * i) for i in Δt]; \nQ = [P∞ - i*P∞*i' for i in A];","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"result_32 = inference(\n    model = gp_regression(n, P∞, A, Q, H, σ²_noise),\n    data = (y = y_data,)\n)","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Inference results:\n  Posteriors       | available for (f, f_0)","category":"page"},{"location":"examples/GPRegression by SSM/#Covariance-function:-Matern-5/2","page":"Solve GP regression by SDE","title":"Covariance function: Matern-5/2","text":"","category":"section"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Now let's try the Matern-5/2 kernel. The matrices for the SDE representation of the Matern-5/2 are:","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nmathbfF = beginpmatrix\n0  1  0\n0  0  1 \n-lambda^3  -3lambda^2  -3lambda\nendpmatrix quad quad mathbfL = beginpmatrix\n0  0  1\nendpmatrix quad quad mathbfH = beginpmatrix\n1  0  0\nendpmatrix quad quad Q_c = frac163 sigma^2 lambda^5 \nendaligned","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where lambda = sqrt5  l. To find mathbfP_infty, we solve the Lyapunov equation","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nfracdmathbfP_inftydt = mathbfF mathbfP_infty + mathbfP_infty mathbfF^T + mathbfL mathbfQ_c mathbfL^T = 0\nendaligned","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"of which the solution is","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nvec(mathbfP_infty) = (mathbfI otimes mathbfF + mathbfFotimesmathbfI)^-1 vec(-mathbfLQ_cmathbfL^T)\nendaligned","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where vec() is the vectorization operator and otimes denotes the Kronecker product. Now we can find mathbfA_k and mathbfQ_k ","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nmathbfA_k = exp(mathbfFDelta t_k) \nendaligned","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nmathbfQ_k = mathbfP_infty - mathbfA_k mathbfP_infty mathbfA_k^T  \nendaligned","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"λ = sqrt(5)/θ[1];\n#### compute matrices for the state-space model ######\nL = [0., 0., 1.];\nH = [1., 0., 0.];\nF = [0. 1. 0.; 0. 0. 1.;-λ^3 -3λ^2 -3λ]\nQc = 16/3 * θ[2] * λ^5;\n\nI = diageye(3) ; \nvec_P = inv(kron(I,F) + kron(F,I)) * vec(-L * Qc * L'); \nP∞ = reshape(vec_P,3,3);\nA = [exp(F * i) for i in Δt]; \nQ = [P∞ - i*P∞*i' for i in A];","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"result_52 = inference(\n    model = gp_regression(n, P∞, A, Q, H, σ²_noise),\n    data = (y = y_data,)\n)","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Inference results:\n  Posteriors       | available for (f, f_0)","category":"page"},{"location":"examples/GPRegression by SSM/#Result","page":"Solve GP regression by SDE","title":"Result","text":"","category":"section"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"slicedim(dim) = (a) -> map(e -> e[dim], a)\n\nplot(t, mean.(result_32.posteriors[:f]) |> slicedim(1), ribbon = var.(result_32.posteriors[:f]) |> slicedim(1) .|> sqrt, label =\"Approx. process_M32\", title = \"Matern-3/2\", legend =false, lw = 2)\nplot!(t, mean.(result_52.posteriors[:f]) |> slicedim(1), ribbon = var.(result_52.posteriors[:f]) |> slicedim(1) .|> sqrt, label =\"Approx. process_M52\",legend = :bottomleft, title = \"GPRegression by SSM\", lw = 2)\nplot!(t, f_true,label=\"true process\", lw = 2)\nscatter!(t_obser, f_noisy[pos], label=\"Observations\")\nxlabel!(\"t\")\nylabel!(\"f(t)\")","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"(Image: )","category":"page"},{"location":"examples/GPRegression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"As we can see from the plot, both cases of Matern kernel provide good approximations (small variance) to the true process at the area with dense observations (namely from t = 0 to around 3.5), and when we move far away from this region the approximated processes become less accurate (larger variance). This result makes sense because GP regression exploits the correlation between observations to predict unobserved points, and the choice of covariance functions as well as their hyper-parameters might not be optimal. We can increase the accuracy of the approximated processes by simply adding more observations. This way of improvement does not trouble the state-space method much but it might cause computational problem for naive GP regression, because with N observations the complexity of naive GP regression scales with N^3 while the state-space method scales linearly with N.     ","category":"page"},{"location":"library/functional-forms/#lib-forms","page":"Built-in functional form constraints","title":"Built-in Functional Forms","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"This section describes built-in functional forms that can be used for posterior marginal and/or messages form constraints specification. Read more information about constraints specification syntax in the Constraints Specification section.","category":"page"},{"location":"library/functional-forms/#lib-forms-custom-constraints","page":"Built-in functional form constraints","title":"Custom functional forms","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"See the ReactiveMP.jl library documentation for more information about defining novel custom functional forms that are compatible with ReactiveMP inference backend.","category":"page"},{"location":"library/functional-forms/#lib-forms-unspecified-constraint","page":"Built-in functional form constraints","title":"UnspecifiedFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"Unspecified functional form constraint is used by default and uses only analytical update rules for computing posterior marginals. Throws an error if a product of two colliding messages cannot be computed analytically.","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"@constraints begin \n    q(x) :: Nothing # This is the default setting\nend","category":"page"},{"location":"library/functional-forms/#lib-forms-point-mass-constraint","page":"Built-in functional form constraints","title":"PointMassFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"The most basic form of posterior marginal approximation is the PointMass function. In a few words PointMass represents delta function. In the context of functional form constraints PointMass approximation corresponds to the MAP estimate. For a given distribution d - PointMass functional form simply finds the argmax of the logpdf(d, x) by default. ","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"@constraints begin \n    q(x) :: PointMass # Materializes to the `PointMassFormConstraint` object\nend","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"RxInfer.PointMassFormConstraint","category":"page"},{"location":"library/functional-forms/#RxInfer.PointMassFormConstraint","page":"Built-in functional form constraints","title":"RxInfer.PointMassFormConstraint","text":"PointMassFormConstraint\n\nOne of the form constraint objects. Constraint a message to be in a form of dirac's delta point mass.  By default uses Optim.jl package to find argmin of -logpdf(x).  Accepts custom optimizer callback which might be used to customise optimisation procedure with different packages  or different arguments for Optim.jl package.\n\nKeyword arguments\n\noptimizer: specifies a callback function for logpdf optimisation. See also: ReactiveMP.default_point_mass_form_constraint_optimizer\nstarting_point: specifies a callback function for initial optimisation point: See also: ReactiveMP.default_point_mass_form_constraint_starting_point\nboundaries: specifies a callback function for determining optimisation boundaries: See also: ReactiveMP.default_point_mass_form_constraint_boundaries\n\nCustom optimizer callback interface\n\n# This is an example of the `custom_optimizer` interface\nfunction custom_optimizer(::Type{ Univariate }, ::Type{ Continuous }, constraint::PointMassFormConstraint, distribution)\n    # should return argmin of the -logpdf(distribution)\nend\n\nCustom starting point callback interface\n\n# This is an example of the `custom_starting_point` interface\nfunction custom_starting_point(::Type{ Univariate }, ::Type{ Continuous }, constraint::PointMassFormConstraint, distribution)\n    # built-in optimizer expects an array, even for a univariate distribution\n    return [ 0.0 ] \nend\n\nCustom boundaries callback interface\n\n# This is an example of the `custom_boundaries` interface\nfunction custom_boundaries(::Type{ Univariate }, ::Type{ Continuous }, constraint::PointMassFormConstraint, distribution)\n    # returns a tuple of `lower` and `upper` boundaries\n    return (-Inf, Inf)\nend\n\nTraits\n\nis_point_mass_form_constraint = true\ndefault_form_check_strategy   = FormConstraintCheckLast()\ndefault_prod_constraint       = ProdGeneric()\nmake_form_constraint          = PointMass (for use in @constraints macro)\n\nSee also: ReactiveMP.constrain_form, ReactiveMP.DistProduct\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#lib-forms-sample-list-constraint","page":"Built-in functional form constraints","title":"SampleListFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"SampleListFormConstraints approximates the resulting posterior marginal (product of two colliding messages) as a list of weighted samples. Hence, it requires one of the arguments to be a proper distribution (or at least be able to sample from it). This setting is controlled with LeftProposal(), RightProposal() or AutoProposal() objects. It also accepts an optional method object, but the only one available sampling method currently is the BootstrapImportanceSampling.","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"@constraints begin \n    q(x) :: SampleList(1000)\n    # or \n    q(y) :: SampleList(1000, LeftProposal())\nend","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"RxInfer.SampleListFormConstraint","category":"page"},{"location":"library/functional-forms/#RxInfer.SampleListFormConstraint","page":"Built-in functional form constraints","title":"RxInfer.SampleListFormConstraint","text":"SampleListFormConstraint(rng, strategy, method)\n\nOne of the form constraint objects. Approximates DistProduct with a SampleList object. \n\nTraits\n\nis_point_mass_form_constraint = false\ndefault_form_check_strategy   = FormConstraintCheckLast()\ndefault_prod_constraint       = ProdGeneric()\nmake_form_constraint          = SampleList (for use in @constraints macro)\n\nSee also: ReactiveMP.constrain_form, ReactiveMP.DistProduct\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#lib-forms-fixed-marginal-constraint","page":"Built-in functional form constraints","title":"FixedMarginalFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"Fixed marginal form constraint replaces the resulting posterior marginal obtained during the inference procedure with the prespecified one. Worth to note that the inference backend still tries to compute real posterior marginal and may fail during this process. Might be useful for debugging purposes. If nothing is passed then the computed posterior marginal is returned.","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"@constraints function block_updates(x_posterior = nothing) \n    # `nothing` returns the computed posterior marginal\n    q(x) :: Marginal(x_posterior)\nend","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"RxInfer.FixedMarginalFormConstraint","category":"page"},{"location":"library/functional-forms/#RxInfer.FixedMarginalFormConstraint","page":"Built-in functional form constraints","title":"RxInfer.FixedMarginalFormConstraint","text":"FixedMarginalFormConstraint\n\nOne of the form constraint objects. Provides a constraint on the marginal distribution such that it remains fixed during inference.  Can be viewed as blocking of updates of a specific edge associated with the marginal. If nothing is passed then the computed posterior marginal is returned.\n\nTraits\n\nis_point_mass_form_constraint = false\ndefault_form_check_strategy   = FormConstraintCheckLast()\ndefault_prod_constraint       = ProdAnalytical()\nmake_form_constraint          = Marginal (for use in @constraints macro)\n\nSee also: ReactiveMP.constrain_form, ReactiveMP.DistProduct\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#lib-forms-composite-constraint","page":"Built-in functional form constraints","title":"CompositeFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"It is possible to create a composite functional form constraint with either + operator or using @constraints macro, e.g:","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"form_constraint = SampleListFormConstraint(1000) + PointMassFormConstraint()","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"@constraints begin \n    q(x) :: SampleList(1000) :: PointMass()\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = RxInfer","category":"page"},{"location":"#RxInfer","page":"Home","title":"RxInfer","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"<div class=\"light-biglogo\">","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: RxInfer Logo)","category":"page"},{"location":"","page":"Home","title":"Home","text":"</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div class=\"dark-biglogo\">","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: RxInfer Logo)","category":"page"},{"location":"","page":"Home","title":"Home","text":"</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"Julia package for automatic Bayesian inference on a factor graph with reactive message passing.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Given a probabilistic model, RxInfer allows for an efficient message-passing based Bayesian inference. It uses the model structure to generate an algorithm that consists of a sequence of local computations on a Forney-style factor graph (FFG) representation of the model. RxInfer.jl has been designed with a focus on efficiency, scalability and maximum performance for running inference with message passing.","category":"page"},{"location":"#Package-Features","page":"Home","title":"Package Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"User friendly syntax for specification of probabilistic models.\nAutomatic generation of message passing algorithms including\nBelief propagation\nVariational message passing\nExpectation maximization\nSupport for hybrid models combining discrete and continuous latent variables.\nSupport for hybrid distinct message passing inference algorithm under a unified paradigm.\nFactorisation and functional form constraints specification.\nEvaluation of Bethe free energy as a model performance measure.\nSchedule-free reactive message passing API.\nHigh performance.\nScalability for large models with millions of parameters and observations.\nInference procedure is differentiable.\nEasy to extend with custom nodes and message update rules.","category":"page"},{"location":"#Ecosystem","page":"Home","title":"Ecosystem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The RxInfer unites 3 core packages into one powerful reactive message passing-based Bayesian inference framework:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ReactiveMP.jl - core package for efficient and scalable for reactive message passing \nGraphPPL.jl - package for model and constraints specification\nRocket.jl - reactive programming tools","category":"page"},{"location":"#How-to-get-started?","page":"Home","title":"How to get started?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Head to the Getting started section to get up and running with RxInfer. Alternatively, explore various examples in the documentation.","category":"page"},{"location":"#Table-of-Contents","page":"Home","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n  \"manuals/background.md\",\n  \"manuals/getting-started.md\",\n  \"manuals/model-specification.md\",\n  \"manuals/constraints-specification.md\",\n  \"manuals/meta-specification.md\",\n  \"manuals/inference-execution.md\",\n  \"examples/overview.md\",\n  \"library/exported-methods.md\",\n  \"library/functional-forms.md\",\n  \"contributing/overview.md\",\n  \"contributing/new-example.md\"\n]\nDepth = 2","category":"page"},{"location":"#Resources","page":"Home","title":"Resources","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For an introduction to message passing and FFGs, see The Factor Graph Approach to Model-Based Signal Processing by Loeliger et al. (2007).","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/#examples-nonlinear-sensor-fusion","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"section"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate(); \nusing RxInfer, Random, LinearAlgebra, Distributions, Plots, StatsPlots, Flux, DataFrames, DelimitedFiles, StableRNGs","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"In a secret ongoing mission to Mars, NASA has deployed its custom lunar roving vehicle, called WALL-E, to explore the area and to discover hidden minerals. During one of the solar storm, WALL-E's GPS unit got damaged, preventing it from accurately locating itself. The engineers at NASA were devastated as they developed the project over the past couple of years and spend most of their funding on it. Without being able to locate WALL-E, they were unable to complete their mission.","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"A smart group of engineers came up with a solution to locate WALL-E. They decided to repurpose 3 nearby satelites as beacons for WALL-E, allowing it to detect its relative location to these beacons. However, these satelites were old and therefore WALL-E was only able to obtain noisy estimates of its distance to these beacons. These distances were communicated back to earth, where the engineers tried to figure our WALL-E's location. Luckily they knew the locations of these satelites and together with the noisy estimates of the distance to WALL-E they can infer the exact location of the moving WALL-E.","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"To illustrate these noisy measurements, the engineers decided to plot them:","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# fetch measurements\nbeacon_locations = readdlm(\"data/sensor_fusion/beacons.txt\")\ndistances = readdlm(\"data/sensor_fusion/distances.txt\")\nposition = readdlm(\"data/sensor_fusion/position.txt\")\nnr_observations = size(distances, 1);","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual location of WALL-E\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\")\n\n# plot noisy distance measurements\np2 = plot(distances, legend=:topleft, linewidth=3, label=[\"distance to beacon 1\" \"distance to beacon 2\" \"distance to beacon 3\"])\nxlabel!(\"time [sec]\"), ylabel!(\"distance [m]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"In order to track the location of WALL-E based on the noisy distance measurements to the beacon, the engineers developed a probabilistic model for the movements for WALL-E and the distance measurements that followed from this. The engineers assumed that the position of WALL-E at time t, denoted by z_t, follows a 2-dimensional normal random walk:","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(z_t mid z_t - 1) = mathcalN(z_t mid z_t-1mathrmI_2)\nendaligned","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"where mathrmI_2 denotes the 2-dimensional identity matrix. From the current position of WALL-E, we specify our noisy distance measurements y_t as a noisy set of the distances between WALL-E and the beacons, specified by s_i:","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(y_t mid z_t)  = mathcalN left (y_t left vert beginbmatrix  z_t - s_1  z_t - s_2  z_t - s_3endbmatrixmathrmI_3 right  right)\nendaligned","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers are smart enough to automate the probabilistic inference procedure using RxInfer.jl. They specify the probabilistic model as:","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# function to compute distance to beacons\nfunction compute_distances(z)    \n    distance1 = norm(z - beacon_locations[1,:])\n    distance2 = norm(z - beacon_locations[2,:])\n    distance3 = norm(z - beacon_locations[3,:])\n    distances = [distance1, distance2, distance3]\nend;","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@model function random_walk_model(nr_observations)\n    \n    # specify covariance matrices\n    W = diageye(2)\n    R = diageye(3)\n\n    # allocate locations and observations\n    z = randomvar(nr_observations)\n    y = datavar(Vector{Float64}, nr_observations)\n\n    # specify initial estimates of the location\n    z[1] ~ MvNormalMeanCovariance(zeros(2), diageye(2)) \n    y[1] ~ MvNormalMeanCovariance(compute_distances(z[1]), diageye(3))\n\n    # loop over time steps\n    for t in 2:nr_observations\n\n        # specify random walk state transition model\n        z[t] ~ MvNormalMeanPrecision(z[t-1], W)\n\n        # specify non-linear distance observations model\n        y[t] ~ MvNormalMeanPrecision(compute_distances(z[t]), R)\n        \n    end\n\nend;","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Because of the non-linearity, exact probabilistic inference is intractable. Therefore we resort to Conjugate-NonConjugate Variational Inference (CVI) following the paper Probabilistic programming with stochastic variational message passing. This requires setting the @meta macro in RxInfer.jl.","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_model_meta(nr_samples, nr_iterations, rng)\n    compute_distances(z) -> CVI(rng, nr_samples, nr_iterations, Descent(0.1))\nend;","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Because the engineers are using RxInfer.jl, they can automate the inference procedure. They run the model once with a limited numbers of samples and iterations, for speed, and once for accuracy.","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_fast = inference(\n    model = random_walk_model(nr_observations),\n    meta = random_walk_model_meta(1, 3, StableRNG(42)),\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 100,\n    free_energy = false,\n    returnvars = (z = KeepLast(),),\n    initmessages = (z = MvNormalMeanPrecision(zeros(2), 0.01 * diageye(2)),),\n);","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_accuracy = inference(\n    model = random_walk_model(nr_observations),\n    meta = random_walk_model_meta(1000, 100, StableRNG(42)),\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 100,\n    free_energy = false,\n    returnvars = (z = KeepLast(),),\n    initmessages = (z = MvNormalMeanPrecision(zeros(2), 0.01 * diageye(2)),),\n);","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"After running this fast inference procedure, the engineers plot the results and evaluate the performance:","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual and estimated location of WALL-E (fast inference)\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_fast.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"), title!(\"Fast (1 sample, 3 iterations)\"); p1.series_list[end][:label] = \"estimated location ±2σ\"\n\n# plot beacon and actual and estimated location of WALL-E (accurate inference)\np2 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_accuracy.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"), title!(\"Accurate (1000 samples, 100 iterations)\"); p2.series_list[end][:label] = \"estimated location ±2σ\"\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers were very happy with the solution, as it meant that the Mars mission could continue. However, they noted that the estimates began to deviate after WALL-E moved further away from the beacons. They deemed this was likely due to the noise in the distance measurements. Therefore, the engineers decided to adapt the model, such that they would also infer the process and observation noise precision matrices, Q and R respectively. They did this by adding Wishart priors to those matrices:","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(Q) = mathcalW(Q mid 3 mathrmI_2) \n  p(R) = mathcalW(R mid 4 mathrmI_3) \n  p(z_t mid z_t - 1 Q) = mathcalN(z_t mid z_t-1 Q^-1)\n  p(y_t mid z_t R)  = mathcalN left (y_t left vert beginbmatrix  z_t - s_1  z_t - s_2  z_t - s_3endbmatrixR^-1 right  right)\nendaligned","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@model function random_walk_model_wishart(nr_observations)\n\n    # allocate locations and observations\n    z = randomvar(nr_observations)\n    y = datavar(Vector{Float64}, nr_observations)\n\n    # set priors on precision matrices\n    Q ~ Wishart(3, diageye(2))\n    R ~ Wishart(4, diageye(3))\n\n    # specify initial estimates of the location\n    z[1] ~ MvNormalMeanCovariance(zeros(2), diageye(2)) \n    y[1] ~ MvNormalMeanCovariance(compute_distances(z[1]), diageye(3))\n\n    # loop over time steps\n    for t in 2:nr_observations\n\n        # specify random walk state transition model\n        z[t] ~ MvNormalMeanPrecision(z[t-1], Q)\n\n        # specify non-linear distance observations model\n        y[t] ~ MvNormalMeanPrecision(compute_distances(z[t]), R)\n        \n    end\n\nend;","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"meta = @meta begin \n    compute_distances(z) -> CVI(StableRNG(42), 2000, 100, Descent(0.01))\nend;","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Because of the added complexity with the Wishart distributions, the engineers simplify the problem by employing a structured mean-field factorization:","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"constraints = @constraints begin\n    q(z, Q, R) = q(z)q(Q)q(R)\nend;","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers run the inference procedure again and decide to track the inference performance using the Bethe free energy.","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_wishart = inference(\n    model = random_walk_model_wishart(nr_observations),\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 100,\n    free_energy = true,\n    returnvars = (z = KeepLast(),),\n    constraints = constraints,\n    meta = meta,\n    initmessages = (z = MvNormalMeanPrecision(zeros(2), 0.01 * diageye(2)),),\n    initmarginals = (R = Wishart(4, diageye(3)), Q = Wishart(3, diageye(2)))\n);","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"They plot the new estimates and the performance over time, and luckily WALL-E is found!","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual and estimated location of WALL-E (fast inference)\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_wishart.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"); p1.series_list[end][:label] = \"estimated location ±2σ\"\n\n# plot bethe free energy performance\np2 = plot(results_wishart.free_energy, label = \"\")\nxlabel!(\"iteration\"), ylabel!(\"Bethe free energy [nats]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Identification Problem/#examples-system-identification-problem","page":"System Identification Problem","title":"System Identification Problem","text":"","category":"section"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"using RxInfer, Distributions, StableRNGs, Plots","category":"page"},{"location":"examples/Identification Problem/#Two-merged-signals","page":"System Identification Problem","title":"Two merged signals","text":"","category":"section"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"In this example we are going to attempt to run Bayesian inference and decouple two random-walk signals, which were combined into a single single through some deterministic function f. We do not have access to the real values of these signals, but only to their combination. First, we create the generate_data function that accepts f as an argument:","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"function generate_data(f, n; seed = 123, x_i_min = -20.0, w_i_min = 20.0, noise = 20.0, real_x_τ = 0.1, real_w_τ = 1.0)\n\n    rng = StableRNG(seed)\n\n    real_x = Vector{Float64}(undef, n)\n    real_w = Vector{Float64}(undef, n)\n    real_y = Vector{Float64}(undef, n)\n\n    for i in 1:n\n        real_x[i] = rand(rng, Normal(x_i_min, sqrt(1.0 / real_x_τ)))\n        real_w[i] = rand(rng, Normal(w_i_min, sqrt(1.0 / real_w_τ)))\n        real_y[i] = rand(rng, Normal(f(real_x[i], real_w[i]), sqrt(noise)))\n\n        x_i_min = real_x[i]\n        w_i_min = real_w[i]\n    end\n    \n    return real_x, real_w, real_y\nend","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"The function returns the real signals real_x and  real_w for later comparison (we are not going to use them during inference) and their combined version real_y (we are going to use it as our observations during the inference). We also assume that real_y is corrupted with some measurement noise.","category":"page"},{"location":"examples/Identification Problem/#Combination-1:-y-x-w","page":"System Identification Problem","title":"Combination 1: y = x + w","text":"","category":"section"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"In our first example, we are going to use a simple addition (+) as the function f. In general, it is impossible to decouple the signals x and w without strong priors, but we can try and see how good an inference can be. The + operation on two random variables also has a special meaning in the probabilistic inference, namely the convolution of pdf's of the two random variables, and RxInfer treats it specially with many precomputed analytical rules, which may make the inference task easier. First, let us create a test dataset:","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"n = 250\nreal_x, real_w, real_y = generate_data(+, n);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, real_x, label = \"x\")\npl = plot!(pl, real_w, label = \"w\")\n\npr = plot(title = \"Combined y = x + w\")\npr = scatter!(pr, real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"(Image: )","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"To run inference, we need to create a probabilistic model: our beliefs about how our data could have been generated. For this we can use the @model macro from RxInfer.jl:","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"@model function identification_problem(f, n, m_x_0, τ_x_0, a_x, b_x, m_w_0, τ_w_0, a_w, b_w, a_y, b_y)\n    \n    x0 ~ Normal(mean = m_x_0, precision = τ_x_0)\n    τ_x ~ Gamma(shape = a_x, rate = b_x)\n    w0 ~ Normal(mean = m_w_0, precision = τ_w_0)\n    τ_w ~ Gamma(shape = a_w, rate = b_w)\n    τ_y ~ Gamma(shape = a_y, rate = b_y)\n    \n    x = randomvar(n)\n    w = randomvar(n)\n    s = randomvar(n)\n    y = datavar(Float64, n)\n    \n    x_i_min = x0\n    w_i_min = w0\n    \n    for i in 1:n\n        x[i] ~ Normal(mean = x_i_min, precision = τ_x)\n        w[i] ~ Normal(mean = w_i_min, precision = τ_w)\n        s[i] ~ f(x[i], w[i])\n        y[i] ~ Normal(mean = s[i], precision = τ_y)\n        \n        x_i_min = x[i]\n        w_i_min = w[i]\n    end\n    \nend","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"RxInfer runs Bayesian inference as a variational optimisation procedure between the real solution and its variational proxy q. In our model specification we assumed noise components to be unknown, thus, we need to enforce a structured mean-field assumption for the variational family of distributions q. This inevitably reduces the accuracy of the result, but makes the task easier and allows for fast and analytical message passing-based variational inference:","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"constraints = @constraints begin \n    q(x0, w0, x, w, τ_x, τ_w, τ_y, s) = q(x, x0, w, w0, s)q(τ_w)q(τ_x)q(τ_y)\nend","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"Constraints:\n  marginals form:\n  messages form:\n  factorisation:\n    q(x0, w0, x, w, τ_x, τ_w, τ_y, s) = q(x, x0, w, w0, s)q(τ_w)q(τ_x)q(τ_y\n)\nOptions:\n  warn = true","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"The next step is to assign priors, initialise needed messages and marginals and call the inference function:","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"m_x_0, τ_x_0 = -20.0, 1.0\nm_w_0, τ_w_0 = 20.0, 1.0\n\n# We set relatively strong priors for random walk noise components\n# and sort of vague prior for the noise of the observations\na_x, b_x = 0.01, 0.01var(real_x)\na_w, b_w = 0.01, 0.01var(real_w)\na_y, b_y = 1.0, 1.0\n\n# We set relatively strong priors for messages\nxinit = map(r -> NormalMeanPrecision(r, τ_x_0), reverse(range(-60, -20, length = n)))\nwinit = map(r -> NormalMeanPrecision(r, τ_w_0), range(20, 60, length = n))\n\nimessages = (x = xinit, w = winit)\nimarginals = (τ_x = GammaShapeRate(a_x, b_x), τ_w = GammaShapeRate(a_w, b_w), τ_y = GammaShapeRate(a_y, b_y))\n\nresult = inference(\n    model = identification_problem(+, n, m_x_0, τ_x_0, a_x, b_x, m_w_0, τ_w_0, a_w, b_w, a_y, b_y),\n    data  = (y = real_y,), \n    options = (limit_stack_depth = 500, ), \n    constraints = constraints, \n    initmessages = imessages, \n    initmarginals = imarginals, \n    iterations = 50\n)","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"Inference results:\n  Posteriors       | available for (w0, w, x0, s, τ_x, τ_w, τ_y, x)","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"Let's examine our inference results:","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"τ_x_marginals = result.posteriors[:τ_x]\nτ_w_marginals = result.posteriors[:τ_w]\nτ_y_marginals = result.posteriors[:τ_y]\n\nsmarginals = result.posteriors[:s]\nxmarginals = result.posteriors[:x]\nwmarginals = result.posteriors[:w];","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"px1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(xmarginals[end]), ribbon = var.(xmarginals[end]), label = \"Estimated X\")\n\npx1 = plot!(px1, real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(wmarginals[end]), ribbon = var.(wmarginals[end]), label = \"Estimated W\")\n\npx2 = scatter!(px2, real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(smarginals[end]), ribbon = std.(smarginals[end]), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"(Image: )","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"The inference results are not so bad, even though RxInfer missed the correct values of the signals between 100 and 150.","category":"page"},{"location":"examples/Identification Problem/#Combination-2:-y-min(x,-w)","page":"System Identification Problem","title":"Combination 2: y = min(x, w)","text":"","category":"section"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"In this example we use a slightly more complex function, for which RxInfer does not have precomputed analytical message update rules. We are going to attempt to run Bayesian inference with min as a combination function. Note, however, that directly using min may cause problems for the built-in approximation methods as it has zero partial derviates with respect to all but one of the variables. We generate data with the min function directly however we model it with a somewhat smoothed version:","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"# Smoothed version of `min` without zero-ed derivatives\nfunction smooth_min(x, y)    \n    if x < y\n        return x + 1e-4 * y\n    else\n        return y + 1e-4 * x\n    end\nend","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"smooth_min (generic function with 1 method)","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"RxInfer supports arbitrary nonlinear functions, but it requires an explicit approximation method specification. That can be achieved with the built-in @meta macro:","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"min_meta = @meta begin \n    # In this example we are going to use a simple `Linearization` method\n    smooth_min() -> Linearization()\nend","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"Meta specification:\n  smooth_min() -> Linearization()\nOptions:\n  warn = true","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"n = 200\nmin_real_x, min_real_w, min_real_y = generate_data(min, n, seed = 1, x_i_min = 0.0, w_i_min = 0.0, noise = 1.0, real_x_τ = 1.0, real_w_τ = 1.0);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, min_real_x, label = \"x\")\npl = plot!(pl, min_real_w, label = \"w\")\n\npr = plot(title = \"Combined y = min(x, w)\")\npr = scatter!(pr, min_real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"(Image: )","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"min_m_x_0, min_τ_x_0 = -1.0, 1.0\nmin_m_w_0, min_τ_w_0 = 1.0, 1.0\n\nmin_a_x, min_b_x = 1.0, 1.0\nmin_a_w, min_b_w = 1.0, 1.0\nmin_a_y, min_b_y = 1.0, 1.0\n\nmin_imessages = (x = NormalMeanPrecision(min_m_x_0, min_τ_x_0), w = NormalMeanPrecision(min_m_w_0, min_τ_w_0))\nmin_imarginals = (τ_x = GammaShapeRate(min_a_x, min_b_x), τ_w = GammaShapeRate(min_a_w, min_b_w), τ_y = GammaShapeRate(min_a_y, min_b_y))\n\nmin_result = inference(\n    model = identification_problem(smooth_min, n, min_m_x_0, min_τ_x_0, min_a_x, min_b_x, min_m_w_0, min_τ_w_0, min_a_w, min_b_w, min_a_y, min_b_y),\n    data  = (y = min_real_y,), \n    meta = min_meta,\n    options = (limit_stack_depth = 500, ), \n    constraints = constraints, \n    initmessages = min_imessages, \n    initmarginals = min_imarginals, \n    iterations = 100\n)","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"Inference results:\n  Posteriors       | available for (w0, w, x0, s, τ_x, τ_w, τ_y, x)","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"min_τ_x_marginals = min_result.posteriors[:τ_x]\nmin_τ_w_marginals = min_result.posteriors[:τ_w]\nmin_τ_y_marginals = min_result.posteriors[:τ_y]\n\nmin_smarginals = min_result.posteriors[:s]\nmin_xmarginals = min_result.posteriors[:x]\nmin_wmarginals = min_result.posteriors[:w]\n\npx1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, min_real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(min_xmarginals[end]), ribbon = var.(min_xmarginals[end]), label = \"Estimated X\")\n\npx1 = plot!(px1, min_real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(min_wmarginals[end]), ribbon = var.(min_wmarginals[end]), label = \"Estimated W\")\n\npx2 = scatter!(px2, min_real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(min_smarginals[end]), ribbon = std.(min_smarginals[end]), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"(Image: )","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"As we can see inference with the min function is significantly harder. Even though the combined signal has been inferred with high precision the underlying x and w signals are barely inferred. This may be expected, since the min function essentially destroy the information about one of the signals, thus, making it impossible to decouple two seemingly identical random walk signals. The only one inferred signal is the one which is lower and we have no inference information about the signal which is above. It might be possible to infer the states, however, with more informative priors and structural information about two different signals (e.g. if these are not random walks). ","category":"page"},{"location":"examples/Identification Problem/#Online-(filtering)-identification:-y-min(x,-w)","page":"System Identification Problem","title":"Online (filtering) identification: y = min(x, w)","text":"","category":"section"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"Another way to approach to this problem is to use online (filtering) inference procedure from RxInfer, but for that we also need to modify our model specification a bit:","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"@model function rx_identification(f)\n    \n    # We are going to continuosly update our priors\n    # based on new posteriors\n    m_x_0 = datavar(Float64) \n    τ_x_0 = datavar(Float64)\n    m_w_0 = datavar(Float64) \n    τ_w_0 = datavar(Float64)\n    a_x   = datavar(Float64) \n    b_x   = datavar(Float64)\n    a_y   = datavar(Float64) \n    b_y   = datavar(Float64)\n    a_w   =  datavar(Float64) \n    b_w   = datavar(Float64)\n    s     = randomvar()\n    y     = datavar(Float64)\n    \n    x0 ~ Normal(mean = m_x_0, precision = τ_x_0)\n    τ_x ~ Gamma(shape = a_x, rate = b_x)\n    w0 ~ Normal(mean = m_w_0, precision = τ_w_0)\n    τ_w ~ Gamma(shape = a_w, rate = b_w)\n    τ_y ~ Gamma(shape = a_y, rate = b_y)\n    \n    x ~ Normal(mean = x0, precision = τ_x)\n    w ~ Normal(mean = w0, precision = τ_w)\n\n    s ~ f(x, w)\n    y ~ Normal(mean = s, precision = τ_y)\n    \nend","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"We impose structured mean-field assumption for this model as well:","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"rx_constraints = @constraints begin \n    q(x0, x, w0, w, τ_x, τ_w, τ_y, s) = q(x0, x)q(w, w0)q(τ_w)q(τ_x)q(s)q(τ_y)\nend","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"Constraints:\n  marginals form:\n  messages form:\n  factorisation:\n    q(x0, x, w0, w, τ_x, τ_w, τ_y, s) = q(x0, x)q(w, w0)q(τ_w)q(τ_x)q(s)q(τ\n_y)\nOptions:\n  warn = true","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"Online inference in the RxInfer supports the @autoupdates specification, which tells inference procedure how to update priors based on new computed posteriors:","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"autoupdates = @autoupdates begin \n    m_x_0, τ_x_0 = mean_precision(q(x))\n    m_w_0, τ_w_0 = mean_precision(q(w))\n    a_x = shape(q(τ_x)) \n    b_x = rate(q(τ_x))\n    a_y = shape(q(τ_y))\n    b_y = rate(q(τ_y))\n    a_w = shape(q(τ_w)) \n    b_w = rate(q(τ_w))\nend","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"(m_x_0,τ_x_0 = mean_precision(q(x)), m_w_0,τ_w_0 = mean_precision(q(w)), a_\nx = shape(q(τ_x)), b_x = rate(q(τ_x)), a_y = shape(q(τ_y)), b_y = rate(q(τ_\ny)), a_w = shape(q(τ_w)), b_w = rate(q(τ_w)))","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"As previously we need to define the @meta structure that specifies the approximation method for the nonlinear function smooth_min (f in the model specification):","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"rx_meta = @meta begin \n    smooth_min() -> Linearization()\nend","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"Meta specification:\n  smooth_min() -> Linearization()\nOptions:\n  warn = true","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"Next step is to generate our dataset and to run the actual inference procedure! For that we use the rxinference function, which has a similar API as the inference function:","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"n = 300\nrx_real_x, rx_real_w, rx_real_y = generate_data(min, n, seed = 1, x_i_min = 1.0, w_i_min = -1.0, noise = 1.0, real_x_τ = 1.0, real_w_τ = 1.0);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, rx_real_x, label = \"x\")\npl = plot!(pl, rx_real_w, label = \"w\")\n\npr = plot(title = \"Combined y = min(x, w)\")\npr = scatter!(pr, rx_real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"(Image: )","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"engine = rxinference(\n    model         = rx_identification(smooth_min),\n    constraints   = rx_constraints,\n    data          = (y = rx_real_y,),\n    autoupdates   = autoupdates,\n    meta          = rx_meta,\n    returnvars    = (:x, :w, :τ_x, :τ_w, :τ_y, :s),\n    keephistory   = 1000,\n    historyvars   =  KeepLast(),\n    initmarginals = (w = NormalMeanVariance(-2.0, 1.0), x = NormalMeanVariance(2.0, 1.0), τ_x = GammaShapeRate(1.0, 1.0), τ_w = GammaShapeRate(1.0, 1.0), τ_y = GammaShapeRate(1.0, 20.0)),\n    iterations    = 10,\n    free_energy = true, \n    free_energy_diagnostics = nothing,\n    autostart     = true,\n)","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"RxInferenceEngine:\n  Posteriors stream    | enabled for (w, s, τ_x, τ_w, τ_y, x)\n  Free Energy stream   | enabled\n  Posteriors history   | available for (x, w, x0, s, τ_x, τ_w, τ_y, w0)\n  Free Energy history  | available\n  Enabled events       | [  ]","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"rx_smarginals = engine.history[:s]\nrx_xmarginals = engine.history[:x]\nrx_wmarginals = engine.history[:w];","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"px1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, rx_real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(rx_xmarginals), ribbon = var.(rx_xmarginals), label = \"Estimated X\")\n\npx1 = plot!(px1, rx_real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(rx_wmarginals), ribbon = var.(rx_wmarginals), label = \"Estimated W\")\n\npx2 = scatter!(px2, rx_real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(rx_smarginals), ribbon = std.(rx_smarginals), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"(Image: )","category":"page"},{"location":"examples/Identification Problem/","page":"System Identification Problem","title":"System Identification Problem","text":"The results are quite similar to the smoothing case and, as we can see, one of the random walk is again in the \"disabled\" state, does not infer anything and simply increases its variance (which is expected for the random walk).","category":"page"},{"location":"manuals/constraints-specification/#user-guide-constraints-specification","page":"Constraints specification","title":"Constraints Specification","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"RxInfer.jl exports @constraints macro for the extra constraints specification that can be used during the inference step in ReactiveMP.jl engine package.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"using RxInfer","category":"page"},{"location":"manuals/constraints-specification/#General-syntax","page":"Constraints specification","title":"General syntax","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints macro accepts either regular Julia function or a single begin ... end block. For example both are valid:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"\n# `functional` style\n@constraints function create_my_constraints(arg1, arg2)\n    ...\nend\n\n# `block` style\nmyconstraints = @constraints begin \n    ...\nend\n","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"In the first case it returns a function that return constraints upon calling, e.g. ","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints function make_constraints(mean_field)\n    q(x) :: PointMass\n\n    if mean_field\n        q(x, y) = q(x)q(y)\n    end\nend\n\nmyconstraints = make_constraints(true)","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"and in the second case it evaluates automatically and returns constraints object directly.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"myconstraints = @constraints begin \n    q(x) :: PointMass\n    q(x, y) = q(x)q(y)\nend","category":"page"},{"location":"manuals/constraints-specification/#Options-specification","page":"Constraints specification","title":"Options specification","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints macro accepts optional list of options as a first argument and specified as an array of key = value pairs, e.g. ","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"myconstraints = @constraints [ warn = false ] begin \n   ...\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"List of available options:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"warn::Bool - enables/disables various warnings with an incompatible model/constraints specification","category":"page"},{"location":"manuals/constraints-specification/#Marginal-and-messages-form-constraints","page":"Constraints specification","title":"Marginal and messages form constraints","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"To specify marginal or messages form constraints @constraints macro uses :: operator (in somewhat similar way as Julia uses it for multiple dispatch type specification)","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"The following constraint:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x) :: PointMass\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"indicates that the resulting marginal of the variable (or array of variables) named x must be approximated with a PointMass object. Message passing based algorithms compute posterior marginals as a normalized product of two colliding messages on corresponding edges of a factor graph. In a few words q(x)::PointMass reads as:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"mathrmapproximate q(x) = fracoverrightarrowmu(x)overleftarrowmu(x)int overrightarrowmu(x)overleftarrowmu(x) mathrmdxmathrmasPointMass","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"Sometimes it might be useful to set a functional form constraint on messages too. For example if it is essential to keep a specific Gaussian parametrisation or if some messages are intractable and need approximation. To set messages form constraint @constraints macro uses μ(...) instead of q(...):","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x) :: PointMass\n    μ(x) :: SampleList(1000)\n    # it is possible to assign different form constraints on the same variable \n    # both for the marginal and for the messages \nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints macro understands \"stacked\" form constraints. For example the following form constraint","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x) :: SampleList(1000) :: PointMass\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"indicates that the q(x) first must be approximated with a SampleList and in addition the result of this approximation should be approximated as a PointMass. ","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"note: Note\nNot all combinations of \"stacked\" form constraints are compatible between each other.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"You can find more information about built-in functional form constraint in the Built-in Functional Forms section. In addition, the ReactiveMP library documentation explains the functional form interfaces and shows how to build a custom functional form constraint that is compatible with RxInfer.jl and ReactiveMP.jl inference engine.","category":"page"},{"location":"manuals/constraints-specification/#Factorisation-constraints-on-posterior-distribution-q()","page":"Constraints specification","title":"Factorisation constraints on posterior distribution q()","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@model macro specifies generative model p(s, y) where s is a set of random variables and y is a set of observations. In a nutshell the goal of probabilistic programming is to find p(s|y). RxInfer approximates p(s|y) with a proxy distribution q(x) using KL divergence and Bethe Free Energy optimisation procedure. By default there are no extra factorisation constraints on q(s) and the optimal solution is q(s) = p(s|y). However, inference may be not tractable for every model without extra factorisation constraints. To circumvent this, RxInfer.jl and ReactiveMP.jl accept optional factorisation constraints specification syntax:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"For example:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x, y) = q(x)q(y)\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"specifies a so-called mean-field assumption on variables x and y in the model. Furthermore, if x is an array of variables in our model we may induce extra mean-field assumption on x in the following way.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x) = q(x[begin])..q(x[end])\n    q(x, y) = q(x)q(y)\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"These constraints specify a mean-field assumption between variables x and y (either single variable or collection of variables) and additionally specify mean-field assumption on variables x_i.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"note: Note\n@constraints macro does not support matrix-based collections of variables. E.g. it is not possible to write q(x[begin, begin])..q(x[end, end])","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"It is possible to write more complex factorisation constraints, for example:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x, y) = q(x[begin], y[begin])..q(x[end], y[end])\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"specifies a mean-field assumption between collection of variables named x and y only for variables with different indices. Another example is","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints function make_constraints(k)\n    q(x) = q(x[begin:k])q(x[k+1:end])\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"In this example we specify a mean-field assumption between a set of variables x[begin:k] and x[k+1:end]. ","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"To create a model with extra constraints the user may pass an optional constraints keyword argument for the create_model function:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@model function my_model(arguments...)\n   ...\nend\n\nconstraints = @constraints begin \n    ...\nend\n\nmodel, returnval = create_model(my_model(arguments...); constraints = constraints)","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"Alternatively, it is possible to use constraints directly in the automatic inference and rxinference functions that accepts constraints keyword argument. ","category":"page"},{"location":"contributing/overview/#contributing-overview","page":"Overview","title":"Contributing","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We welcome all possible contributors. This page details the some of the guidelines that should be followed when contributing to this package.","category":"page"},{"location":"contributing/overview/#Reporting-bugs","page":"Overview","title":"Reporting bugs","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We track bugs using GitHub issues. We encourage you to write complete, specific, reproducible bug reports. Mention the versions of Julia and RxInfer for which you observe unexpected behavior. Please provide a concise description of the problem and complement it with code snippets, test cases, screenshots, tracebacks or any other information that you consider relevant. This will help us to replicate the problem and narrow the search space for solutions.","category":"page"},{"location":"contributing/overview/#Suggesting-features","page":"Overview","title":"Suggesting features","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We welcome new feature proposals. However, before submitting a feature request, consider a few things:","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"Does the feature require changes in the core RxInfer code? If it doesn't (for example, you would like to add a factor node for a particular application), you can add local extensions in your script/notebook or consider making a separate repository for your extensions.\nIf you would like to add an implementation of a feature that changes a lot in the core RxInfer code, please open an issue on GitHub and describe your proposal first. This will allow us to discuss your proposal with you before you invest your time in implementing something that may be difficult to merge later on.","category":"page"},{"location":"contributing/overview/#Contributing-code","page":"Overview","title":"Contributing code","text":"","category":"section"},{"location":"contributing/overview/#Installing-RxInfer","page":"Overview","title":"Installing RxInfer","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We suggest that you use the dev command from the new Julia package manager to install RxInfer for development purposes. To work on your fork of RxInfer, use your fork's URL address in the dev command, for example:","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"] dev git@github.com:your_username/RxInfer.jl.git","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"The dev command clones RxInfer to ~/.julia/dev/RxInfer. All local changes to RxInfer code will be reflected in imported code.","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"note: Note\nIt is also might be useful to install Revise.jl package as it allows you to modify code and use the changes without restarting Julia.","category":"page"},{"location":"contributing/overview/#Committing-code","page":"Overview","title":"Committing code","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We use the standard GitHub Flow workflow where all contributions are added through pull requests. In order to contribute, first fork the repository, then commit your contributions to your fork, and then create a pull request on the main branch of the RxInfer repository.","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"Before opening a pull request, please make sure that all tests pass without failing! All examples (can be found in /examples/ directory) have to run without errors as well. ","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"note: Note\nUse make test, make examples and make docs commands to ensure that all tests, examples and the documentation build run without any issues. See below for the Makefile commands description in more details.","category":"page"},{"location":"contributing/overview/#Style-conventions","page":"Overview","title":"Style conventions","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We use default Julia style guide. We list here a few important points and our modifications to the Julia style guide:","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"Use 4 spaces for indentation\nType names use UpperCamelCase. For example: AbstractFactorNode, RandomVariable, etc..\nFunction names are lowercase with underscores, when necessary. For example: activate!, randomvar, as_variable, etc..\nVariable names and function arguments use snake_case\nThe name of a method that modifies its argument(s) must end in !","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"note: Note\nRxInfer repository contains scripts to automatically format code according to our guidelines. Use make format command to fix code style. This command overwrites files. Use make lint to run a linting procedure without overwriting the actual source files.","category":"page"},{"location":"contributing/overview/#Unit-tests","page":"Overview","title":"Unit tests","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We use the test-driven development (TDD) methodology for RxInfer development. The test coverage should be as complete as possible. Please make sure that you write tests for each piece of code that you want to add.","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"All unit tests are located in the /test/ directory. The /test/ directory follows the structure of the /src/ directory. Each test file should have following filename format: test_*.jl. Some tests are also present in jldoctest docs annotations directly in the source code. See Julia's documentation about doctests.","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"The tests can be evaluated by running following command in the Julia REPL:","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"] test RxInfer","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"In addition tests can be evaluated by running following command in the RxInfer root directory:","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"make test","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"note: Note\nUse make devtest to use local dev-ed versions of the core packages.","category":"page"},{"location":"contributing/overview/#Makefile","page":"Overview","title":"Makefile","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"RxInfer.jl uses Makefile for most common operations:","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"make help: Shows help snippet\nmake test: Run tests, supports extra arguments\nmake test testest=\"distributions:normal_mean_variance\" would run tests only from distributions/test_normal_mean_variance.jl\nmake test testset=\"distributions:normal_mean_variance models:lgssm\" would run tests both from distributions/test_normal_mean_variance.jl and models/test_lgssm.jl\nmake test dev=true would run tests while using dev-ed versions of core packages\nmake devtest: Alias for the make test dev=true ...\nmake docs: Compile documentation\nmake devdocs: Same as make docs, but uses dev-ed versions of core packages\nmake examples: Run all examples and put them in the docs/ folder if successfull \nmake devexamples: Same as make examples, but uses dev-ed versions of core packages\nmake lint: Check codestyle\nmake format: Check and fix codestyle ","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"note: Note\nCore packages include ReactiveMP.jl, GraphPPL.jl and Rocket.jl. When using any of the dev commands from the Makefile those packages must be present in the Pkg.devdir() directory.","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Global Parameter Optimisation/#examples-global-parameter-optimisation","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"","category":"section"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"This notebook demonstrates how to optimize the parameters of the state-space model (implemented in RxInfer.jl) through an external optimization packages such as Optim.jl","category":"page"},{"location":"examples/Global Parameter Optimisation/#Univariate-case","page":"Global Parameter Optimisation","title":"Univariate case","text":"","category":"section"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"First, lets try use the following simple model (multivariate version is presented below):","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\n    x_t = x_t-1 + c \n    y_t sim mathcalNleft(x_t p right) \nendaligned","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"with prior x_0 sim mathcalN(m_x_0 v_x_0). Our goal is to optimize parameters c and m_x_0.","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"using RxInfer, BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"@model function smoothing(n, x0, c::ConstVariable, P::ConstVariable)\n    \n    x_prior ~ NormalMeanVariance(mean(x0), cov(x0)) \n\n    x = randomvar(n)\n    y = datavar(Float64, n)\n\n    x_prev = x_prior\n\n    for i in 1:n\n        x[i] ~ x_prev + c\n        y[i] ~ NormalMeanVariance(x[i], P)\n        \n        x_prev = x[i]\n    end\n\n    return x, y\nend","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"rng = MersenneTwister(42)\n\nP      = 1.0\nn      = 250\nc_real = -5.0\ndata   = c_real .+ collect(1:n) + rand(rng, Normal(0.0, sqrt(P)), n);","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# c[1] is C\n# c[2] is μ0\nfunction f(c)\n    x0_prior = NormalMeanVariance(c[2], 100.0)\n    result = inference(\n        model = smoothing(n, x0_prior, c[1], P), \n        data  = (y = data,), \n        free_energy = true\n    )\n    return result.free_energy[end]\nend","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"f (generic function with 1 method)","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"using Optim","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"res = optimize(f, ones(2), GradientDescent(), Optim.Options(g_tol = 1e-3, iterations = 100, store_trace = true, show_trace = true, show_every = 10))","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Iter     Function value   Gradient norm \n     0     3.683766e+02     7.580911e+02\n * time: 0.025815963745117188\n    10     3.679681e+02     2.277267e+01\n * time: 9.412580966949463\n * Status: success\n\n * Candidate solution\n    Final objective value:     3.679676e+02\n\n * Found with\n    Algorithm:     Gradient Descent\n\n * Convergence measures\n    |x - x'|               = 5.09e-06 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.01e-06 ≰ 0.0e+00\n    |f(x) - f(x')|         = 1.69e-05 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 4.59e-08 ≰ 0.0e+00\n    |g(x)|                 = 9.57e-04 ≤ 1.0e-03\n\n * Work counters\n    Seconds run:   16  (vs limit Inf)\n    Iterations:    19\n    f(x) calls:    142\n    ∇f(x) calls:   142","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"res.minimizer # Real values are indeed (c = 1.0 and μ0 = -5.0)","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"2-element Vector{Float64}:\n  1.0005879774879456\n -5.043988393663863","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"println(\"Real value vs Optimized\")\nprintln(\"Real:      \", [ 1.0, c_real ])\nprintln(\"Optimized: \", res.minimizer)","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Real value vs Optimized\nReal:      [1.0, -5.0]\nOptimized: [1.0005879774879456, -5.043988393663863]","category":"page"},{"location":"examples/Global Parameter Optimisation/#Multivariate-case","page":"Global Parameter Optimisation","title":"Multivariate case","text":"","category":"section"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"We could perform the same experiment with more complex model and multivariate observations:","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\n    mathbfx_t sim mathcalNleft(mathbfAx_t-1 mathbfQ right) \n    mathbfy_t sim mathcalNleft(mathbfx_t mathbfP right) \nendaligned","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"with prior ","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nmathbfx_0 sim mathcalN(mathbfm_x_0 mathbfV_x_0)\nendaligned","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"and transition matrix ","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nmathbfA = beginbmatrix costheta  -sintheta  sintheta  costheta endbmatrix\nendaligned","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Covariance matrices mathbfV_x_0, mathbfP and mathbfQ are known. Our goal is to optimize parameters mathbfm_x_0 and theta.","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"@model function rotate_ssm(n, θ, x0, Q::ConstVariable, P::ConstVariable)\n    \n    x = randomvar(n)\n    y = datavar(Vector{Float64}, n)\n    \n    x_prior ~ MvNormalMeanCovariance(mean(x0), cov(x0))\n    \n    x_prev = x_prior\n    \n    A = constvar([ cos(θ) -sin(θ); sin(θ) cos(θ) ])\n    \n    for i in 1:n\n        x[i] ~ MvNormalMeanCovariance(A * x_prev, Q)\n        y[i] ~ MvNormalMeanCovariance(x[i], P)\n        \n        x_prev = x[i]\n    end\n    \nend","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Generate data\nfunction generate_rotate_ssm_data()\n    rng = MersenneTwister(1234)\n\n    θ = π / 8\n    A = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\n    Q = Matrix(Diagonal(1.0 * ones(2)))\n    P = Matrix(Diagonal(1.0 * ones(2)))\n\n    n = 300\n\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        \n        x[i] = rand(rng, MvNormal(A * x_prev, Q))\n        y[i] = rand(rng, MvNormal(x[i], Q))\n        \n        x_prev = x[i]\n    end\n\n    return θ, A, Q, P, n, x, y\nend","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"generate_rotate_ssm_data (generic function with 1 method)","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"θ, A, Q, P, n, x, y = generate_rotate_ssm_data();","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), ribbon = diag(Q)[1] .|> sqrt, fillalpha = 0.2, label = \"dim1\")\npx = plot!(px, getindex.(x, 2), ribbon = diag(Q)[2] .|> sqrt, fillalpha = 0.2, label = \"dim2\")\n\nplot(px, size = (1200, 450))","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"function f(θ)\n    x0 = MvNormalMeanCovariance([ θ[2], θ[3] ], Matrix(Diagonal(0.01 * ones(2))))\n    result = inference(\n        model = rotate_ssm(n, θ[1], x0, Q, P), \n        data  = (y = y,), \n        free_energy = true\n    )\n    return result.free_energy[end]\nend","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"f (generic function with 1 method)","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"res = optimize(f, zeros(3), LBFGS(), Optim.Options(f_tol = 1e-14, g_tol = 1e-12, show_trace = true, show_every = 10))","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Iter     Function value   Gradient norm \n     0     2.192003e+04     9.032537e+04\n * time: 9.608268737792969e-5\n * Status: success (objective increased between iterations)\n\n * Candidate solution\n    Final objective value:     1.161372e+03\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 1.41e-08 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.24e-09 ≰ 0.0e+00\n    |f(x) - f(x')|         = 2.27e-13 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 1.96e-16 ≤ 1.0e-14\n    |g(x)|                 = 9.39e-08 ≰ 1.0e-12\n\n * Work counters\n    Seconds run:   20  (vs limit Inf)\n    Iterations:    7\n    f(x) calls:    47\n    ∇f(x) calls:   47","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"println(\"Real value vs Optimized\")\nprintln(\"Real:      \", θ)\nprintln(\"Optimized: \", res.minimizer[1])\n\n@show sin(θ), sin(res.minimizer[1])\n@show cos(θ), cos(res.minimizer[1])","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Real value vs Optimized\nReal:      0.39269908169872414\nOptimized: 0.392933248139125\n(sin(θ), sin(res.minimizer[1])) = (0.3826834323650898, 0.38289976345258336)\n(cos(θ), cos(res.minimizer[1])) = (0.9238795325112867, 0.9237898955649795)\n(0.9238795325112867, 0.9237898955649795)","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"x0 = MvNormalMeanCovariance([ res.minimizer[2], res.minimizer[3] ], Matrix(Diagonal(100.0 * ones(2))))\n\nresult = inference(\n    model = rotate_ssm(n, res.minimizer[1], x0, Q, P), \n    data  = (y = y,), \n    free_energy = true\n)\n\nxmarginals = result.posteriors[:x]\n\npx = plot()\n\npx = plot!(px, getindex.(x, 1), ribbon = diag(Q)[1] .|> sqrt, fillalpha = 0.2, label = \"dim1\")\npx = plot!(px, getindex.(x, 2), ribbon = diag(Q)[2] .|> sqrt, fillalpha = 0.2, label = \"dim2\")\npx = plot!(px, getindex.(mean.(xmarginals), 1), ribbon = getindex.(var.(xmarginals), 1) .|> sqrt, fillalpha = 0.5, label = \"dim1_e\")\npx = plot!(px, getindex.(mean.(xmarginals), 2), ribbon = getindex.(var.(xmarginals), 2) .|> sqrt, fillalpha = 0.5, label = \"dim2_e\")\n\nplot(px, size = (1200, 450))","category":"page"},{"location":"examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/#examples-invertible-neural-networks:-a-tutorial","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"","category":"section"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Table of contents","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Introduction\nModel specification\nModel compilation\nProbabilistic inference\nParameter estimation","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/#Introduction","page":"Invertible neural networks: a tutorial","title":"Introduction","text":"","category":"section"},{"location":"examples/Invertible Neural Network Tutorial/#Load-required-packages","page":"Invertible neural networks: a tutorial","title":"Load required packages","text":"","category":"section"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Before we can start, we need to import some packages:","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"using RxInfer\nusing Random\nusing StableRNGs\n\nusing LinearAlgebra     # only used for some matrix specifics\nusing Plots             # only used for visualisation\nusing Distributions     # only used for sampling from multivariate distributions\nusing Optim             # only used for parameter optimisation","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/#Model-specification","page":"Invertible neural networks: a tutorial","title":"Model specification","text":"","category":"section"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Specifying an invertible neural network model is easy. The general recipe looks like follows: model = FlowModel(input_dim, (layer1(options), layer2(options), ...)). Here the first argument corresponds to the input dimension of the model and the second argument is a tuple of layers. An example model can be defined as ","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"model = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Alternatively, the input_dim can also be passed as an InputLayer layer as ","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"In the above AdditiveCouplingLayer layers the input bfx = x_1 x_2 ldots x_N is partitioned into chunks of unit length. These partitions are additively coupled to an output bfy = y_1 y_2 ldots y_N as ","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"beginaligned\n    y_1 = x_1 \n    y_2 = x_2 + f_1(x_1) \n    vdots \n    y_N = x_N + f_N-1(x_N-1)\nendaligned","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Importantly, this structure can easily be converted as ","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"beginaligned\n    x_1 = y_1 \n    x_2 = y_2 - f_1(x_1) \n    vdots \n    x_N = y_N - f_N-1(x_N-1)\nendaligned","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"f_n","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"is an arbitrarily complex function, here chosen to be a PlanarFlow, but this can be interchanged for any function or neural network. The permute keyword argument (which defaults to true) specifies whether the output of this layer should be randomly permuted or shuffled. This makes sure that the first element is also transformed in consecutive layers.","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"A permutation layer can also be added by itself as a PermutationLayer layer with a custom permutation matrix if desired.","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false),\n        PermutationLayer(PermutationMatrix(2)),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/#Model-compilation","page":"Invertible neural networks: a tutorial","title":"Model compilation","text":"","category":"section"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"In the current models, the layers are setup to work with the passed input dimension. This means that the function f_n is repeated input_dim-1 times for each of the partitions. Furthermore the permutation layers are set up with proper permutation matrices. If we print the model we get","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"model","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"FlowModel{3, Tuple{ReactiveMP.AdditiveCouplingLayerEmpty{Tuple{ReactiveMP.P\nlanarFlowEmpty{1}}}, PermutationLayer{Int64}, ReactiveMP.AdditiveCouplingLa\nyerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}}}(2, (ReactiveMP.AdditiveCou\nplingLayerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}(2, (ReactiveMP.Planar\nFlowEmpty{1}(),), 1), PermutationLayer{Int64}(2, [0 1; 1 0]), ReactiveMP.Ad\nditiveCouplingLayerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}(2, (Reactive\nMP.PlanarFlowEmpty{1}(),), 1)))","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"The text below describes the terms above. Please note the distinction in typing and elements, i.e. FlowModel{types}(elements):","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"FlowModel - specifies that we are dealing with a flow model.\n3 - Number of layers.\nTuple{AdditiveCouplingLayerEmpty{...},PermutationLayer{Int64},AdditiveCouplingLayerEmpty{...}} - tuple of layer types.\nTuple{ReactiveMP.PlanarFlowEmpty{1},ReactiveMP.PlanarFlowEmpty{1}} - tuple of functions f_n.\nPermutationLayer{Int64}(2, [0 1; 1 0]) - permutation layer with input dimension 2 and permutation matrix [0 1; 1 0].","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"From inspection we can see that the AdditiveCouplingLayerEmpty and PlanarFlowEmpty objects are different than before. They are initialized for the correct dimension, but they do not have any parameters registered to them. This is by design to allow for separating the model specification from potential optimization procedures. Before we perform inference in this model, the parameters should be initialized. We can randomly initialize the parameters as","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"compiled_model = compile(model)","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"CompiledFlowModel{3, Tuple{AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, \nFloat64}}}, PermutationLayer{Int64}, AdditiveCouplingLayer{Tuple{PlanarFlow\n{Float64, Float64}}}}}(2, (AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, \nFloat64}}}(2, (PlanarFlow{Float64, Float64}(-0.39206383968298236, 0.8736792\n992773851, -0.1998862904891868),), 1), PermutationLayer{Int64}(2, [0 1; 1 0\n]), AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, Float64}}}(2, (PlanarFl\now{Float64, Float64}(-0.6736229867362263, -0.8379020791726801, -0.817236567\n8092264),), 1)))","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Now we can see that random parameters have been assigned to the individual functions inside of our model. Alternatively if we would like to pass our own parameters, then this is also possible. You can easily find the required number of parameters using the nr_params(model) function.","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"compiled_model = compile(model, randn(StableRNG(321), nr_params(model)))","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"CompiledFlowModel{3, Tuple{AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, \nFloat64}}}, PermutationLayer{Int64}, AdditiveCouplingLayer{Tuple{PlanarFlow\n{Float64, Float64}}}}}(2, (AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, \nFloat64}}}(2, (PlanarFlow{Float64, Float64}(0.7296412319250487, -0.97673361\n28037319, -0.4749869451771002),), 1), PermutationLayer{Int64}(2, [0 1; 1 0]\n), AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, Float64}}}(2, (PlanarFlo\nw{Float64, Float64}(0.3490911082645933, -0.8184067956921087, -1.45782147323\n52386),), 1)))","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/#Probabilistic-inference","page":"Invertible neural networks: a tutorial","title":"Probabilistic inference","text":"","category":"section"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"We can perform inference in our compiled model through standard usage of ReactiveMP. Let's first generate some random 2D data which has been sampled from a standard normal distribution and is consecutively passed through an invertible neural network. Using the forward(model, data) function we can propagate data in the forward direction.","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"function generate_data(nr_samples::Int64, model::CompiledFlowModel; seed = 123)\n\n    rng = StableRNG(seed)\n    \n    # specify latent sampling distribution\n    dist = MvNormal([1.5, 0.5], I)\n\n    # sample from the latent distribution\n    x = rand(rng, dist, nr_samples)\n\n    # transform data\n    y = zeros(Float64, size(x))\n    for k = 1:nr_samples\n        y[:,k] .= ReactiveMP.forward(model, x[:,k])\n    end\n\n    # return data\n    return y, x\n\nend;","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# generate data\ny, x = generate_data(1000, compiled_model)\n\n# plot generated data\np1 = scatter(x[1,:], x[2,:], alpha=0.3, title=\"Original data\", size=(800,400))\np2 = scatter(y[1,:], y[2,:], alpha=0.3, title=\"Transformed data\", size=(800,400))\nplot(p1, p2, legend = false)","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"The probabilistic model for doing inference can be described as ","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"@model function invertible_neural_network(nr_samples::Int64)\n    \n    # initialize variables\n    z_μ   = randomvar()\n    z_Λ   = randomvar()\n    x     = randomvar(nr_samples)\n    y_lat = randomvar(nr_samples)\n    y     = datavar(Vector{Float64}, nr_samples)\n\n    # specify prior\n    z_μ ~ MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2)))\n    z_Λ ~ Wishart(2.0, tiny*diagm(ones(2)))\n\n    # specify observations\n    for k = 1:nr_samples\n\n        # specify latent state\n        x[k] ~ MvNormalMeanPrecision(z_μ, z_Λ)\n\n        # specify transformed latent value\n        y_lat[k] ~ Flow(x[k])\n\n        # specify observations\n        y[k] ~ MvNormalMeanCovariance(y_lat[k], tiny*diagm(ones(2)))\n\n    end\n\n    # return variables\n    return z_μ, z_Λ, x, y_lat, y\n\nend;","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Here the model is passed inside a meta data object of the flow node. Inference then resorts to","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"observations = [y[:,k] for k=1:size(y,2)]\n\nfmodel         = invertible_neural_network(length(observations))\ndata          = (y = observations, )\ninitmarginals = (z_μ = MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2))), z_Λ = Wishart(2.0, tiny*diagm(ones(2))))\nreturnvars    = (z_μ = KeepLast(), z_Λ = KeepLast(), x = KeepLast(), y_lat = KeepLast())\n\nconstraints = @constraints begin\n    q(z_μ, x, z_Λ) = q(z_μ)q(z_Λ)q(x)\nend\n\n@meta function fmeta(model)\n    compiled_model = compile(model, randn(StableRNG(321), nr_params(model)))\n    Flow(y_lat, x) -> FlowMeta(compiled_model) # defaults to FlowMeta(compiled_model; approximation=Linearization()). \n                                               # other approximation methods can be e.g. FlowMeta(compiled_model; approximation=Unscented(input_dim))\nend\n\n# First execution is slow due to Julia's initial compilation \nresult = inference(\n    model = fmodel, \n    data  = data,\n    constraints   = constraints,\n    meta          = fmeta(model),\n    initmarginals = initmarginals,\n    returnvars    = returnvars,\n    free_energy   = true,\n    iterations    = 10, \n    showprogress  = false\n)","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Inference results:\n  Posteriors       | available for (z_μ, z_Λ, y_lat, x)\n  Free Energy:     | Real[29485.3, 23762.9, 23570.6, 23570.6, 23570.6, 2357\n0.6, 23570.6, 23570.6, 23570.6, 23570.6]","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"fe_flow = result.free_energy\nzμ_flow = result.posteriors[:z_μ]\nzΛ_flow = result.posteriors[:z_Λ]\nx_flow  = result.posteriors[:x]\ny_flow  = result.posteriors[:y_lat];","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"As we can see, the variational free energy decreases inside of our model.","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"plot(1:10, fe_flow/size(y,2), xlabel=\"iteration\", ylabel=\"normalized variational free energy [nats/sample]\", legend=false)","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"If we plot a random noisy observation and its approximated transformed uncertainty we obtain:","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# pick a random observation\nid = rand(StableRNG(321), 1:size(y,2))\nrand_observation = MvNormal(y[:,id], 5e-1*diagm(ones(2)))\nwarped_observation = MvNormal(ReactiveMP.backward(compiled_model, y[:,id]), ReactiveMP.inv_jacobian(compiled_model, y[:,id])*5e-1*diagm(ones(2))*ReactiveMP.inv_jacobian(compiled_model, y[:,id])');\n\np1 = scatter(x[1,:], x[2,:], alpha=0.1, title=\"Latent distribution\", size=(1200,500), label=\"generated data\")\ncontour!(-5:0.1:5, -5:0.1:5, (x, y) -> pdf(MvNormal([1.5, 0.5], I), [x, y]), c=:viridis, colorbar=false, linewidth=2)\nscatter!([mean(zμ_flow)[1]], [mean(zμ_flow)[2]], color=\"red\", markershape=:x, markersize=5, label=\"inferred mean\")\ncontour!(-5:0.01:5, -5:0.01:5, (x, y) -> pdf(warped_observation, [x, y]), colors=\"red\", levels=1, linewidth=2, colorbar=false)\nscatter!([mean(warped_observation)[1]], [mean(warped_observation)[2]], color=\"red\", label=\"transformed noisy observation\")\np2 = scatter(y[1,:], y[2,:], alpha=0.1, label=\"generated data\")\nscatter!([ReactiveMP.forward(compiled_model, mean(zμ_flow))[1]], [ReactiveMP.forward(compiled_model, mean(zμ_flow))[2]], color=\"red\", marker=:x, label=\"inferred mean\")\ncontour!(-10:0.1:10, -10:0.1:10, (x, y) -> pdf(MvNormal([1.5, 0.5], I), ReactiveMP.backward(compiled_model, [x, y])), c=:viridis, colorbar=false, linewidth=2)\ncontour!(-10:0.1:10, -10:0.1:10, (x, y) -> pdf(rand_observation, [x, y]), colors=\"red\", levels=1, linewidth=2, label=\"random noisy observation\", colorba=false)\nscatter!([mean(rand_observation)[1]], [mean(rand_observation)[2]], color=\"red\", label=\"random noisy observation\")\nplot(p1, p2, legend = true)","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/#Parameter-estimation","page":"Invertible neural networks: a tutorial","title":"Parameter estimation","text":"","category":"section"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"The flow model is often used to learn unknown probabilistic mappings. Here we will demonstrate it as follows for a binary classification task with the following data:","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"function generate_data(nr_samples::Int64; seed = 123)\n    \n    rng = StableRNG(seed)\n\n    # sample weights\n    w = rand(rng, nr_samples, 2)\n\n    # sample appraisal\n    y = zeros(Float64, nr_samples)\n    for k = 1:nr_samples\n        y[k] = 1.0*(w[k,1] > 0.5)*(w[k,2] < 0.5)\n    end\n\n    # return data\n    return y, w\n\nend;","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"data_y, data_x = generate_data(50);\nscatter(data_x[:,1], data_x[:,2], marker_z=data_y, xlabel=\"w1\", ylabel=\"w2\", colorbar=false, legend=false)","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"We will then specify a possible model as","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# specify flow model\nmodel = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()), # defaults to AdditiveCouplingLayer(PlanarFlow(); permute=true)\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"The corresponding probabilistic model for the binary classification task can be created as","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"@model function invertible_neural_network_classifier(nr_samples::Int64)\n    \n    # initialize variables\n    x_lat  = randomvar(nr_samples)\n    y_lat1 = randomvar(nr_samples)\n    y_lat2 = randomvar(nr_samples)\n    y      = datavar(Float64, nr_samples)\n    x      = datavar(Vector{Float64}, nr_samples)\n\n    # specify observations\n    for k = 1:nr_samples\n\n        # specify latent state\n        x_lat[k] ~ MvNormalMeanPrecision(x[k], 1e3*diagm(ones(2)))\n\n        # specify transformed latent value\n        y_lat1[k] ~ Flow(x_lat[k])\n        y_lat2[k] ~ dot(y_lat1[k], [1, 1])\n\n        # specify observations\n        y[k] ~ Probit(y_lat2[k]) # default: where { pipeline = RequireMessage(in = NormalMeanPrecision(0, 1.0)) }\n\n    end\n\n    # return variables\n    return x_lat, x, y_lat1, y_lat2, y\n\nend;","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"fcmodel       = invertible_neural_network_classifier(length(data_y))\ndata          = (y = data_y, x = [data_x[k,:] for k=1:size(data_x,1)], )\n\n@meta function fmeta(model, params)\n    compiled_model = compile(model, params)\n    Flow(y_lat1, x_lat) -> FlowMeta(compiled_model)\nend","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"fmeta (generic function with 2 methods)","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Here we see that the compilation occurs inside of our probabilistic model. As a result we can pass parameters (and a model) to this function which we wish to opmize for some criterium, such as the variational free energy. Inference can be described as","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"For the optimization procedure, we will simplify our inference loop, such that it only accepts parameters as an argument (which is wishes to optimize) and outputs a performance metric.","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"function f(params)\n    Random.seed!(123) # Flow uses random permutation matrices, which is not good for the optimisation procedure\n    result = inference(\n        model                   = fcmodel, \n        data                    = data,\n        meta                    = fmeta(model, params),\n        free_energy             = true,\n        free_energy_diagnostics = nothing, # Free Energy can be set to NaN due to optimization procedure\n        iterations              = 10, \n        showprogress            = false\n    );\n    \n    result.free_energy[end]\nend;","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Optimization can be performed using the Optim package. Alternatively, other (custom) optimizers can be implemented, such as:","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"res = optimize(f, randn(StableRNG(42), nr_params(model)), GradientDescent(), Optim.Options(store_trace = true, show_trace = true, show_every = 50), autodiff=:forward)","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"uses finitediff and is slower/less accurate.","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"or","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# create gradient function\ng = (x) -> ForwardDiff.gradient(f, x);\n\n# specify initial params\nparams = randn(nr_params(model))\n\n# create custom optimizer (here Adam)\noptimizer = Adam(params; λ=1e-1)\n\n# allocate space for gradient\n∇ = zeros(nr_params(model))\n\n# perform optimization\nfor it = 1:10000\n\n    # backward pass\n    ∇ .= ForwardDiff.gradient(f, optimizer.x)\n\n    # gradient update\n    ReactiveMP.update!(optimizer, ∇)\n\nend\n","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"res = optimize(f, randn(StableRNG(42), nr_params(model)), GradientDescent(), Optim.Options(f_tol = 1e-3, store_trace = true, show_trace = true, show_every = 100), autodiff=:forward)","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Iter     Function value   Gradient norm \n     0     6.087938e+02     8.678361e+02\n * time: 0.028512001037597656\n   100     1.553191e+01     4.961177e+00\n * time: 32.503560066223145\n   200     8.405865e+00     4.597434e+00\n * time: 63.12502908706665\n * Status: success\n\n * Candidate solution\n    Final objective value:     7.090973e+00\n\n * Found with\n    Algorithm:     Gradient Descent\n\n * Convergence measures\n    |x - x'|               = 1.33e-03 ≰ 0.0e+00\n    |x - x'|/|x'|          = 5.77e-04 ≰ 0.0e+00\n    |f(x) - f(x')|         = 6.89e-03 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 9.71e-04 ≤ 1.0e-03\n    |g(x)|                 = 2.54e+00 ≰ 1.0e-08\n\n * Work counters\n    Seconds run:   88  (vs limit Inf)\n    Iterations:    287\n    f(x) calls:    736\n    ∇f(x) calls:   736","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"optimization results are then given as","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"params = Optim.minimizer(res)\ninferred_model = compile(model, params)\ntrans_data_x_1 = hcat(map((x) -> ReactiveMP.forward(inferred_model, x), [data_x[k,:] for k=1:size(data_x,1)])...)'\ntrans_data_x_2 = map((x) -> dot([1, 1], x), [trans_data_x_1[k,:] for k=1:size(data_x,1)])\ntrans_data_x_2_split = [trans_data_x_2[data_y .== 1.0], trans_data_x_2[data_y .== 0.0]]\np1 = scatter(data_x[:,1], data_x[:,2], marker_z = data_y, size=(1200,400), c=:viridis, colorbar=false, title=\"original data\")\np2 = scatter(trans_data_x_1[:,1], trans_data_x_1[:,2], marker_z = data_y, c=:viridis, size=(1200,400), colorbar=false, title=\"|> warp\")\np3 = histogram(trans_data_x_2_split; stacked=true, bins=50, size=(1200,400), title=\"|> dot\")\nplot(p1, p2, p3, layout=(1,3), legend=false)","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"using StatsFuns: normcdf\np1 = scatter(data_x[:,1], data_x[:,2], marker_z = data_y, title=\"original labels\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\np2 = scatter(data_x[:,1], data_x[:,2], marker_z = normcdf.(trans_data_x_2), title=\"predicted labels\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\np3 = contour(0:0.01:1, 0:0.01:1, (x, y) -> normcdf(dot([1,1], ReactiveMP.forward(inferred_model, [x,y]))), title=\"Classification map\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\nplot(p1, p2, p3, layout=(1,3), legend=false)","category":"page"},{"location":"examples/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/#examples-conjugate-computational-variational-message-passing-(cvi)","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"","category":"section"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();\nusing RxInfer, Random, LinearAlgebra, Plots, Flux, Plots, StableRNGs, SpecialFunctions","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In this notebook, the usage of Conjugate-NonConjugate Variational Inference (CVI) will be described. The implementation of CVI follows the paper Probabilistic programming with stochastic variational message passing.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"This notebook will first describe an example in which CVI is used, then it discusses several limitations, followed by an explanation on how to extend upon CVI.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/#An-example:-nonlinear-dynamical-system","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"An example: nonlinear dynamical system","text":"","category":"section"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"A group of researchers is performing a tracking experiment on some moving object along a 1-dimensional trajectory. The object is moving at a constant velocity, meaning that its position increases constantly over time. However, the researchers do not have access to the absolute position z_t at time t. Instead they have access to the observed squared distance y_t between the object and some reference point s. Because of budget cuts, the servo moving the object and the measurement devices are quite outdated and therefore lead to noisy measurements:","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# data generating process\nnr_observations = 50\nreference_point = 53\nhidden_location = collect(1:nr_observations) + rand(MersenneTwister(124), NormalMeanVariance(0.0, sqrt(5)), nr_observations)\nmeasurements = (hidden_location .- reference_point).^2 + rand(MersenneTwister(124), NormalMeanVariance(0.0, 5), nr_observations);","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# plot hidden location and reference frame\np1 = plot(1:nr_observations, hidden_location, linewidth=3, legend=:topleft, label=\"hidden location\")\nhline!([reference_point], linewidth=3, label=\"reference point\")\nxlabel!(\"time [sec]\"), ylabel!(\"location [cm]\")\n\n# plot measurements\np2 = scatter(1:nr_observations, measurements, linewidth=3, label=\"measurements\")\nxlabel!(\"time [sec]\"), ylabel!(\"squared distance [cm2]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"(Image: )","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The researchers are interested in quantifying this noise and in tracking the unobserved location of the object. As a result of this uncertainty, the researchers employ a probabilistic modeling approach. They formulate the probabilistic model","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"beginaligned\n p(tau)  = Gamma(tau mid alpha_tau beta_tau)\n p(gamma)  = Gamma(gamma mid alpha_gamma beta_gamma)\n p(z_t mid z_t - 1 tau)  = mathcalN(z_t mid z_t - 1 + 1 tau^-1)\n p(y_t mid z_t gamma)  = mathcalN(y_t mid (z_t - s)^2 gamma^-1)\nendaligned","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"where the researchers put priors on the process and measurement noise parameters, tau and gamma, respectively. They do this, because they do not know the accuracy of their devices.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The researchers have recently heard of this cool probabilistic programming package RxInfer.jl. They decided to give it a try and create the above model as follows:","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"function compute_squared_distance(z)\n    (z - reference_point)^2\nend;","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@model function measurement_model(nr_observations)\n\n    # allocate random variables and observations\n    z = randomvar(nr_observations)\n    y = datavar(Float64, nr_observations)\n\n    # set priors on precision parameters\n    τ ~ Gamma(shape = 1.0, rate = 1.0e-12)\n    γ ~ Gamma(shape = 1.0, rate = 1.0e-12)\n    \n    # specify estimate of initial location\n    z[1] ~ Normal(mean = 0, precision = τ)\n    y[1] ~ Normal(mean = compute_squared_distance(z[1]), precision = γ)\n\n    # loop over observations\n    for t in 2:nr_observations\n\n        # specify state transition model\n        z[t] ~ Normal(mean = z[t-1] + 1, precision = τ)\n\n        # specify non-linear observation model\n        y[t] ~ Normal(mean = compute_squared_distance(z[t]), precision = γ)\n        \n    end\n\nend","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"But here is the problem, our compute_squared_distance function is already compelx enough such that the exact Bayesian inference is intractable in this model. But the researchers knew that the RxInfer.jl supports a various collection of approximation methods for exactly such cases. One of these approximations is called CVI. CVI allows us to perform probabilistic inference around the non-linear measurement function. In general, for any (non-)linear relationship y = f(x1, x2, ..., xN) CVI can be employed, by specifying the function f and by adding this relationship inside the @model macro as y ~ f(x1, x2, ...,xN). The @model macro will generate a factor node with node function p(y | x1, x2, ..., xN) = δ(y - f(x1, x2, ...,xN)).","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The use of this non-linearity requires us to specify that we would like to use CVI. This can be done by specifying the metadata using the @meta macro as:","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@meta function measurement_meta(rng, nr_samples, nr_iterations, optimizer)\n    compute_squared_distance() -> CVI(rng, nr_samples, nr_iterations, optimizer)\nend;","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In general, for any (non-)linear function f(), CVI can be enabled with the @meta macro as:","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@meta function model_meta(...)\n    f() -> CVI(args...)\nend","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"See ?CVI for more information about the args....","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In our model, the z variables are connected to the non-linear node function. So in order to run probabilstic inference with CVI we need to enforce a constraint on the joint posterior distribution. Specifically, we need to create a factorization in which the variables that are directly connected to non-linearities are assumed to be independent from the rest of the variables.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In the above example, we will assume the following posterior factorization:","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@constraints function measurement_constraints()\n    q(z, τ, γ) = q(z)q(τ)q(γ)\nend;","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"This constraint can be explained by the set of two constraints, one for getting CVI to run, and one for assuming a mean-field factorization around the normal node as ","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@constraints function posterior_constraints() begin\n    q(z, γ) = q(z)q(γ) # CVI\n    q(z, τ) = q(z)q(τ) # the mean-field assumption around normal node\nend","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Because the engineers are using RxInfer.jl, they can automate the inference procedure. They track the inference performance using the Bethe free energy.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"results = inference(\n    model = measurement_model(nr_observations),\n    data = (y = measurements,),\n    iterations = 5,\n    free_energy = true,\n    returnvars = (z = KeepLast(),),\n    constraints = measurement_constraints(),\n    meta = measurement_meta(StableRNG(42), 100, 200, Descent(0.01)),\n    initmessages = (z = NormalMeanVariance(0, 5),),\n    initmarginals = (z = NormalMeanVariance(0, 5), τ = GammaShapeRate(1.0, 1.0e-12), γ = GammaShapeRate(1.0, 1.0e-12),),\n)","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Inference results:\n  Posteriors       | available for (z)\n  Free Energy:     | Real[1241.68, 428.557, 374.753, 364.755, 361.644]","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# plot estimates for location\np1 = plot(collect(1:nr_observations), hidden_location, label = \"hidden location\", legend=:topleft, linewidth=3, color = :red)\nplot!(map(mean, results.posteriors[:z]), label = \"estimated location (±2σ)\", ribbon = map(x -> 2*std(x), results.posteriors[:z]), fillalpha=0.5, linewidth=3, color = :orange)\nxlabel!(\"time [sec]\"), ylabel!(\"location [cm]\")\n\n# plot Bethe free energy\np2 = plot(results.free_energy, linewidth=3, label = \"\")\nxlabel!(\"iteration\"), ylabel!(\"Bethe free energy [nats]\")\n\nplot(p1, p2, size = (1200, 500))","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"(Image: )","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/#Requirements","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Requirements","text":"","category":"section"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"There are several main requirements for the CVI procedure to satisfy:","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The out interface of the non-linearity must be independently factorized with respect to other variables in the model.\nThe messages on input interfaces (x1, x2, ..., xN) are required to be from the exponential family of distributions.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In RxInfer, you can satisfy the first requirement by using appropriate factor nodes (Normal, Gamma, Bernoulli, etc) and second requirement by specifying the @constraints macro. In general you can specify this procedure as","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@model function model(...)\n    ...\n    y ~ f(x1, x2, ..., xN)\n    ... ~ Node2(z1,..., y, zM) # some node that is using the out interface of the non-linearity\n    ... \nend\n\n@constraints function constraints_meta() begin\n    q(y, z1, ..., zn) = q(y)q(z1,...,zM)\n    ...\nend;\n\n@meta function model_meta(...)\n    f() -> CVI(rng, nr_samples, nr_iterations, optimizer))\nend","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Note that not all exponential family distributions are implemented.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/#Extensions","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Extensions","text":"","category":"section"},{"location":"examples/Conjugate-Computational Variational Message Passing/#Using-a-custom-optimizer","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Using a custom optimizer","text":"","category":"section"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"CVI only supports Flux optimizers out of the box.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Below an explanation on how to extend to it to a custom optimizer.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Suppose we have CustomDescent structure which we want to use inside CVI for optimization.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"To do so, we need to implement ReactiveMP.cvi_update!(opt::CustomDescent, λ, ∇).","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"ReactiveMP.cvi_update! incapsulates the gradient step:","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"opt is used to select your optimizer structure\nλ is the current value\n∇ is a gradient value computed inside CVI.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"struct CustomDescent \n    learning_rate::Float64\nend\n\nfunction ReactiveMP.cvi_update!(opt::CustomDescent, λ, ∇)\n    return vec(λ) - (opt.learning_rate .* vec(∇))\nend","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Let's try to apply it to a model: beginaligned  p(x)  = mathcalN(0 1)\n p(y_imid x)  = mathcalN(y_i mid x^2 1)\nendaligned","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Let's generate some synthetic data for the model","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# generate data\ny = rand(StableRNG(123), NormalMeanVariance(19^2, 1), 1000)\nhistogram(y)","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"(Image: )","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Again we can create the corresponding model as:","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# specify non-linearity\nf(x) = x ^ 2\n\n# specify model\n@model function normal_square_model(nr_observations)\n\n    # allocate observations\n    y = datavar(Float64, nr_observations)\n\n    # describe prior on latent state\n    x ~ NormalMeanPrecision(0, 100)\n\n    # transform latent state\n    mean ~ f(x)\n\n    # observation model\n    y .~ NormalMeanVariance(mean, 1)\n\nend\n\n# specify meta\n@meta function normal_square_meta(rng, nr_samples, nr_iterations, optimizer)\n    f() ->  CVI(rng, nr_samples, nr_iterations, optimizer)\nend","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"normal_square_meta (generic function with 1 method)","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"We will use the inference function from ReactiveMP to run inference, where we provide an instance of the CustomDescent structure in our meta macro function:","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Side note: To run the inference for this model, we do not need to init the message for x but with this initialization, the inference procedure is more stable.","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"res = inference(\n    model = normal_square_model(1000),\n    data = (y = y,),\n    iterations = 10,\n    free_energy = true,\n    initmessages = (x = NormalMeanVariance(0, 100),),\n    meta = normal_square_meta(StableRNG(123), 100, 100, CustomDescent(0.1))\n)\n\nmean(res.posteriors[:x][end])","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"-18.998360101686355","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The mean inferred value of x is indeed close to 19, which was used to generate the data. Inference is working! ","category":"page"},{"location":"examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Note:  x^2 can not be inverted; the sign information can be lost: -19 and 19 are both equally good solutions.","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Gamma Mixture/#examples-gamma-mixture-model","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"","category":"section"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"This notebook implements one of the experiments outlined in https://biaslab.github.io/publication/mp-based-inference-in-gmm/.","category":"page"},{"location":"examples/Gamma Mixture/#Load-packages","page":"Gamma Mixture Model","title":"Load packages","text":"","category":"section"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"using RxInfer, Random, StatsPlots","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# create custom structure for model parameters for simplicity\nstruct GammaMixtureModelParameters\n    nmixtures   # number of mixtures\n    priors_as   # tuple of priors for variable a\n    priors_bs   # tuple of priors for variable b\n    prior_s     # prior of variable s\nend","category":"page"},{"location":"examples/Gamma Mixture/#Model-specification","page":"Gamma Mixture Model","title":"Model specification","text":"","category":"section"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"@model function gamma_mixture_model(nobservations, parameters::GammaMixtureModelParameters)\n\n    # fetch information from struct\n    nmixtures = parameters.nmixtures\n    priors_as = parameters.priors_as\n    priors_bs = parameters.priors_bs\n    prior_s   = parameters.prior_s\n\n    # set prior on global selection variable\n    s ~ Dirichlet(probvec(prior_s))\n\n    # allocate vectors of random variables\n    as = randomvar(nmixtures)\n    bs = randomvar(nmixtures)\n\n    # set priors on variables of mixtures\n    for i in 1:nmixtures\n        as[i] ~ Gamma(shape = shape(priors_as[i]), rate = rate(priors_as[i]))\n        bs[i] ~ Gamma(shape = shape(priors_bs[i]), rate = rate(priors_bs[i]))\n    end\n\n    # introduce random variables for local selection variables and data\n    z = randomvar(nobservations)\n    y = datavar(Float64, nobservations)\n\n    # convert vector to tuples for proper functioning of GammaMixture node\n    tas = tuple(as...)\n    tbs = tuple(bs...)\n\n    # specify local selection variable and data generating process\n    for i in 1:nobservations\n        z[i] ~ Categorical(s)\n        y[i] ~ GammaMixture(z[i], tas, tbs)\n    end\n    \nend","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"constraints = @constraints begin \n\n    q(z, as, bs, s) = q(z)q(as)q(bs)q(s)\n\n    q(as) = q(as[begin])..q(as[end])\n    q(bs) = q(bs[begin])..q(bs[end])\n    \n    q(as) :: PointMass(starting_point = (args...) -> [ 1.0 ])\nend","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"Constraints:\n  marginals form:\n    q(as) :: PointMassFormConstraint() [ prod_constraint = ProdGeneric(fall\nback = ProdAnalytical()) ]\n  messages form:\n  factorisation:\n    q(z, as, bs, s) = q(z)q(as)q(bs)q(s)\n    q(as) = q(as[(begin)..(end)])\n    q(bs) = q(bs[(begin)..(end)])\nOptions:\n  warn = true","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# specify seed and number of data points\nrng = MersenneTwister(43)\nn_samples = 2500\n\n# specify parameters of mixture model that generates the data\n# Note that mixture components have exactly the same means\nmixtures  = [ Gamma(9.0, inv(27.0)), Gamma(90.0, inv(270.0)) ]\nnmixtures = length(mixtures)\nmixing    = rand(rng, nmixtures)\nmixing    = mixing ./ sum(mixing)\nmixture   = MixtureModel(mixtures, mixing)\n\n# generate data set\ndataset = rand(rng, mixture, n_samples);","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# specify priors of probabilistic model\n# NOTE: As the means of the mixtures \"collide\", we specify informative prior for selector variable\nnmixtures = 2\ngpriors = GammaMixtureModelParameters(\n    nmixtures,                                                    # number of mixtures\n    [ Gamma(1.0, 0.1), Gamma(1.0, 1.0) ],                         # priors on variables a\n    [ GammaShapeRate(10.0, 2.0), GammaShapeRate(1.0, 3.0) ],      # priors on variables b\n    Dirichlet(1e3*mixing)                                         # prior on variable s\n)\n\ngmodel         = gamma_mixture_model(length(dataset), gpriors)\ngdata          = (y = dataset, )\nginitmarginals = (s = gpriors.prior_s, z = vague(Categorical, gpriors.nmixtures), bs = GammaShapeRate(1.0, 1.0))\ngreturnvars    = (s = KeepLast(), z = KeepLast(), as = KeepEach(), bs = KeepEach())\n\ngoptions = (\n     \n    default_factorisation = MeanField() # Mixture models require Mean-Field assumption currently\n)\n\ngresult = inference(\n    model         = gmodel, \n    data          = gdata,\n    constraints   = constraints,\n    options       = (limit_stack_depth = 100,),\n    initmarginals = ginitmarginals,\n    returnvars    = greturnvars,\n    free_energy   = true,\n    iterations    = 250, \n    showprogress  = true\n);","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# extract inferred parameters\n_as, _bs = mean.(gresult.posteriors[:as][end]), mean.(gresult.posteriors[:bs][end])\n_dists   = map(g -> Gamma(g[1], inv(g[2])), zip(_as, _bs))\n_mixing = mean(gresult.posteriors[:s])\n\n# create model from inferred parameters\n_mixture   = MixtureModel(_dists, _mixing);","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# report on outcome of inference\nprintln(\"Generated means: $(mean(mixtures[1])) and $(mean(mixtures[2]))\")\nprintln(\"Inferred means: $(mean(_dists[1])) and $(mean(_dists[2]))\")\nprintln(\"========\")\nprintln(\"Generated mixing: $(mixing)\")\nprintln(\"Inferred mixing: $(_mixing)\")","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"Generated means: 0.3333333333333333 and 0.33333333333333337\nInferred means: 0.33820861865469865 and 0.3330565159295471\n========\nGenerated mixing: [0.18923488676601088, 0.8107651132339891]\nInferred mixing: [0.11399584135718784, 0.8860041586428122]","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# plot results\np1 = histogram(dataset, ylim = (0, 13), xlim = (0, 1), normalize=:pdf, label=\"observations\")\np1 = plot!(mixture, label=false, title=\"Generated mixtures\")\n\np2 = histogram(dataset, ylim = (0, 13), xlim = (0, 1), normalize=:pdf, label=\"data\", opacity=0.3)\np2 = plot!(_mixture, label=false, title=\"Inferred mixtures\", linewidth=3.0)\n\n# evaluate the convergence of the algorithm by monitoring the BFE\np3 = plot(gresult.free_energy, label=false, xlabel=\"iterations\", title=\"Bethe FE\")\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"(Image: )","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"plot(p3)","category":"page"},{"location":"examples/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"(Image: )","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/#examples-kalman-filter-with-lstm-network-driven-dynamic","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"","category":"section"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"In this demo, we are interested in Bayesian state estimation in Nonlinear State-Space Model. For example, we will use the time series induced by Lorenz system.","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"In other words, we want to compute the  marginal posterior distribution of the latent (hidden) state x_k at each time step k given the history of the measurements up to the time step k: $ p(xk | y{1:k}). $ ","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"using RxInfer, BenchmarkTools, Flux, ReverseDiff, Random, Plots, LinearAlgebra, ProgressMeter","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"Random.seed!(1234);","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/#Generate-data","page":"Kalman filter with LSTM network driven dynamic","title":"Generate data","text":"","category":"section"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"# Lorenz system equations to be used to generate dataset\nBase.@kwdef mutable struct Lorenz\n    dt::Float64\n    σ::Float64\n    ρ::Float64\n    β::Float64\n    x::Float64\n    y::Float64\n    z::Float64\nend\n\nfunction step!(l::Lorenz)\n    dx = l.σ * (l.y - l.x);         l.x += l.dt * dx\n    dy = l.x * (l.ρ - l.z) - l.y;   l.y += l.dt * dy\n    dz = l.x * l.y - l.β * l.z;     l.z += l.dt * dz\nend\n;","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"#Dataset\nordered_dataset = []\nordered_parameters = []\n\nfor σ = 11:15\n    for ρ = 23:27\n        for β_nom = 6:9\n            attractor = Lorenz(0.02, σ, ρ, β_nom/3.0, 1, 1, 1)\n            noise_free_data = [[1.0, 1.0, 1.0]]\n            for i=1:99\n                step!(attractor)\n                push!(noise_free_data, [attractor.x, attractor.y, attractor.z])\n            end\n            push!(ordered_dataset, noise_free_data)\n            push!(ordered_parameters, [σ, ρ, β_nom/3.0])\n        end\n    end\nend\n\nnew_order = collect(1:100)\nshuffle!(new_order)\n\ndataset = [] #noisy dataset\nnoise_free_dataset = [] #noise free dataset\nlorenz_parameters = []\n\nfor i in new_order\n    data = []\n    push!(noise_free_dataset, ordered_dataset[i])\n    push!(lorenz_parameters, ordered_parameters[i])\n    for nfd in ordered_dataset[i]\n        push!(data,nfd+randn(3))\n    end\n    push!(dataset, data)\nend\n\ntrainset = dataset[1:60]\nvalidset = dataset[61:80]\ntestset = dataset[81:end]\n\nnoise_free_trainset = noise_free_dataset[1:60]\nnoise_free_validset = noise_free_dataset[61:80]\nnoise_free_testset = noise_free_dataset[81:end]\n;","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/#Data-visualization","page":"Kalman filter with LSTM network driven dynamic","title":"Data visualization","text":"","category":"section"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"one_nonoise=noise_free_trainset[1]\none=trainset[1]\n\ngx, gy, gz = zeros(100), zeros(100), zeros(100)\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\n\nfor i=1:100\n    rx[i], ry[i], rz[i] = one[i][1], one[i][2], one[i][3]\n    gx[i], gy[i], gz[i] = one_nonoise[i][1], one_nonoise[i][2], one_nonoise[i][3]\n\nend\np1=plot(rx,ry,label=\"Noise observations\")\np1=plot!(gx,gy,label=\"True state\")\nxlabel!(\"x\")\nylabel!(\"y\")\n\np2=plot(rx,rz,label=\"Noise observations\")\np2=plot!(gx,gz,label=\"True state\")\nxlabel!(\"x\")\nylabel!(\"z\")\n\np3=plot(ry,rz,label=\"Noise observations\")\np3=plot!(gy,gz,label=\"True state\")\nxlabel!(\"y\")\nylabel!(\"z\")\n\nplot(p1, p2, p3, size = (800, 200),layout=(1,3))","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"(Image: )","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/#Inference","page":"Kalman filter with LSTM network driven dynamic","title":"Inference","text":"","category":"section"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"We use the following state-space model representation:","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"beginaligned\nx_k sim p(x_k  x_k-1) \ny_k sim p(y_k  x_k)\nendaligned","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"where x_k sim p(x_k  x_k-1) represents the hidden dynamics of our system.  The hidden dynamics of the Lorenz system exhibit nonlinearities and hence cannot be solved in the closed form. One manner of solving this problem is by introducing a neural network to approximate the transition matrix of the Lorenz system. ","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"beginaligned\nA_k-1=NN(y_k-1) \np(x_k  x_k-1)=mathcalN(x_k  A_k-1x_k-1 Q) \np(y_k  x_k)=mathcalN(y_k  Bx_k R)\nendaligned","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"where NN is the neural network. The input is the observation y_k-1, and output is the trasition matrix A_k-1. B denote distortion or measurment matrix. Q and R are covariance matrices. Note that the hidden state x_k comprises three coordinates, i.e. x_k = (rx_k ry_k rz_k)","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"# Neural Network model\nmutable struct NN\n    InputLayer\n    OutputLater\n    g\n    params\n    function NN(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n        InputLayer = Dense(W1, b1, relu)\n        Lstm = LSTM(W2_1,W2_2,b2,s2_1)\n        OutputLayer = Dense(W3, b3)\n        g = Chain(InputLayer, OutputLayer);\n        new(InputLayer, OutputLayer, g, (W1,b1,W2_1,W2_2,b2,s2_1,W3,b3))\n    end\nend","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/#Model-specification","page":"Kalman filter with LSTM network driven dynamic","title":"Model specification","text":"","category":"section"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"Note that we treat the trasition matrix A_k-1 as time-varying.","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"#State Space Model\n@model function ssm(n, As, Q::ConstVariable, B::ConstVariable, R::ConstVariable)\n   \n    x = randomvar(n)\n    y = datavar(Vector{Float64}, n)\n    \n    x_prior_mean = zeros(3)\n    x_prior_cov  = Matrix(Diagonal(ones(3)))\n    \n    x[1] ~ MvNormalMeanCovariance(x_prior_mean, x_prior_cov)\n    y[1] ~ MvNormalMeanCovariance(B * x[1], R) where { q = q(mean)q(out)q(cov) }\n    \n    for i in 2:n\n        x[i] ~ MvNormalMeanCovariance(As[i - 1] * x[i - 1], Q) where { q = q(mean, out)q(cov) }\n        y[i] ~ MvNormalMeanCovariance(B * x[i], R) where { q = q(mean)q(out)q(cov) }\n    end\n    \n    return x, y\nend","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"We set distortion matrix B and the covariance matrices Q and R as identity matrix.","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"Q = Matrix(Diagonal(ones(3)))\nB = Matrix(Diagonal(ones(3)))\nR = Matrix(Diagonal(ones(3)))\n;","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"We use the inference function in the RxInfer.jl. Before that, we need to bulid a function to get the matrix A output by the neural network. And the A is treated as a datavar in the inference function.","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"function get_matrix_AS(data,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    n = length(data)\n    neural = NN(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    Flux.reset!(neural)\n    As  = map((d) -> Matrix(Diagonal(neural.g(d))), data[1:end-1])\n    return As\nend","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"get_matrix_AS (generic function with 1 method)","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"The weights of neural network NN are initialized as follows:","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"# Initial model parameters\nW1, b1 = randn(5,3)./100, randn(5)./100\nW2_1, W2_2, b2, s2_1, s2_2 = randn(5 * 4, 5)./100, randn(5 * 4, 5)./100, randn(5*4)./100, zeros(5), zeros(5)\nW3, b3 = randn(3,5)./100, randn(3)./100\n;","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"Before network training, we show the inference results for the hidden states:","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"# Performance on an instance from the testset before training\nindex = 1\ndata=testset[index]\nn=length(data)\nresult = inference(\n    model = ssm(n, get_matrix_AS(data,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3),Q,B,R), \n    data  = (y = data, ), \n    returnvars = (x = KeepLast(), ),\n    free_energy = true\n)\nx_est=result.posteriors[:x]\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\nrx_est_m, ry_est_m, rz_est_m = zeros(100), zeros(100), zeros(100)\nrx_est_var, ry_est_var, rz_est_var = zeros(100), zeros(100), zeros(100)\n\nfor i=1:100\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mean(x_est[i])[1], mean(x_est[i])[2], mean(x_est[i])[3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = var(x_est[i])[1], var(x_est[i])[2], var(x_est[i])[3]\nend\n\np1 = plot(rx,label=\"Hidden state rx\")\np1 = plot!(rx_est_m,label=\"Inferred states\", ribbon=rx_est_var)\np1 = scatter!(first.(testset[index]), label=\"Observations\", markersize=1.0)\n\np2 = plot(ry,label=\"Hidden state ry\")\np2 = plot!(ry_est_m,label=\"Inferred states\", ribbon=ry_est_var)\np2 = scatter!(getindex.(testset[index], 2), label=\"Observations\", markersize=1.0)\n\np3 = plot(rz,label=\"Hidden state rz\")\np3 = plot!(rz_est_m,label=\"Inferred states\", ribbon=rz_est_var)\np3 = scatter!(last.(testset[index]), label=\"Observations\", markersize=1.0)\n\n\nplot(p1, p2, p3, size = (1000, 300))","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"(Image: )","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/#Training-network","page":"Kalman filter with LSTM network driven dynamic","title":"Training network","text":"","category":"section"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"In this part, we use the Free Energy as the objective function to optimize the weights of network.","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"# free energy objective to be optimized during training\nfunction fe_tot_est(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    fe_ = 0\n    for train_instance in trainset\n        result = inference(\n            model = ssm(n, get_matrix_AS(train_instance,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3),Q,B,R), \n            data  = (y = train_instance, ), \n            returnvars = (x = KeepLast(), ),\n            free_energy = true\n        )\n        fe_ += result.free_energy[end]\n    end\n    return fe_\nend","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"fe_tot_est (generic function with 1 method)","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"# To track the performance on validation set\nfunction fe_valid_est(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    fe_ = 0\n    for valid_instance in validset\n        result = inference(\n            model = Model(ssm, n, get_matrix_AS(valid_instance,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3),Q,B,R), \n            data  = (y = valid_instance, ), \n            returnvars = (x = KeepLast(), ),\n            free_energy = true\n        )\n        fe_ += result.free_energy[end]\n    end\n    return fe_\nend","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"fe_valid_est (generic function with 1 method)","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"# training\nopt = Flux.Optimise.RMSProp(0.02, 0.9)\nparams = (W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n#if you want get more accurate result, please refine the learning rate and epoch.\n@showprogress for epoch in 1:8\n    grads = ReverseDiff.gradient(fe_tot_est, params);\n    for i=1:length(params)\n        Flux.Optimise.update!(opt,params[i],grads[i])\n    end\nend","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"The following figure shows the state estimation results after training the trained neural network with respect to free energy.","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"# test after training\nindex = 1\ndata = testset[index]\nn = length(data)\nresult = inference(\n    model = ssm(n, get_matrix_AS(data,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3),Q,B,R), \n    data  = (y = data, ), \n    returnvars = (x = KeepLast(), ),\n    free_energy = true\n)\nx_est=result.posteriors[:x]\n\ngx, gy, gz = zeros(100), zeros(100), zeros(100)\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\nrx_est_m, ry_est_m, rz_est_m = zeros(100), zeros(100), zeros(100)\nrx_est_var, ry_est_var, rz_est_var = zeros(100), zeros(100), zeros(100)\n\nfor i=1:100\n    gx[i], gy[i], gz[i] = noise_free_testset[index][i][1], noise_free_testset[index][i][2], noise_free_testset[index][i][3]\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mean(x_est[i])[1], mean(x_est[i])[2], mean(x_est[i])[3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = var(x_est[i])[1], var(x_est[i])[2], var(x_est[i])[3]\nend\n\np1 = plot(rx,label=\"Hidden state rx\")\np1 = plot!(rx_est_m,label=\"Inferred states\", ribbon=rx_est_var)\np1 = scatter!(first.(testset[index]), label=\"Observations\", markersize=1.0)\n\np2 = plot(ry,label=\"Hidden state ry\")\np2 = plot!(ry_est_m,label=\"Inferred states\", ribbon=ry_est_var)\np2 = scatter!(getindex.(testset[index], 2), label=\"Observations\", markersize=1.0)\n\np3 = plot(rz,label=\"Hidden state rz\")\np3 = plot!(rz_est_m,label=\"Inferred states\", ribbon=rz_est_var)\np3 = scatter!(last.(testset[index]), label=\"Observations\", markersize=1.0)\n\nplot(p1, p2, p3, size = (1000, 300))","category":"page"},{"location":"examples/Kalman filter with LSTM network driven dynamic/","page":"Kalman filter with LSTM network driven dynamic","title":"Kalman filter with LSTM network driven dynamic","text":"(Image: )","category":"page"},{"location":"library/exported-methods/#lib-using-methods","page":"Exported methods","title":"Using methods from RxInfer","text":"","category":"section"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"In the Julia programming language (in contrast to Python for example) the most common way of loading a module is:","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"using RxInfer","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"A nice explanation about how modules/packages work in Julia can be found in the official documentation.","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"In a nutshell, Julia automatically resolves all name collisions and there is no a lot of benefit of importing specific names, e.g.:","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"import RxInfer: mean","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"One of the reasons for that is that Julia uses multiple-dispatch capabilities to merge names automatically and will indicate (with a warning) if something went wrong or names have unresolvable collisions on types. As a small example of this feature consider the following small import example:","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"import RxInfer: mean as mean_from_rxinfer\nimport Distributions: mean as mean_from_distributions\n\nmean_from_rxinfer === mean_from_distributions","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"Even though we import mean function from two different packages they actually refer to the same object. Worth noting that this is not always the case - Julia will print a warning in case it finds unresolvable conflicts and usage of such functions will be disallowed unless user import them specifically. Read more about this in the section of the Julia's documentation.","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"# It is easier to let Julia resolve names automatically\n# Julia will not overwrite `mean` that is coming from both packages\nusing RxInfer, Distributions ","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"mean(Normal(0.0, 1.0)) # `Normal` is an object from `Distributions.jl`","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"mean(NormalMeanVariance(0.0, 1.0)) # `NormalMeanVariance` is an object from `RxInfer.jl`","category":"page"},{"location":"library/exported-methods/#lib-list-methods","page":"Exported methods","title":"List of available methods","text":"","category":"section"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"Below you can find a list of exported methods from RxInfer.jl. All methods (even private) can be always accessed with RxInfer. prefix, e.g RxInfer.mean.","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"note: Note\nSome exported names are (for legacy reasons) intended for private usage only. As a result some of these methods do not have a proper associated documentation with them. We constantly improve RxInfer.jl library and continue to add better documentation for many exported methods, but a small portion of these methods could be removed from this list in the future.","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"using RxInfer #hide\nforeach(println, names(RxInfer))","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Custom nonlinear node/#examples-custom-nonlinear-node","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"","category":"section"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"using RxInfer, Random, StableRNGs","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"Here is an example of creating custom node with nonlinear function approximation with samplelist.","category":"page"},{"location":"examples/Custom nonlinear node/#Custom-node-creation","page":"Custom Nonlinear Node","title":"Custom node creation","text":"","category":"section"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"struct NonlinearNode end # Dummy structure just to make Julia happy\n\nstruct NonlinearMeta{R, F}\n    rng      :: R\n    fn       :: F   # Nonlinear function, we assume 1 float input - 1 float output\n    nsamples :: Int # Number of samples used in approximation\nend","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"@node NonlinearNode Deterministic [ out, in ]","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"We need to define two Sum-product message computation rules for our new custom node","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"Rule for outbound message on out edge given inbound message on in edge\nRule for outbound message on in edge given inbound message on out edge\nBoth rules accept optional meta object","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"# Rule for outbound message on `out` edge given inbound message on `in` edge\n@rule NonlinearNode(:out, Marginalisation) (m_in::NormalMeanVariance, meta::NonlinearMeta) = begin \n    samples = rand(meta.rng, m_in, meta.nsamples)\n    return SampleList(map(meta.fn, samples))\nend","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"# Rule for outbound message on `in` edge given inbound message on `out` edge\n@rule NonlinearNode(:in, Marginalisation) (m_out::Gamma, meta::NonlinearMeta) = begin     \n    return ContinuousUnivariateLogPdf((x) -> logpdf(m_out, meta.fn(x)))\nend","category":"page"},{"location":"examples/Custom nonlinear node/#Model-specification","page":"Custom Nonlinear Node","title":"Model specification","text":"","category":"section"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"After we have defined our custom node with custom rules we may proceed with a model specification:","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"beginaligned\np(theta) = mathcalN(thetamu_theta sigma_theta)\np(m) = mathcalN(thetamu_m sigma_m)\np(w) = f(theta)\np(y_im w) = mathcalN(y_im w)\nendaligned","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"Given this IID model, we aim to estimate the precision of a Gaussian distribution. We pass a random variable theta through a non-linear transformation f to make it positive and suitable for a precision parameter of a Gaussian distribution. We, later on, will estimate the posterior of theta. ","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"@model function nonlinear_estimation(n)\n    \n    θ ~ Normal(mean = 0.0, variance = 100.0)\n    m ~ Normal(mean = 0.0, variance = 1.0)\n    \n    w ~ NonlinearNode(θ)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ Normal(mean = m, precision = w)\n    end\n    \nend","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"@constraints function nconstsraints(nsamples)\n    q(θ) :: SampleList(nsamples, LeftProposal())\n    q(w) :: SampleList(nsamples, RightProposal())\n    \n    q(θ, w, m) = q(θ)q(m)q(w)\nend","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"nconstsraints (generic function with 1 method)","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"@meta function nmeta(fn, nsamples)\n    NonlinearNode(θ, w) -> NonlinearMeta(StableRNG(123), fn, nsamples)\nend","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"nmeta (generic function with 1 method)","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"Here we generate some data","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"nonlinear_fn(x) = abs(exp(x) * sin(x))","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"nonlinear_fn (generic function with 1 method)","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"seed = 123\nrng  = StableRNG(seed)\n\nniters   = 15 # Number of VMP iterations\nnsamples = 5_000 # Number of samples in approximation\n\nn = 500 # Number of IID samples\nμ = -10.0\nθ = -1.0\nw = nonlinear_fn(θ)\n\ndata = rand(rng, NormalMeanPrecision(μ, w), n);","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"result = inference(\n    model = nonlinear_estimation(n),\n    meta =  nmeta(nonlinear_fn, nsamples),\n    constraints = nconstsraints(nsamples),\n    data = (y = data, ), \n    initmarginals = (m = vague(NormalMeanPrecision), w = vague(Gamma)),\n    returnvars = (θ = KeepLast(), ),\n    iterations = niters,  \n    showprogress = true\n)","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"θposterior = result.posteriors[:θ]","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"SampleList(Univariate, 5000)","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"using Plots, StatsPlots\n\nestimated = Normal(mean_std(θposterior)...)\n\nplot(estimated, title=\"Posterior for θ\", label = \"Estimated\", legend = :bottomright, fill = true, fillopacity = 0.2, xlim = (-3, 3), ylim = (0, 2))\nvline!([ θ ], label = \"Real value of θ\")","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"(Image: )","category":"page"},{"location":"examples/Custom nonlinear node/","page":"Custom Nonlinear Node","title":"Custom Nonlinear Node","text":"","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Autoregressive Model/#examples-autoregressive-model","page":"Autoregressive Model","title":"Autoregressive Model","text":"","category":"section"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"In this example we are going to perform an automated Variational Bayesian Inference for autoregressive model that can be represented as following:","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"beginaligned\np(gamma) = mathrmGamma(gammaa b)\np(mathbftheta) = mathcalN(mathbfthetamathbfmu Sigma)\np(x_tmathbfx_t-1t-k) = mathcalN(x_tmathbftheta^Tmathbfx_t-1t-k gamma^-1)\np(y_tx_t) = mathcalN(y_tx_t tau^-1)\nendaligned","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"where x_t is a current state of our system, mathbfx_t-1t-k is a sequence of k previous states, k is an order of autoregression process, mathbftheta is a vector of transition coefficients, gamma is a precision of state transition process, y_k is a noisy observation of x_k with precision tau.","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"For a more rigorous introduction to Bayesian inference in Autoregressive models we refer to Albert Podusenko, Message Passing-Based Inference for Time-Varying Autoregressive Models.","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We start with importing all needed packages:","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"using RxInfer, Distributions, LinearAlgebra, Random, Plots, BenchmarkTools, Parameters","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Let's generate some synthetic dataset, we use a predefined sets of coeffcients for k = 1, 3 and 5 respectively:","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"# The following coefficients correspond to stable poles\ncoefs_ar_1 = [-0.27002517200218096]\ncoefs_ar_2 = [0.4511170798064709, -0.05740081602446657]\ncoefs_ar_5 = [0.10699399235785655, -0.5237303489793305, 0.3068897071844715, -0.17232255282458891, 0.13323964347539288];","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"function generate_ar_data(rng, n, θ, γ, τ)\n    order        = length(θ)\n    states       = Vector{Vector{Float64}}(undef, n + 3order)\n    observations = Vector{Float64}(undef, n + 3order)\n    \n    γ_std = sqrt(inv(γ))\n    τ_std = sqrt(inv(τ))\n    \n    states[1] = randn(rng, order)\n    \n    for i in 2:(n + 3order)\n        states[i]       = vcat(rand(rng, Normal(dot(θ, states[i - 1]), γ_std)), states[i-1][1:end-1])\n        observations[i] = rand(rng, Normal(states[i][1], τ_std))\n    end\n    \n    return states[1+3order:end], observations[1+3order:end]\nend","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"generate_ar_data (generic function with 1 method)","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"# Seed for reproducibility\nseed = 123\nrng  = MersenneTwister(seed)\n\n# Number of observations in synthetic dataset\nn = 500\n\n# AR process parameters\nreal_γ = 1.0\nreal_τ = 0.5\nreal_θ = coefs_ar_5\n\nstates, observations = generate_ar_data(rng, n, real_θ, real_γ, real_τ);","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Let's plot our synthetic dataset:","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"plot(first.(states), label = \"Hidden states\")\nscatter!(observations, label = \"Observations\")","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"(Image: )","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Next step is to specify probabilistic model, inference constraints and run inference procedure with RxInfer. We will specify two different models for Multivariate AR with order k > 1 and for Univariate AR (reduces to simple State-Space-Model) with order k = 1.","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"@model function lar_model(T::Type, n, order, c, τ)\n    \n     \n    # We create a sequence of random variables for hidden states\n    x = randomvar(n)\n    # As well a sequence of observartions\n    y = datavar(Float64, n)\n    \n    ct = constvar(c)\n    # We assume observation noise to be known\n    cτ = constvar(τ)\n    \n    γ  = randomvar()\n    θ  = randomvar()\n    x0 = randomvar()\n    \n    # Prior for first state\n    if T === Multivariate\n        γ  ~ Gamma(α = 1.0, β = 1.0)\n        θ  ~ MvNormal(μ = zeros(order), Λ = diageye(order))\n        x0 ~ MvNormal(μ = zeros(order), Λ = diageye(order))\n    else\n        γ  ~ Gamma(α = 1.0, β = 1.0)\n        θ  ~ Normal(μ = 0.0, γ = 1.0)\n        x0 ~ Normal(μ = 0.0, γ = 1.0)\n    end\n    \n    x_prev = x0\n    \n    for i in 1:n\n        \n        x[i] ~ AR(x_prev, θ, γ) \n        \n        if T === Multivariate\n            y[i] ~ Normal(μ = dot(ct, x[i]), γ = cτ)\n        else\n            y[i] ~ Normal(μ = ct * x[i], γ = cτ)\n        end\n        \n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"@constraints function ar_constraints() \n    q(x0, x, θ, γ) = q(x0, x)q(θ)q(γ)\nend","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"ar_constraints (generic function with 1 method)","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"@meta function ar_meta(artype, order, stype)\n    AR(x, θ, γ) -> ARMeta(artype, order, stype)\nend","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"ar_meta (generic function with 1 method)","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"morder  = 5\nmartype = Multivariate\nmc      = ReactiveMP.ar_unit(martype, morder)\nmconstraints = ar_constraints()\nmmeta        = ar_meta(martype, morder, ARsafe())\n\nmoptions = (limit_stack_depth = 100, )\n\nmmodel         = lar_model(martype, length(observations), morder, mc, real_τ)\nmdata          = (y = observations, )\nminitmarginals = (γ = GammaShapeRate(1.0, 1.0), θ = MvNormalMeanPrecision(zeros(morder), diageye(morder)))\nmreturnvars    = (x = KeepLast(), γ = KeepEach(), θ = KeepEach())\n\n# First execution is slow due to Julia's initial compilation \n# Subsequent runs will be faster (benchmarks are below)\nmresult = inference(\n    model = mmodel, \n    data  = mdata,\n    constraints   = mconstraints,\n    meta          = mmeta,\n    options       = moptions,\n    initmarginals = minitmarginals,\n    returnvars    = mreturnvars,\n    free_energy   = true,\n    iterations    = 25, \n    showprogress  = false\n);","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"@unpack x, γ, θ = mresult.posteriors\n\nfe = mresult.free_energy;","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We will use different initial marginals depending on type of our AR process","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"p1 = plot(first.(states), label=\"Hidden state\")\np1 = scatter!(p1, observations, label=\"Observations\")\np1 = plot!(p1, first.(mean.(x)), ribbon = first.(std.(x)), label=\"Inferred states\", legend = :bottomright)\n\np2 = plot(mean.(γ), ribbon = std.(γ), label = \"Inferred transition precision\", legend = :topright)\np2 = plot!([ real_γ ], seriestype = :hline, label = \"Real transition precision\")\n\np3 = plot(fe, label = \"Bethe Free Energy\")\n\nplot(p1, p2, p3, layout = @layout([ a; b c ]))","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"(Image: )","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Let's also plot a subrange of our results:","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"subrange = div(n,5):(div(n, 5) + div(n, 5))\n\nplot(subrange, first.(states)[subrange], label=\"Hidden state\")\nscatter!(subrange, observations[subrange], label=\"Observations\")\nplot!(subrange, first.(mean.(x))[subrange], ribbon = sqrt.(first.(var.(x)))[subrange], label=\"Inferred states\", legend = :bottomright)","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"(Image: )","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"It is also interesting to see where our AR coefficients converge to:","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"let\n    pθ = plot()\n\n    θms = mean.(θ)\n    θvs = var.(θ)\n    \n    l = length(θms)\n\n    edim(e) = (a) -> map(r -> r[e], a)\n\n    for i in 1:length(first(θms))\n        pθ = plot!(pθ, θms |> edim(i), ribbon = θvs |> edim(i) .|> sqrt, label = \"Estimated θ[$i]\")\n    end\n    \n    for i in 1:length(real_θ)\n        pθ = plot!(pθ, [ real_θ[i] ], seriestype = :hline, label = \"Real θ[$i]\")\n    end\n    \n    plot(pθ, legend = :outertopright, size = (800, 300))\nend","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"(Image: )","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"println(\"$(length(real_θ))-order AR inference Bethe Free Energy: \", last(fe))","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"5-order AR inference Bethe Free Energy: 1026.3779744365854","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We can also run a 1-order AR inference on 5-order AR data:","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"uorder  = 1\nuartype = Univariate\nuc      = ReactiveMP.ar_unit(uartype, uorder)\nuconstraints = ar_constraints()\numeta        = ar_meta(uartype, uorder, ARsafe())\n\nuoptions = (limit_stack_depth = 100, )\n\numodel         = lar_model(uartype, length(observations), uorder, uc, real_τ)\nudata          = (y = observations, )\nuinitmarginals = (γ = GammaShapeRate(1.0, 1.0), θ = NormalMeanPrecision(0.0, 1.0))\nureturnvars    = (x = KeepLast(), γ = KeepEach(), θ = KeepEach())\n\nuresult = inference(\n    model = umodel, \n    data  = udata,\n    meta  = umeta,\n    constraints   = uconstraints,\n    initmarginals = uinitmarginals,\n    returnvars    = ureturnvars,\n    free_energy   = true,\n    iterations    = 15, \n    showprogress  = false\n);","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We can see that, according to final Bethe Free Energy value, in this example 5-order AR process can describe data better than 1-order AR.","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"We may be also interested in benchmarking our algorithm:","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"println(\"Benchmark for n = $n and AR $(uorder) order inference\");\n\n@benchmark inference(\n    model = $umodel, \n    constraints = $uconstraints,\n    meta = $umeta, \n    data = $udata, \n    initmarginals = $uinitmarginals, \n    free_energy = true, \n    iterations = 15, \n    showprogress = false\n)","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Benchmark for n = 500 and AR 1 order inference\nBenchmarkTools.Trial: 10 samples with 1 evaluation.\n Range (min … max):  424.619 ms … 554.058 ms  ┊ GC (min … max):  0.00% … 14\n.96%\n Time  (median):     533.002 ms               ┊ GC (median):    11.41%\n Time  (mean ± σ):   505.052 ms ±  49.407 ms  ┊ GC (mean ± σ):   9.87% ±  8\n.33%\n\n  ▁             █ ▁                               ▁    ▁▁ ▁▁  ▁  \n  █▁▁▁▁▁▁▁▁▁▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁██▁██▁▁█ ▁\n  425 ms           Histogram: frequency by time          554 ms <\n\n Memory estimate: 100.04 MiB, allocs estimate: 2133086.","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"println(\"Benchmark for n = $n and AR $(morder) order inference\");\n\n@benchmark inference(\n    model = $mmodel, \n    constraints = $mconstraints, \n    meta = $mmeta, \n    data = $mdata, \n    initmarginals = $minitmarginals, \n    free_energy = true, \n    iterations = 15, \n    showprogress = false\n)","category":"page"},{"location":"examples/Autoregressive Model/","page":"Autoregressive Model","title":"Autoregressive Model","text":"Benchmark for n = 500 and AR 5 order inference\nBenchmarkTools.Trial: 5 samples with 1 evaluation.\n Range (min … max):  1.042 s …   1.164 s  ┊ GC (min … max):  8.06% … 16.13%\n Time  (median):     1.116 s              ┊ GC (median):    12.80%\n Time  (mean ± σ):   1.112 s ± 46.890 ms  ┊ GC (mean ± σ):  12.80% ±  2.99%\n\n  █                        █        █           █         █  \n  █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁█ ▁\n  1.04 s         Histogram: frequency by time        1.16 s <\n\n Memory estimate: 305.20 MiB, allocs estimate: 2848612.","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Nonlinear Virus Spread/#examples-nonlinear-virus-spread","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"","category":"section"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"In this demo we consider a model for the spead of a virus (not COVID-19!) in a population. We are interested in estimating the reproduction rate from daily observations of the number of infected individuals. The reproduction rate indicates how many others are (on average) infected by one infected individual per time unit.","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"using RxInfer, Plots, Distributions","category":"page"},{"location":"examples/Nonlinear Virus Spread/#Generate-Data","page":"Nonlinear Virus Spread","title":"Generate Data","text":"","category":"section"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"We start by generating a toy dataset for a virus with a reproduction rate a. Here, y represents the measured number of infected individuals, and x a latent state.","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"# Generate toy dataset\nT = 30\nx_0_data = 0.6\na_data = 1.1\n\nx_data = Vector{Float64}(undef, T)\ny_data = Vector{Float64}(undef, T)\n\nx_t_min_data = x_0_data\nfor t=1:T\n    global x_data[t] = a_data*x_t_min_data\n    global y_data[t] = ceil(x_data[t])\n    global x_t_min_data = x_data[t]\nend    \n;","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"# Inspect data\nbar(1:T, y_data, xlabel = \"t [days]\", ylabel = \"y [infected]\", label = false)","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"(Image: )","category":"page"},{"location":"examples/Nonlinear Virus Spread/#Model-specification","page":"Nonlinear Virus Spread","title":"Model specification","text":"","category":"section"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"We define a state-space model, where we specify the state transition by a g function. ","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"g(x_t_min, a) = a * x_t_min","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"g (generic function with 1 method)","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"@model function virus_spread(n)\n    x = randomvar(n)\n    y = datavar(Float64, n)\n\n    a ~ NormalMeanVariance(0.0, 100.0)\n    x_0 ~ NormalMeanVariance(1.0, 10.0)\n\n    x_prev = x_0\n    for i in 1:n\n        x[i] ~ g(x_prev, a)\n        y[i] ~ NormalMeanVariance(x[i], 0.1)\n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"meta = @meta begin \n    # `meta` specifies the approximation method, which will be used \n    # to approximate the nonlinearity\n    g() -> Linearization()\nend","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"Meta specification:\n  g() -> Linearization()\nOptions:\n  warn = true","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"result = inference(\n    model = virus_spread(length(y_data)), \n    data = (y = y_data,),\n    meta = meta,\n    options = (limit_stack_depth = 100, ),\n    returnvars = KeepLast(), \n    initmessages = (a = NormalMeanVariance(0.0, 10.0), ), \n)","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"Inference results:\n  Posteriors       | available for (a, x_0, x)","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"m_x_t, v_x_t = mean.(result.posteriors[:x]), cov.(result.posteriors[:x]);\nm_a, v_a = mean.(result.posteriors[:a]), cov.(result.posteriors[:a]);","category":"page"},{"location":"examples/Nonlinear Virus Spread/#Inference-results","page":"Nonlinear Virus Spread","title":"Inference results","text":"","category":"section"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"p1 = bar(1:T, y_data, label=false)\np1 = plot!(p1, 1:T, m_x_t, color=:black, ribbon=sqrt.(v_x_t), fillalpha=0.3, xlim=(1,T), ylabel=\"y [infected]\", label=false)\np2 = plot(1:T, m_a*ones(T,), color=:black, ribbon=sqrt.(v_a)*ones(T,), fillalpha=0.3, xlim=(1,T), ylabel=\"a [repr. rate]\", label=\"inferred\")\np2 = plot!(p2, 1:T, a_data*ones(T), linestyle=:dash, label=\"true \", ylim = (0, 2))\n\nrθ = range(1.05, 1.15, length = 1000)\n\np3 = plot(title = \"Inference results\", xlim = (1.05, 1.15))\n\np3 = plot!(p3, rθ, (x) -> pdf(Normal(0.0, 100.0), x), fillalpha=0.3, fillrange = 0, label=\"P(a)\", c=1,)\np3 = plot!(p3, rθ, (x) -> pdf(Normal(mean(result.posteriors[:a]), var(result.posteriors[:a])), x), fillalpha=0.3, fillrange = 0, label=\"P(a|y)\", c=3)\np3 = vline!(p3, [ a_data ], label=\"Real θ\", linestyle=:dash, color = :red)\n\nplot(p1, p2, p3,  layout = @layout([a; b c]))","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"(Image: )","category":"page"},{"location":"examples/Nonlinear Virus Spread/","page":"Nonlinear Virus Spread","title":"Nonlinear Virus Spread","text":"As we can see the inference results match hidden states with high precision, as well as for the a parameter.","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Hidden Markov Model/#examples-ensemble-learning-of-a-hidden-markov-model","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"","category":"section"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"In this demo we are interested in Bayesian inference of parameters of a hidden Markov model (HMM)., Specifically, we consider a first-order HMM with hidden states s_0 s_1 dots s_T and observations x_1 dots x_T governed by a state transition probability matrix A and an observation probability matrix B:","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"beginaligned\n    s_t  sim mathcalCat(A s_t-1)\n    x_t  sim mathcalCat(B s_t)\nendaligned","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"We assume three possible states (\\\"red\\\", \\\"green\\\" and \\\"blue\\\"), and the goal is to estimate matrices A and B from a simulated data set. To have a full Bayesian treatment of the problem, both A and B are endowed with priors (Dirichlet distributions on the columns).\"","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"using RxInfer, Random, BenchmarkTools, Distributions, LinearAlgebra, Plots","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"function rand_vec(rng, distribution::Categorical) \n    k = ncategories(distribution)\n    s = zeros(k)\n    s[ rand(rng, distribution) ] = 1.0\n    s\nend\n\nfunction generate_data(n_samples; seed = 124)\n    \n    rng = MersenneTwister(seed)\n    \n    # Transition probabilities (some transitions are impossible)\n    A = [0.9 0.0 0.1; 0.1 0.9 0.0; 0.0 0.1 0.9] \n    # Observation noise\n    B = [0.9 0.05 0.05; 0.05 0.9 0.05; 0.05 0.05 0.9] \n    # Initial state\n    s_0 = [1.0, 0.0, 0.0] \n    # Generate some data\n    s = Vector{Vector{Float64}}(undef, n_samples) # one-hot encoding of the states\n    x = Vector{Vector{Float64}}(undef, n_samples) # one-hot encoding of the observations\n    \n    s_prev = s_0\n    \n    for t = 1:n_samples\n        a = A * s_prev\n        s[t] = rand_vec(rng, Categorical(a ./ sum(a)))\n        b = B * s[t]\n        x[t] = rand_vec(rng, Categorical(b ./ sum(b)))\n        s_prev = s[t]\n    end\n    \n    return x, s\nend","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"# Test data\nN = 100\nx_data, s_data = generate_data(N);","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"scatter(argmax.(s_data))","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"(Image: )","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"# Model specification\n@model function hidden_markov_model(n)\n    \n    A ~ MatrixDirichlet(ones(3, 3)) \n    B ~ MatrixDirichlet([ 10.0 1.0 1.0; 1.0 10.0 1.0; 1.0 1.0 10.0 ])\n    \n    s_0 ~ Categorical(fill(1.0 / 3.0, 3))\n    \n    s = randomvar(n)\n    x = datavar(Vector{Float64}, n)\n    \n    s_prev = s_0\n    \n    for t in 1:n\n        s[t] ~ Transition(s_prev, A) \n        x[t] ~ Transition(s[t], B)\n        s_prev = s[t]\n    end\n    \nend\n\n@constraints function hidden_markov_model_constraints()\n    q(s_0, s, A, B) = q(s_0, s)q(A)q(B)\nend","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"hidden_markov_model_constraints (generic function with 1 method)","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"idata = (x = x_data, )\n\nimodel = hidden_markov_model(N)\n\nimarginals = (\n    A = vague(MatrixDirichlet, 3, 3), \n    B = vague(MatrixDirichlet, 3, 3), \n    s = vague(Categorical, 3)\n)\n\nireturnvars = (\n    A = KeepLast(),\n    B = KeepLast(),\n    s = KeepLast()\n)\n\nresult = inference(\n    model         = imodel, \n    data          = idata,\n    constraints   = hidden_markov_model_constraints(),\n    initmarginals = imarginals, \n    returnvars    = ireturnvars, \n    iterations    = 20, \n    free_energy   = true\n);","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"mean(result.posteriors[:A])","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"3×3 Matrix{Float64}:\n 0.892911   0.0830398  0.133822\n 0.0858821  0.691112   0.042798\n 0.0212073  0.225849   0.82338","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"mean(result.posteriors[:B])","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"3×3 Matrix{Float64}:\n 0.908176   0.0422229  0.0718142\n 0.0694807  0.882211   0.0430307\n 0.0223437  0.0755656  0.885155","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"p1 = scatter(argmax.(s_data), title=\"Inference results\", label = \"real\", ms = 6)\np1 = scatter!(p1, argmax.(ReactiveMP.probvec.(result.posteriors[:s])), label = \"inferred\", ms = 2)\np2 = plot(result.free_energy, label=\"Free energy\")\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"(Image: )","category":"page"},{"location":"examples/Hidden Markov Model/#Benchmark-timings","page":"Ensemble Learning of a Hidden Markov Model","title":"Benchmark timings","text":"","category":"section"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"@benchmark inference(\n    model         = $imodel, \n    data          = $idata,\n    constraints   = hidden_markov_model_constraints(),\n    initmarginals = $imarginals, \n    returnvars    = $ireturnvars, \n    iterations    = 20, \n    free_energy   = true\n)","category":"page"},{"location":"examples/Hidden Markov Model/","page":"Ensemble Learning of a Hidden Markov Model","title":"Ensemble Learning of a Hidden Markov Model","text":"BenchmarkTools.Trial: 70 samples with 1 evaluation.\n Range (min … max):  59.763 ms … 122.878 ms  ┊ GC (min … max): 0.00% …  0.0\n0%\n Time  (median):     63.867 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   71.694 ms ±  17.353 ms  ┊ GC (mean ± σ):  9.01% ± 14.0\n5%\n\n    █▃▅                                                         \n  ▅█████▆▁▁▁▄▁▁▃▃▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▆▄▁▁▃▁▃▁▁▃▁▁▁▁▁▃ ▁\n  59.8 ms         Histogram: frequency by time          120 ms <\n\n Memory estimate: 23.38 MiB, allocs estimate: 398532.","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Active Inference Mountain car/#examples-active-inference-mountain-car","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"","category":"section"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"This notebooks covers fundamentals of the Active Inference framework implemented with the Bethe Free Energy optimisation with message passing on factor graphs. We use the mountain car problem as a simple example.","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"The original code has been written by Thijs van de Laar and adapted by Dmitry Bagaev. Visuals have been coded by Sepideh Adamiat.","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"We refer reader to the Thijs van de Laar (2019) \"Simulating active inference processes by message passing\" original paper with more in-depth overview and explanation of the active inference agent implementation by message passing.\nThe original environment/task description is from Ueltzhoeffer (2017) \"Deep active inference\".","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"import Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"using RxInfer, Plots","category":"page"},{"location":"examples/Active Inference Mountain car/#The-mountain-and-physics","page":"Active Inference Mountain car","title":"The mountain and physics","text":"","category":"section"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"For the purpose of this example we create a simple mountain valley with hard-coded physics, such that we do not depend on any external complex library. We have several configurable parameters for the experiment:","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Engine-force limit\nTires friction coefficient","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"import HypergeometricFunctions: _₂F₁\n\nfunction create_physics(; engine_force_limit = 0.04, friction_coefficient = 0.1)\n    \n    # Engine force as function of action\n    Fa = (a::Real) -> engine_force_limit * tanh(a) \n    # Derivative of the engine force\n    Fa_prime = (a::Real) -> engine_force_limit - engine_force_limit * tanh(a)^2 \n    # Friction force as function of velocity\n    Ff = (y_dot::Real) -> -friction_coefficient * y_dot \n    # Derivative of the friction force\n    Ff_prime = (y_dot::Real) -> -friction_coefficient \n    \n    # Gravitational force (horizontal component) as function of position\n    Fg = (y::Real) -> begin\n        if y < 0\n            0.05*(-2*y - 1)\n        else\n            0.05*(-(1 + 5*y^2)^(-0.5) - (y^2)*(1 + 5*y^2)^(-3/2) - (y^4)/16)\n        end\n    end\n\n    # Derivative of the gravitational force\n    Fg_prime = (y::Real) -> begin \n        if y < 0\n            -0.1\n        else\n            0.05*((-4*y^3)/16 + (5*y)/(1 + 5*y^2)^1.5 + (3*5*y^3)/(1 + 5*y^2)^(5/2) - (2*y)/(1 + 5*y^2)^(3/2))\n        end\n    end\n    \n    # The height of the landscape as a function of the horizontal coordinate\n    height = (x::Float64) -> begin\n        if x < 0\n            h = x^2 + x\n        else\n            h = x * _₂F₁(0.5,0.5,1.5, -5*x^2) + x^3 * _₂F₁(1.5, 1.5, 2.5, -5*x^2) / 3 + x^5 / 80\n        end\n        return 0.05*h\n    end\n    \n    return (Fa, Fa_prime, Ff, Ff_prime, Fg, Fg_prime, height)\nend","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"create_physics (generic function with 1 method)","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"engine_force_limit   = 0.04\nfriction_coefficient = 0.1\n\nFa, Fa_prime, Ff, Ff_prime, Fg, Fg_prime, height = create_physics(\n    engine_force_limit = engine_force_limit,\n    friction_coefficient = friction_coefficient\n);\n\ninitial_position = -0.5\ninitial_velocity = 0.0\n\nx_target = [0.5, 0.0] \n\nvalley_x = range(-2, 2, length=400)\nvalley_y = [ height(xs) for xs in valley_x ]\nplot(valley_x, valley_y, title = \"Mountain valley\", label = \"Landscape\", color = \"black\")\nscatter!([ initial_position ], [ height(initial_position) ], label=\"initial position\")   \nscatter!([x_target[1]], [height(x_target[1])], label=\"goal\")","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"(Image: )","category":"page"},{"location":"examples/Active Inference Mountain car/#World-agent-interaction","page":"Active Inference Mountain car","title":"World - agent interaction","text":"","category":"section"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Because the states of the world are unknown to the agent, we wrap them in a comprehension. The comprehension returns only the functions for interacting with the world and not the hidden states. This way, we introduce a stateful world whose states cannot be directly observed.","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"function create_world(; Fg, Ff, Fa, initial_position = -0.5, initial_velocity = 0.0)\n\n    y_t_min = initial_position\n    y_dot_t_min = initial_velocity\n    \n    y_t = y_t_min\n    y_dot_t = y_dot_t_min\n    \n    execute = (a_t::Float64) -> begin\n        # Compute next state\n        y_dot_t = y_dot_t_min + Fg(y_t_min) + Ff(y_dot_t_min) + Fa(a_t)\n        y_t = y_t_min + y_dot_t\n    \n        # Reset state for next step\n        y_t_min = y_t\n        y_dot_t_min = y_dot_t\n    end\n    \n    observe = () -> begin \n        return [y_t, y_dot_t]\n    end\n        \n    return (execute, observe)\nend","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"create_world (generic function with 1 method)","category":"page"},{"location":"examples/Active Inference Mountain car/#Naive-approach","page":"Active Inference Mountain car","title":"Naive approach","text":"","category":"section"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"In this simulation we are going to perform a naive action policy for tight full-power only. In this case, with limited engine power, the agent should not be able to achieve its goal:","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"N_naive  = 100 # Total simulation time\npi_naive = 100.0 * ones(N_naive) # Naive policy for right full-power only\n\n# Let there be a world\n(execute_naive, observe_naive) = create_world(; \n    Fg = Fg, Ff = Ff, Fa = Fa, \n    initial_position = initial_position, \n    initial_velocity = initial_velocity\n);\n\ny_naive = Vector{Vector{Float64}}(undef, N_naive)\nfor t = 1:N_naive\n    execute_naive(pi_naive[t]) # Execute environmental process\n    y_naive[t] = observe_naive() # Observe external states\nend\n\nanimation_naive = @animate for i in 1:N_naive\n    plot(valley_x, valley_y, title = \"Naive policy\", label = \"Landscape\", color = \"black\", size = (800, 400))\n    scatter!([y_naive[i][1]], [height(y_naive[i][1])], label=\"car\")\n    scatter!([x_target[1]], [height(x_target[1])], label=\"goal\")   \nend\n\ngif(animation_naive, \"./../assets/examples/ai-mountain-car-naive.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"(Image: )","category":"page"},{"location":"examples/Active Inference Mountain car/#Active-inference-approach","page":"Active Inference Mountain car","title":"Active inference approach","text":"","category":"section"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"In the active inference approach we are going to create an agent that models the environment around itself as well as the best possible actions in a probabilistic manner. That should help agent to understand that the brute-force approach is not the most efficient one and hopefully to realise that a little bit of swing is necessary to achieve its goal.","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"The code in the next block defines the agent's internal beliefs over the external dynamics and its probabilistic model of the environment, which correspond accurately by directly using the functions defined above. We use the @model macro from RxInfer to define the probabilistic model and the meta block to define approximation methods for the nonlinear state-transition functions.","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"In the model specification we in addition to the current state of the agent we include the beliefs over its future states (up to T steps ahead):","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"@model function mountain_car(; T, Fg, Fa, Ff, engine_force_limit)\n    \n    # Transition function modeling transition due to gravity and friction\n    g = (s_t_min::AbstractVector) -> begin \n        s_t = similar(s_t_min) # Next state\n        s_t[2] = s_t_min[2] + Fg(s_t_min[1]) + Ff(s_t_min[2]) # Update velocity\n        s_t[1] = s_t_min[1] + s_t[2] # Update position\n        return s_t\n    end\n    \n    # Function for modeling engine control\n    h = (u::AbstractVector) -> [0.0, Fa(u[1])] \n    \n    # Inverse engine force, from change in state to corresponding engine force\n    h_inv = (delta_s_dot::AbstractVector) -> [atanh(clamp(delta_s_dot[2], -engine_force_limit+1e-3, engine_force_limit-1e-3)/engine_force_limit)] \n    \n    # Internal model perameters\n    Gamma = 1e4*diageye(2) # Transition precision\n    Theta = 1e-4*diageye(2) # Observation variance\n    \n    m_s_t_min = datavar(Vector{Float64})\n    V_s_t_min = datavar(Matrix{Float64})\n\n    s_t_min ~ MvNormal(mean = m_s_t_min, cov = V_s_t_min)\n    s_k_min = s_t_min\n    \n    m_u = datavar(Vector{Float64}, T)\n    V_u = datavar(Matrix{Float64}, T)\n    \n    m_x = datavar(Vector{Float64}, T)\n    V_x = datavar(Matrix{Float64}, T)\n    \n    u = randomvar(T)\n    s = randomvar(T)\n    x = randomvar(T)\n    \n    u_h_k = randomvar(T)\n    s_g_k = randomvar(T)\n    u_s_sum = randomvar(T)\n    \n    for k in 1:T\n        u[k] ~ MvNormal(mean = m_u[k], cov = V_u[k])\n        u_h_k[k] ~ h(u[k]) where { meta = DeltaMeta(method = Linearization(), inverse = h_inv) }\n        s_g_k[k] ~ g(s_k_min) where { meta = DeltaMeta(method = Linearization()) }\n        u_s_sum[k] ~ s_g_k[k] + u_h_k[k]\n        s[k] ~ MvNormal(mean = u_s_sum[k], precision = Gamma)\n        x[k] ~ MvNormal(mean = s[k], cov = Theta)\n        x[k] ~ MvNormal(mean = m_x[k], cov = V_x[k]) # goal\n        s_k_min = s[k]\n    end\n    \n    return (s, )\nend","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Because states of the agent are unknown to the world, we wrap them in a comprehension. The comprehension only returns functions for interacting with the agent. Internal beliefs cannot be directly observed, and interaction is only allowed through the Markov blanket","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"# We are going to use some private functionality from ReactiveMP, \n# in the future we should expose a proper API for this\nimport RxInfer.ReactiveMP: getrecent, messageout\n\nfunction create_agent(; T = 20, Fg, Fa, Ff, engine_force_limit, x_target, initial_position, initial_velocity)\n    Epsilon = fill(huge, 1, 1)                # Control prior variance\n    m_u = Vector{Float64}[ [ 0.0] for k=1:T ] # Set control priors\n    V_u = Matrix{Float64}[ Epsilon for k=1:T ]\n\n    Sigma    = 1e-4*diageye(2) # Goal prior variance\n    m_x      = [zeros(2) for k=1:T]\n    V_x      = [huge*diageye(2) for k=1:T]\n    V_x[end] = Sigma # Set prior to reach goal at t=T\n\n    # Set initial brain state prior\n    m_s_t_min = [initial_position, initial_velocity] \n    V_s_t_min = tiny * diageye(2)\n    \n    # Set current inference results\n    result = nothing\n\n    # The `infer` function is the heart of the agent\n    # It calls the `RxInfer.inference` function to perform Bayesian inference by message passing\n    infer = (upsilon_t::Float64, y_hat_t::Vector{Float64}) -> begin\n        m_u[1] = [ upsilon_t ] # Register action with the generative model\n        V_u[1] = fill(tiny, 1, 1) # Clamp control prior to performed action\n\n        m_x[1] = y_hat_t # Register observation with the generative model\n        V_x[1] = tiny*diageye(2) # Clamp goal prior to observation\n\n        data = Dict(:m_u       => m_u, \n                    :V_u       => V_u, \n                    :m_x       => m_x, \n                    :V_x       => V_x,\n                    :m_s_t_min => m_s_t_min,\n                    :V_s_t_min => V_s_t_min)\n        \n        model  = mountain_car(; T = T, Fg = Fg, Fa = Fa, Ff = Ff, engine_force_limit = engine_force_limit) \n        result = inference(model = model, data = data)\n    end\n    \n    # The `act` function returns the inferred best possible action\n    act = () -> begin\n        if result !== nothing\n            return mode(result.posteriors[:u][2])[1]\n        else\n            return 0.0 # Without inference result we return some 'random' action\n        end\n    end\n    \n    # The `future` function returns the inferred future states\n    future = () -> begin \n        if result !== nothing \n            return getindex.(mode.(result.posteriors[:s]), 1)\n        else\n            return zeros(T)\n        end\n    end\n\n    # The `slide` function modifies the `(m_s_t_min, V_s_t_min)` for the next step\n    # and shifts (or slides) the array of future goals `(m_x, V_x)` and inferred actions `(m_u, V_u)`\n    slide = () -> begin\n        (s, ) = result.returnval\n        \n        slide_msg_idx = 3 # This index is model dependend\n        (m_s_t_min, V_s_t_min) = mean_cov(getrecent(messageout(s[2], slide_msg_idx)))\n\n        m_u = circshift(m_u, -1)\n        m_u[end] = [0.0]\n        V_u = circshift(V_u, -1)\n        V_u[end] = Epsilon\n\n        m_x = circshift(m_x, -1)\n        m_x[end] = x_target\n        V_x = circshift(V_x, -1)\n        V_x[end] = Sigma\n    end\n\n    return (infer, act, slide, future)    \nend","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"create_agent (generic function with 1 method)","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"(execute_ai, observe_ai) = create_world(\n    Fg = Fg, Ff = Ff, Fa = Fa, \n    initial_position = initial_position, \n    initial_velocity = initial_velocity\n) # Let there be a world\n\nT_ai = 50\n\n(infer_ai, act_ai, slide_ai, future_ai) = create_agent(; # Let there be an agent\n    T  = T_ai, \n    Fa = Fa,\n    Fg = Fg, \n    Ff = Ff, \n    engine_force_limit = engine_force_limit,\n    x_target = x_target,\n    initial_position = initial_position,\n    initial_velocity = initial_velocity\n) \n\nN_ai = 100\n\n# Step through experimental protocol\nagent_a = Vector{Float64}(undef, N_ai) # Actions\nagent_f = Vector{Vector{Float64}}(undef, N_ai) # Predicted future\nagent_x = Vector{Vector{Float64}}(undef, N_ai) # Observations\n\nfor t=1:N_ai\n    agent_a[t] = act_ai()            # Invoke an action from the agent\n    agent_f[t] = future_ai()         # Fetch the predicted future states\n    execute_ai(agent_a[t])           # The action influences hidden external states\n    agent_x[t] = observe_ai()        # Observe the current environmental outcome (update p)\n    infer_ai(agent_a[t], agent_x[t]) # Infer beliefs from current model state (update q)\n    slide_ai()                       # Prepare for next iteration\nend\n\nanimation_ai = @animate for i in 1:N_ai\n    # pls - plot landscape\n    pls = plot(valley_x, valley_y, title = \"Active inference results\", label = \"Landscape\", color = \"black\")\n    pls = scatter!(pls, [agent_x[i][1]], [height(agent_x[i][1])], label=\"car\")\n    pls = scatter!(pls, [x_target[1]], [height(x_target[1])], label=\"goal\")   \n    pls = scatter!(pls, agent_f[i], height.(agent_f[i]), label = \"Predicted future\", alpha = map(i -> 0.5 / i, 1:T_ai))\n    \n    # pef - plot engine force\n    pef = plot(Fa.(agent_a[1:i]), title = \"Engine force (agents actions)\", xlim = (0, N_ai), ylim = (-0.05, 0.05))\n    \n    plot(pls, pef, size = (800, 400))\nend\n    \ngif(animation_ai, \"./../assets/examples/ai-mountain-car-ai.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"(Image: )","category":"page"},{"location":"examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"As we can see the agent does indeed swing in order to reach its goal. Its interesting though that in the beginning the agent does not attempt to do that but only after some time has passed. That can be explained by the fact that we set T_ai = 50, which means that the agent must reach its goal after 50 time steps. In the beginning of the simulation this time horizon appears to be so far in the future that the agent decides not to do anything (in this way the Active Inference agent proved that procrastinating is smart!). After around 30 time steps the goal target becomes closer in time (agent has less than 20 time steps left to achieve the goal) and agent finally decides to act, predicts its future states and realises that in order to achieve its goal it must swing.","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/#examples-gaussian-linear-dynamical-system","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"","category":"section"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"In this example the goal is to estimate hidden states of a Linear Dynamical process where all hidden states are Gaussians. A simple multivariate Linear Gaussian State Space Model can be described with the following equations:","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"beginaligned\n p(x_ix_i - 1)  = mathcalN(x_iA * x_i - 1 mathcalP)\n p(y_ix_i)  = mathcalN(y_iB * x_i mathcalQ)\nendaligned","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"where x_i are hidden states, y_i are noisy observations, A, B are state transition and observational matrices, mathcalP and mathcalQ are state transition noise and observation noise covariance matrices. For a more rigorous introduction to Linear Gaussian Dynamical systems we refer to Simo Sarkka, Bayesian Filtering and Smoothing book.","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"To model this process in RxInfer, first, we start with importing all needed packages:","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"using RxInfer, BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"function generate_data(rng, A, B, Q, P)\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        x[i] = rand(rng, MvNormal(A * x_prev, Q))\n        y[i] = rand(rng, MvNormal(B * x[i], P))\n        x_prev = x[i]\n    end\n    \n    return x, y\nend","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"# Seed for reproducibility\nseed = 1234\n\nrng = MersenneTwister(1234)\n\n# We will model 2-dimensional observations with rotation matrix `A`\n# To avoid clutter we also assume that matrices `A`, `B`, `P` and `Q`\n# are known and fixed for all time-steps\nθ = π / 35\nA = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\nB = diageye(2)\nQ = diageye(2)\nP = 25.0 .* diageye(2)\n\n# Number of observations\nn = 300;","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"x, y = generate_data(rng, A, B, Q, P);","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"Let's plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations, which are represented as dots.","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = scatter!(px, getindex.(y, 1), label = false, markersize = 2, color = :orange)\npx = plot!(px, getindex.(x, 2), label = \"Hidden Signal (dim-2)\", color = :green)\npx = scatter!(px, getindex.(y, 2), label = false, markersize = 2, color = :green)\n\nplot(px)","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"(Image: )","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"To create a model we use GraphPPL package and @model macro:","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"@model function rotate_ssm(n, x0, A, B, Q, P)\n    \n    # We create constvar references for better efficiency\n    cA = constvar(A)\n    cB = constvar(B)\n    cQ = constvar(Q)\n    cP = constvar(P)\n    \n    # `x` is a sequence of hidden states\n    x = randomvar(n)\n    # `y` is a sequence of \"clamped\" observations\n    y = datavar(Vector{Float64}, n)\n    \n    x_prior ~ MvNormalMeanCovariance(mean(x0), cov(x0))\n    x_prev = x_prior\n    \n    for i in 1:n\n        x[i] ~ MvNormalMeanCovariance(cA * x_prev, cQ)\n        y[i] ~ MvNormalMeanCovariance(cB * x[i], cP)\n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"To run inference we also specify prior for out first hidden state:","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"x0 = MvNormalMeanCovariance(zeros(2), 100.0 * diageye(2));","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"# For large number of observations you need to use `limit_stack_depth = 100` option during model creation, e.g. \n# inference(..., options = (limit_stack_depth = 500, ))`\nresult = inference(\n    model = rotate_ssm(length(y), x0, A, B, Q, P), \n    data = (y = y,),\n    free_energy = true\n);\n\nxmarginals = result.posteriors[:x]\nbfe        = result.free_energy;","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = plot!(px, getindex.(x, 2), label = \"Hidden Signal (dim-2)\", color = :green)\n\npx = plot!(px, getindex.(mean.(xmarginals), 1), ribbon = getindex.(var.(xmarginals), 1) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :teal)\npx = plot!(px, getindex.(mean.(xmarginals), 2), ribbon = getindex.(var.(xmarginals), 2) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :violet)\n\nplot(px)","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"(Image: )","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the value for minus log evidence:","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"bfe","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"1-element Vector{Real}:\n 1882.2434870101347","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"We may be also interested in performance of our resulting Belief Propagation algorithm:","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"@benchmark inference(\n    model = rotate_ssm(length($y), $x0, $A, $B, $Q, $P), \n    data = (y = $y,)\n)","category":"page"},{"location":"examples/Gaussian Linear Dynamical System/","page":"Gaussian Linear Dynamical System","title":"Gaussian Linear Dynamical System","text":"BenchmarkTools.Trial: 120 samples with 1 evaluation.\n Range (min … max):  33.543 ms … 84.537 ms  ┊ GC (min … max): 0.00% … 50.46\n%\n Time  (median):     38.049 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   42.082 ms ± 12.391 ms  ┊ GC (mean ± σ):  8.43% ± 13.95\n%\n\n     ▄▆█                                                       \n  ▃▃▅████▆▄▆▃▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▃▃▄▃▁▃ ▃\n  33.5 ms         Histogram: frequency by time        83.7 ms <\n\n Memory estimate: 12.00 MiB, allocs estimate: 243002.","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Nonlinear Rabbit Population/#examples-nonlinear-smoothing:-rabbit-population","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"","category":"section"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"In this demo, we will look at dynamical systems with nonlinear state transitions. We will start with a one-dimensional problem; the number of rabbits on an island. This problem seems overly simple, but it is a good way to demonstrate the basic pipeline of working with RxInfer.","category":"page"},{"location":"examples/Nonlinear Rabbit Population/#.-Rabbit-population-size","page":"Nonlinear Smoothing: Rabbit Population","title":"1. Rabbit population size","text":"","category":"section"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"We consider a model for the number of rabbits on a particular island. We assume the population size follows a logistic map: initially, it grows exponentially, but as the number of rabbits increases, so does the number of foxes. At some point, population size will drop again. But as the number of rabbits drops, so does the number of foxes, which means the number of rabbits can grow again. We express the change in the population with the following nonlinear state transition:","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"x_t+1 = r cdot x_t(1-x_t)","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"where r is a fertility parameter, reflecting how fast the number of rabbits can grow. x does not reflect the number of rabbits, but rather the proportion of rabbits relative to a maximum population N on the island. This means x is bounded in the interval 01.","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"Every month, we count the number of rabbits y_t. We assume that these counts are noisy: sometimes, we count one rabbit twice and sometimes, we miss one. This noise is expressed as a Gaussian distribution centered at 0, with some measurement noise precision.","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"# Import libraries to julia workspace\nusing RxInfer, Plots, Distributions, StableRNGs","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"# Range for observation noise\nrng = StableRNG(1234);\n\n# Length of time-series\nT = 36\n\n# Maximum population size\nN = 100\n\n# Fertility parameter (0 < r < 4)\nfertility = 3.2\n\n# Measurement noise precision\nnoise_precision = 0.01\n\n# Initial proportion of rabbits\nx0 = 0.04\n\n# Initialize data array\nstates = zeros(T,)\nobservations = zeros(T,)\n\n# Initialize previous state variable\nprev_state = x0\n\nfor t = 1:T\n    \n    # State transition\n    global states[t] = fertility*prev_state*(1-prev_state)\n    \n    # Observation likelihood\n    global observations[t] = max(round(N*states[t] .+ sqrt(inv(noise_precision))*randn(rng,)[1]), 0.)\n    \n    # Update \"previous state\"\n    global prev_state = states[t]\n    \nend","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"# Inspect data\nplot(1:T, states.*N, xlabel = \"t (months)\", ylabel = \"rabbits\", label = \"true population\")\nscatter!(1:T, observations, ylims = [0, N], label = \"observed\", legend = :bottomright)","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"(Image: )","category":"page"},{"location":"examples/Nonlinear Rabbit Population/#Model-specification","page":"Nonlinear Smoothing: Rabbit Population","title":"Model specification","text":"","category":"section"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"We define a state-space model, consisting of a state transition between x_t-1 and x_t and an observation likelihood between y_t and x_t. These are conditional distributions, which we model with certain parametric distributions. In this case, mostly Gaussian distributions.","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"The states are the proportion of rabbits relative to a maximum N. To model the rabbit counts, we must multiply the states with the maximum population number N. Furthermore, we said that the rabbit counts are noisy (see 'Generate data' section). We assumed that we were counting some rabbits twice and missed others. That justifies using a white noise term: the current rabbit count is Gaussian distributed centered on the current state times the maximum population. For the sake of simplicity in this demo, we assume that we know the size of the noise. It is however straightforward to add a prior for measurement noise and estimate it simultaneously.\nThe state transition that we defined above (the logistic map) is nonlinear in nature (polynomial order 2). To capture this mapping, we have to use a \"Delta\" node. The \"Delta{Unscented}\" node performs an unscented transform to approximate the given function (exact up to polynomial order 3) and approximates the result with a Gaussian distribution.\nWe have to specify a prior distribution for the states. Below, we choose a Gaussian distribution, but this is actually not completely valid. In a model using the logistic map, the states are confined to the interval 0 1. We would therefore have to use a bounded distribution, such as the Beta. However, a Beta process is much more complicated than a Gaussian process and we will therefore avoid it here.\nWe have to specify a prior for the fertility parameter r. It is supposed to be a strictly positive number, so ideally we would use something like a Gamma or log-Normal distribution. However, this, again, complicates inference and we have therefore opted for a Gaussian distribution.","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"We are going to specify the state-space model in recursive form, i.e. we only specify the previous state, the state transition and the likelihood, and update estimates as observations arrive. We will probably observe that our estimates start out relatively poor but improve over time. ","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"g(x, r) = r*x*(1 - x)","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"g (generic function with 1 method)","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"@model function rabbit_population(n, precision = 10)\n    x = randomvar(n)\n    y = datavar(Float64, n)\n\n    r ~ Normal(μ = 1.0, σ² = 10.0)\n    x_0 ~ Normal(μ = 0.5, σ² = 2.0)\n\n    x_prev = x_0\n    for i in 1:n\n        x[i] ~ g(x_prev, r)\n        y[i] ~ Normal(μ = N * x[i], γ = precision)\n        \n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"meta = @meta begin \n    # `meta` specifies the approximation method, which will\n    # be used to approximate the nonlinearity\n    g() -> Unscented(alpha = 1.1)\nend","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"Meta specification:\n  g() -> Unscented{Float64, Float64, Float64, Nothing}(1.1, 2.0, 0.0, nothi\nng)\nOptions:\n  warn = true","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"result = inference(\n    model = rabbit_population(length(observations), noise_precision,), \n    meta = meta,\n    initmessages = (r = NormalMeanVariance(1.0, 1.0), ), \n    data = (y = observations,), \n    showprogress = false,\n    returnvars = KeepLast(),\n    iterations = 15\n)","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"Inference results:\n  Posteriors       | available for (x_0, r, x)","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"Lets also check the inference results:","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"m_x_t, v_x_t = mean.(result.posteriors[:x]), cov.(result.posteriors[:x]);","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"# Plot estimate of fertility parameter r\nusing StatsPlots\n\n# Plot true states and overlay estimates\np1 = plot(1:T, states, linewidth=2, ylims=[0., 1.], xlabel=\"time (months)\", ylabel=\"x (proportion rabbits)\", label=\"true\", legend=:bottomright)\np1 = plot!(p1, 1:T, m_x_t, linewidth=4, color=\"red\", ribbon=[sqrt.(v_x_t) sqrt.(v_x_t)], alpha=0.6, label=\"estimated\")\np1 = scatter!(p1, 1:T, observations./N, color=\"black\", markersize=3, label=\"observed\", legend=:bottomright)\np1 = title!(p1, \"Rabbit population size, relative to max\")\n\nestimated = Normal(mean(result.posteriors[:r]), std(result.posteriors[:r]))\nprior = Normal(1.0, 1.0)\np2 = plot(prior, label=\"prior\", fillalpha=0.3, fillrange = 0, legend = :topleft)\np2 = plot!(p2, estimated, label=\"posterior\", fillalpha=0.3, fillrange = 0)\np2 = vline!(p2, [ fertility ], linecolor = :red, linestyle = :dash, label=\"true\")\n\np2 = title!(p2, \"Estimate of fertility parameter\")\np2 = xlabel!(p2, \"r (fertility)\")\np2 = ylabel!(p2, \"probability density\")\np2 = xlims!(p2, -1, 4)\n\nplot(p1, p2, layout = @layout([ a; b ]), size = (750, 600))","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"(Image: )","category":"page"},{"location":"examples/Nonlinear Rabbit Population/","page":"Nonlinear Smoothing: Rabbit Population","title":"Nonlinear Smoothing: Rabbit Population","text":"As we can see the inferred results match the real hidden state with high precision, as well as the inferred fertility parameter.","category":"page"},{"location":"manuals/inference/rxinference/#user-guide-rxinference","page":"Real-time dataset / reactive inference","title":"Automatic inference specification on real-time datasets","text":"","category":"section"},{"location":"manuals/inference/rxinference/","page":"Real-time dataset / reactive inference","title":"Real-time dataset / reactive inference","text":"RxInfer exports the rxinference function to quickly run and test you model with dynamic and potentially real-time datasets. Note, however, that this function does cover almost all capabilities of the reactive inference engine, but for advanced use cases you may want to resort to the manual inference specification.","category":"page"},{"location":"manuals/inference/rxinference/","page":"Real-time dataset / reactive inference","title":"Real-time dataset / reactive inference","text":"For running inference on static datasets see the Static Inference section. For manual inference specification see the Manual Inference section.","category":"page"},{"location":"manuals/inference/rxinference/","page":"Real-time dataset / reactive inference","title":"Real-time dataset / reactive inference","text":"rxinference\nRxInfer.start\nRxInfer.stop\n@autoupdates\nRxInferenceEngine\nRxInferenceEvent","category":"page"},{"location":"manuals/inference/rxinference/#RxInfer.rxinference","page":"Real-time dataset / reactive inference","title":"RxInfer.rxinference","text":"rxinference(\n    model,\n    data = nothing,\n    datastream = nothing,\n    initmarginals = nothing,\n    initmessages = nothing,\n    autoupdates = nothing,\n    constraints = nothing,\n    meta = nothing,\n    options = nothing,\n    returnvars = nothing,\n    historyvars = nothing,\n    keephistory = nothing,\n    iterations = nothing,\n    free_energy = false,\n    free_energy_diagnostics = BetheFreeEnergyDefaultChecks,\n    autostart = true,\n    events = nothing,\n    callbacks = nothing,\n    addons = nothing,\n    postprocess = DefaultPostprocess(),\n    uselock = false,\n    warn = true\n)\n\nThis function provides a generic way to perform probabilistic inference in RxInfer.jl. Returns RxInferenceEngine.\n\nArguments\n\nFor more information about some of the arguments, please check below.\n\nmodel: specifies a model generator, required\ndata: NamedTuple or Dict with data, required (or datastream)\ndatastream: A stream of NamedTuple with data, required (or data)\ninitmarginals = nothing: NamedTuple or Dict with initial marginals, optional\ninitmessages = nothing: NamedTuple or Dict with initial messages, optional\nautoupdates = nothing: auto-updates specification, required for many models, see @autoupdates\nconstraints = nothing: constraints specification object, optional, see @constraints\nmeta  = nothing: meta specification object, optional, may be required for some models, see @meta\noptions = nothing: model creation options, optional, see ModelInferenceOptions\nreturnvars = nothing: return structure info, optional, by default creates observables for all random variables that return posteriors at last vmp iteration, see below for more information\nhistoryvars = nothing: history structure info, optional, defaults to no history, see below for more information\nkeephistory = nothing: history buffer size, defaults to empty buffer, see below for more information\niterations = nothing: number of iterations, optional, defaults to nothing, the inference engine does not distinguish between variational message passing or Loopy belief propagation or expectation propagation iterations, see below for more information\nfree_energy = false: compute the Bethe free energy, optional, defaults to false. Can be passed a floating point type, e.g. Float64, for better efficiency, but disables automatic differentiation packages, such as ForwardDiff.jl\nfree_energy_diagnostics = BetheFreeEnergyDefaultChecks: free energy diagnostic checks, optional, by default checks for possible NaNs and Infs. nothing disables all checks.\nautostart = true: specifies whether to call RxInfer.start on the created engine automatically or not\nshowprogress = false: show progress module, optional, defaults to false\nevents = nothing: inference cycle events, optional, see below for more info\ncallbacks = nothing: inference cycle callbacks, optional, see below for more info\naddons = nothing: inject and send extra computation information along messages, see below for more info\npostprocess = DefaultPostprocess(): inference results postprocessing step, optional, see below for more info\nuselock = false: specifies either to use the lock structure for the inference or not, if set to true uses Base.Threads.SpinLock. Accepts custom AbstractLock.\nwarn = true: enables/disables warnings\n\nNote on NamedTuples\n\nWhen passing NamedTuple as a value for some argument, make sure you use a trailing comma for NamedTuples with a single entry. The reason is that Julia treats returnvars = (x = KeepLast()) and returnvars = (x = KeepLast(), ) expressions differently. This first expression creates (or overwrites!) new local/global variable named x with contents KeepLast(). The second expression (note trailing comma) creates NamedTuple with x as a key and KeepLast() as a value assigned for this key.\n\nExtended information about some of the arguments\n\ndata or datastream\n\nEither data or datastream keyword argument is required, but specifying both is not supported and will result in an error.\n\ndata\n\nThe data keyword argument must be a NamedTuple (or Dict) where keys (of Symbol type) correspond to all datavars defined in the model specification. For example, if a model defines x = datavar(Float64) the data field must have an :x key (of Symbol type) which holds an iterable container with values of type Float64. The elements of such containers in the data must have the exact same shape as the datavar container. In other words, if a model defines x = datavar(Float64, n) then data[:x] must provide an iterable container with elements of type Vector{Float64}. \n\nAll entries in the data argument are zipped together with the Base.zip function to form one slice of the data chunck. This means all containers in the data argument must be of the same size (zip iterator finished as soon as one container has no remaining values). In order to use a fixed value for some specific datavar it is not necessary to create a container with that fixed value, but rather more efficient to use Iterators.repeated to create an infinite iterator.\n\nNote: The behavior of the data keyword argument is different from that which is used in the inference function.\n\ndatastream\n\nThe datastream keyword argument must be an observable that supports subscribe! and unsubscribe! functions (streams from the Rocket.jl package are also supported). The elements of the observable must be of type NamedTuple where keys (of Symbol type) correspond to all datavars defined in the model specification, except for those which are listed in the autoupdates specification.  For example, if a model defines x = datavar(Float64) (which is not part of the autoupdates specification) the named tuple from the observable must have an :x key (of Symbol type) which holds a value of type Float64. The values in the named tuple must have the exact same shape as the datavar container. In other words, if a model defines x = datavar(Float64, n) then  namedtuple[:x] must provide a container with length n and with elements of type Float64.\n\nNote: The behavior of the individual named tuples from the datastream observable is similar to that which is used in the inference function and its data argument. In fact, you can see the rxinference function as an efficient streamed version of the inference function, which automatically updates some datavars with the autoupdates specification and listens to the datastream to update the rest of the datavars.\n\nmodel\n\nThe model argument accepts a ModelGenerator as its input. The easiest way to create the ModelGenerator is to use the @model macro.  For example:\n\n@model function coin_toss(some_argument, some_keyword_argument = 3)\n   ...\nend\n\nresult = rxinference(\n    model = coin_toss(some_argument; some_keyword_argument = 3)\n)\n\nNote: The model keyword argument does not accept a FactorGraphModel instance as a value, as it needs to inject constraints and meta during the inference procedure.\n\ninitmarginals\n\nFor specific types of inference algorithms, such as variational message passing, it might be required to initialize (some of) the marginals before running the inference procedure in order to break the dependency loop. If this is not done, the inference algorithm will not be executed due to the lack of information and message and/or marginals will not be updated. In order to specify these initial marginals, you can use the initmarginals argument, such as\n\nrxinference(...\n    initmarginals = (\n        # initialize the marginal distribution of x as a vague Normal distribution\n        # if x is a vector, then it simply uses the same value for all elements\n        # However, it is also possible to provide a vector of distributions to set each element individually \n        x = vague(NormalMeanPrecision),  \n    ),\n)\n\nThis argument needs to be a named tuple, i.e. initmarginals = (a = ..., ), or dictionary.\n\ninitmessages\n\nFor specific types of inference algorithms, such as loopy belief propagation or expectation propagation, it might be required to initialize (some of) the messages before running the inference procedure in order to break the dependency loop. If this is not done, the inference algorithm will not be executed due to the lack of information and message and/or marginals will not be updated. In order to specify these initial messages, you can use the initmessages argument, such as\n\nrxinference(...\n    initmessages = (\n        # initialize the messages distribution of x as a vague Normal distribution\n        # if x is a vector, then it simply uses the same value for all elements\n        # However, it is also possible to provide a vector of distributions to set each element individually \n        x = vague(NormalMeanPrecision),  \n    ),\n)\n\nThis argument needs to be a named tuple, i.e. initmessages = (a = ..., ), or dictionary.\n\nautoupdates\n\nSee @autoupdates for more information.\n\noptions\nlimit_stack_depth: limits the stack depth for computing messages, helps with StackOverflowError for some huge models, but reduces the performance of inference backend. Accepts integer as an argument that specifies the maximum number of recursive depth. Lower is better for stack overflow error, but worse for performance.\npipeline: changes the default pipeline for each factor node in the graph\nglobal_reactive_scheduler: changes the scheduler of reactive streams, see Rocket.jl for more info, defaults to no scheduler\nreturnvars\n\nreturnvars accepts a tuple of symbols and specifies the latent variables of interests. For each symbol in the returnvars specification the rxinference function will prepare an observable stream (see Rocket.jl) of posterior updates. An agent may subscribe on the new posteriors events and perform some actions. For example:\n\nengine = rxinference(\n    ...,\n    returnvars = (:x, :τ),\n    autostart  = false\n)\n\nx_subscription = subscribe!(engine.posteriors[:x], (update) -> println(\"x variable has been updated: \", update))\nτ_subscription = subscribe!(engine.posteriors[:τ], (update) -> println(\"τ variable has been updated: \", update))\n\nRxInfer.start(engine)\n\n...\n\nunsubscribe!(x_subscription)\nunsubscribe!(τ_subscription)\n\nRxInfer.stop(engine)\n\nhistoryvars\n\nhistoryvars specifies the variables of interests and the amount of information to keep in history about the posterior updates. The specification is similar to the returnvars in the inference procedure. The historyvars requires keephistory to be greater than zero.\n\nhistoryvars accepts a NamedTuple or Dict or return var specification. There are two specifications:\n\nKeepLast: saves the last update for a variable, ignoring any intermediate results during iterations\nKeepEach: saves all updates for a variable for all iterations\n\nExample: \n\nresult = rxinference(\n    ...,\n    historyvars = (\n        x = KeepLast(),\n        τ = KeepEach()\n    ),\n    keephistory = 10\n)\n\nIt is also possible to set either historyvars = KeepLast() or historyvars = KeepEach() that acts as an alias and sets the given option for all random variables in the model.\n\nExample:\n\nresult = rxinference(\n    ...,\n    historyvars = KeepLast(),\n    keephistory = 10\n)\n\nkeep_history\n\nSpecifies the buffer size for the updates history both for the historyvars and the free_energy buffers.\n\niterations\n\nSpecifies the number of variational (or loopy belief propagation) iterations. By default set to nothing, which is equivalent of doing 1 iteration. \n\nfree_energy\n\nThis setting specifies whenever the inference function should create an observable of Bethe Free Energy (BFE) values. The BFE observable returns a new computed value for each VMP iteration. Note, however, that it may be not possible to compute BFE values for every model. If free_energy = true and keephistory > 0 the engine exposes extra fields to access the history of the Bethe free energy updates:\n\nengine.free_energy_history: Returns a free energy history averaged over the VMP iterations\nengine.free_energy_final_only_history: Returns a free energy history of values computed on last VMP iterations for every observation\nengine.free_energy_raw_history: Returns a raw free energy history\n\nAdditionally, the argument may accept a floating point type, instead of a Bool value. Using this option, e.g.Float64, improves performance of Bethe Free Energy computation, but restricts using automatic differentiation packages.\n\nfree_energy_diagnostics\n\nThis settings specifies either a single or a tuple of diagnostic checks for Bethe Free Energy values stream. By default checks for NaNs and Infs. See also BetheFreeEnergyCheckNaNs and BetheFreeEnergyCheckInfs. Pass nothing to disable any checks.\n\nevents\n\nThe engine from the rxinference function has its own lifecycle. The events can be listened by subscribing to the engine.events field. E.g.\n\nengine = rxinference(\n    ...,\n    autostart = false\n)\n\nsubscription = subscribe!(engine.events, (event) -> println(event))\n\nRxInfer.start(engine)\n\nBy default all events are disabled, in order to enable an event its identifier must be listed in the Val tuple of symbols passed to the events keyword arguments.\n\nengine = rxinference(\n    events = Val((:on_new_data, :before_history_save, :after_history_save))\n)\n\nThe list of all possible events and their event data is present below (see RxInferenceEvent for more information about the type of event data):\n\non_new_data:           args: (model::FactorGraphModel, data)\nbefore_iteration       args: (model::FactorGraphModel, iteration)\nbefore_auto_update     args: (model::FactorGraphModel, iteration, auto_updates)\nafter_auto_update      args: (model::FactorGraphModel, iteration, auto_updates)\nbefore_data_update     args: (model::FactorGraphModel, iteration, data)\nafter_data_update      args: (model::FactorGraphModel, iteration, data)\nafter_iteration        args: (model::FactorGraphModel, iteration)\nbefore_history_save    args: (model::FactorGraphModel, )\nafter_history_save     args: (model::FactorGraphModel, )\non_tick                args: (model::FactorGraphModel, )\non_error               args: (model::FactorGraphModel, err)\non_complete            args: (model::FactorGraphModel, )\ncallbacks\n\nThe rxinference function has its own lifecycle. The user is free to provide some (or none) of the callbacks to inject some extra logging or other procedures in the preparation of the inference engine. To inject extra procedures during the inference use the events. Here is the example of the callbacks\n\nresult = rxinference(\n    ...,\n    callbacks = (\n        after_model_creation = (model, returnval) -> println(\"The model has been created. Number of nodes: $(length(getnodes(model)))\"),\n    )\n)\n\nThe callbacks keyword argument accepts a named-tuple of 'name = callback' pairs.  The list of all possible callbacks and their input arguments is present below:\n\nbefore_model_creation:    args: ()\nafter_model_creation:     args: (model::FactorGraphModel, returnval)\nbefore_autostart:         args: (engine::RxInferenceEngine)\nafter_autostart:          args: (engine::RxInferenceEngine)\naddons\n\nThe addons field extends the default message computation rules with some extra information, e.g. computing log-scaling factors of messages or saving debug-information. Accepts a single addon or a tuple of addons. If set, replaces the corresponding setting in the options. Automatically changes the default value of the postprocess argument to NoopPostprocess.\n\npostprocess\n\nThe postprocess keyword argument controls whether the inference results must be modified in some way before exiting the inference function. By default, the inference function uses the DefaultPostprocess strategy, which by default removes the Marginal wrapper type from the results. Change this setting to NoopPostprocess if you would like to keep the Marginal wrapper type, which might be useful in the combination with the addons argument. If the addons argument has been used, automatically changes the default strategy value to NoopPostprocess.\n\nSee also inference\n\n\n\n\n\n","category":"function"},{"location":"manuals/inference/rxinference/#RxInfer.start","page":"Real-time dataset / reactive inference","title":"RxInfer.start","text":"start(engine::RxInferenceEngine)\n\nStarts the RxInferenceEngine by subscribing to the data source, instantiating free energy (if enabled) and starting the event loop. Use RxInfer.stop to stop the RxInferenceEngine. Note that it is not always possible to stop/restart the engine and this depends on the data source type.\n\nSee also: RxInfer.stop\n\n\n\n\n\n","category":"function"},{"location":"manuals/inference/rxinference/#RxInfer.stop","page":"Real-time dataset / reactive inference","title":"RxInfer.stop","text":"stop(engine::RxInferenceEngine)\n\nStops the RxInferenceEngine by unsubscribing to the data source, free energy (if enabled) and stopping the event loop. Use RxInfer.start to start the RxInferenceEngine again. Note that it is not always possible to stop/restart the engine and this depends on the data source type.\n\nSee also: RxInfer.start\n\n\n\n\n\n","category":"function"},{"location":"manuals/inference/rxinference/#RxInfer.@autoupdates","page":"Real-time dataset / reactive inference","title":"RxInfer.@autoupdates","text":"@autoupdates\n\nCreates the auto-updates specification for the rxinference function. In the online-streaming Bayesian inference procedure it is important to update your priors for the future  states based on the new updated posteriors. The @autoupdates structure simplify such a specification. It accepts a single block of code where each line defines how to update  the datavar's in the probabilistic model specification. \n\nEach line of code in the auto-update specification defines datavars, which need to be updated, on the left hand side of the equality expression and the update function on the right hand side of the expression. The update function operates on posterior marginals in the form of the q(symbol) expression.\n\nFor example:\n\n@autoupdates begin \n    x = f(q(z))\nend\n\nThis structure specifies to automatically update x = datavar(...) as soon as the inference engine computes new posterior over z variable. It then applies the f function to the new posterior and calls update!(x, ...) automatically. \n\nAs an example consider the following model and auto-update specification:\n\n@model function kalman_filter()\n    x_current_mean = datavar(Float64)\n    x_current_var  = datavar(Float64)\n\n    x_current ~ Normal(mean = x_current_mean, var = x_current_var)\n\n    x_next ~ Normal(mean = x_current, var = 1.0)\n\n    y = datavar(Float64)\n    y ~ Normal(mean = x_next, var = 1.0)\nend\n\nThis model has two datavars that represent our prior knowledge of the x_current state of the system. The x_next random variable represent the next state of the system that  is connected to the observed variable y. The auto-update specification could look like:\n\nautoupdates = @autoupdates begin\n    x_current_mean, x_current_var = mean_cov(q(x_next))\nend\n\nThis structure specifies to update our prior as soon as we have a new posterior q(x_next). It then applies the mean_cov function on the updated posteriors and updates  datavars x_current_mean and x_current_var automatically.\n\nSee also: rxinference\n\n\n\n\n\n","category":"macro"},{"location":"manuals/inference/rxinference/#RxInfer.RxInferenceEngine","page":"Real-time dataset / reactive inference","title":"RxInfer.RxInferenceEngine","text":"RxInferenceEngine\n\nThe return value of the rxinference function. \n\nPublic fields\n\nposteriors: Dict or NamedTuple of 'random variable' - 'posterior stream' pairs. See the returnvars argument for the rxinference.\nfree_energy: (optional) A stream of Bethe Free Energy values per VMP iteration. See the free_energy argument for the rxinference.\nhistory: (optional) Saves history of previous marginal updates. See the historyvars and keephistory arguments for the rxinference.\nfree_energy_history: (optional) Free energy history, average over variational iterations \nfree_energy_raw_history: (optional) Free energy history, returns returns computed values of all variational iterations for each data event (if available)\nfree_energy_final_only_history: (optional) Free energy history, returns computed values of final variational iteration for each data event (if available)\nevents: (optional) A stream of events send by the inference engine. See the events argument for the rxinference.\nmodel: FactorGraphModel object reference.\nreturnval: Return value from executed @model.\n\nUse the RxInfer.start(engine) function to subscribe on the data source and start the inference procedure. Use RxInfer.stop(engine) to unsubscribe from the data source and stop the inference procedure.  Note, that it is not always possible to start/stop the inference procedure.\n\nSee also: rxinference, RxInferenceEvent, RxInfer.start, RxInfer.stop\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/rxinference/#RxInfer.RxInferenceEvent","page":"Real-time dataset / reactive inference","title":"RxInfer.RxInferenceEvent","text":"RxInferenceEvent{T, D}\n\nThe RxInferenceEngine sends events in a form of the RxInferenceEvent structure. T represents the type of an event, D represents the type of a data associated with the event. The type of data depends on the type of an event, but usually represents a tuple, which can be unrolled automatically with the Julia's splitting syntax, e.g. model, iteration = event.  See the documentation of the rxinference function for possible event types and their associated data types.\n\nThe events system itself uses the Rocket.jl library API. For example, one may create a custom event listener in the following way:\n\nusing Rocket\n\nstruct MyEventListener <: Rocket.Actor{RxInferenceEvent}\n    # ... extra fields\nend\n\nfunction Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :after_iteration })\n    model, iteration = event\n    println(\"Iteration $(iteration) has been finished.\")\nend\n\nfunction Rocket.on_error!(listener::MyEventListener, err)\n    # ...\nend\n\nfunction Rocket.on_complete!(listener::MyEventListener)\n    # ...\nend\n\n\nand later on:\n\nengine = rxinference(events = Val((:after_iteration, )), ...)\n\nsubscription = subscribe!(engine.events, MyEventListener(...))\n\nSee also: rxinference, RxInferenceEngine\n\n\n\n\n\n","category":"type"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Infinite Data Stream/#examples-infinite-data-stream","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"section"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"This example shows the capabilities of RxInfer to perform Bayesian inference on real-time signals. As usual, first, we start with importing necessary packages:","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"using RxInfer, Plots, Random, StableRNGs","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"For demonstration purposes we will create a synthetic environment that has a hidden underlying signal, which we cannot observer directly. Instead, we will observe a noised realisation of this hidden signal:","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"mutable struct Environment\n    rng                   :: AbstractRNG\n    current_state         :: Float64\n    observation_precision :: Float64\n    history               :: Vector{Float64}\n    observations          :: Vector{Float64}\n    \n    Environment(current_state, observation_precision; seed = 123) = begin \n         return new(StableRNG(seed), current_state, observation_precision, [], [])\n    end\nend\n\nfunction getnext!(environment::Environment)\n    environment.current_state = environment.current_state + 1.0\n    nextstate  = 10sin(0.1 * environment.current_state)\n    observation = rand(NormalMeanPrecision(nextstate, environment.observation_precision))\n    push!(environment.history, nextstate)\n    push!(environment.observations, observation)\n    return observation\nend\n\nfunction gethistory(environment::Environment)\n    return environment.history\nend\n\nfunction getobservations(environment::Environment)\n    return environment.observations\nend","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"getobservations (generic function with 1 method)","category":"page"},{"location":"examples/Infinite Data Stream/#Model-specification","page":"Infinite Data Stream","title":"Model specification","text":"","category":"section"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"We assume that we don't know the shape of our signal in advance. So we try to fit a simple gaussian random walk with unknown observation noise:","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"@model function kalman_filter()\n    \n    # Prior for the previous state\n    x_prev_mean = datavar(Float64)\n    x_prev_var  = datavar(Float64)\n    \n    x_prev ~ Normal(mean = x_prev_mean, variance = x_prev_var)\n    \n    # Prior for the observation noise\n    τ_shape = datavar(Float64)\n    τ_rate  = datavar(Float64)\n    \n    τ ~ Gamma(shape = τ_shape, rate = τ_rate)\n    \n    # Random walk with fixed precision\n    x_current ~ Normal(mean = x_prev, precision = 1.0)\n    \n    # Noisy observation\n    y = datavar(Float64)\n    y ~ Normal(mean = x_current, precision = τ)\n    \nend\n\n# We assume the following factorisation between variables \n# in the variational distribution\n@constraints function filter_constraints()\n    q(x_prev, x_current, τ) = q(x_prev, x_current)q(τ)\nend","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"filter_constraints (generic function with 1 method)","category":"page"},{"location":"examples/Infinite Data Stream/#Prepare-environment","page":"Infinite Data Stream","title":"Prepare environment","text":"","category":"section"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"initial_state         = 0.0\nobservation_precision = 0.1","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"0.1","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"After we have created the environment we can observe how our signal behaves:","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"testenvironment = Environment(initial_state, observation_precision);\n\nanimation = @animate for i in 1:100\n    getnext!(testenvironment)\n    \n    history = gethistory(testenvironment)\n    observations = getobservations(testenvironment)\n    \n    p = plot(size = (1000, 300))\n    \n    p = plot!(p, 1:i, history[1:i], label = \"Hidden signal\")\n    p = scatter!(p, 1:i, observations[1:i], ms = 4, alpha = 0.7, label = \"Observation\")\nend\n\ngif(animation, \"./../assets/examples/infinite-data-stream.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"examples/Infinite Data Stream/#Filtering-on-static-dataset","page":"Infinite Data Stream","title":"Filtering on static dataset","text":"","category":"section"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"RxInfer is flexible and allows for running inference both on real-time and static datasets. In the next section we show how to perform the filtering procedure on a static dataset. We also will verify our inference procedure by checking on the Bethe Free Energy values:","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"n                  = 300\nstatic_environment = Environment(initial_state, observation_precision);\n\nfor i in 1:n\n    getnext!(static_environment)\nend\n\nstatic_history      = gethistory(static_environment)\nstatic_observations = getobservations(static_environment);\nstatic_datastream   = from(static_observations) |> map(NamedTuple{(:y,), Tuple{Float64}}, (d) -> (y = d, ));","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"function run_static(environment, datastream)\n    \n    # `@autoupdates` structure specifies how to update our priors based on new posteriors\n    # For example, every time we have updated a posterior over `x_current` we update our priors\n    # over `x_prev`\n    autoupdates = @autoupdates begin \n        x_prev_mean, x_prev_var = mean_var(q(x_current))\n        τ_shape = shape(q(τ))\n        τ_rate = rate(q(τ))\n    end\n    \n    engine = rxinference(\n        model         = kalman_filter(),\n        constraints   = filter_constraints(),\n        datastream    = datastream,\n        autoupdates   = autoupdates,\n        returnvars    = (:x_current, ),\n        keephistory   = 10_000,\n        historyvars   = (x_current = KeepLast(), τ = KeepLast()),\n        initmarginals = (x_current = NormalMeanVariance(0.0, 1e3), τ = GammaShapeRate(1.0, 1.0)),\n        iterations    = 10,\n        free_energy   = true,\n        autostart     = true,\n    )\n    \n    return engine\nend","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"run_static (generic function with 1 method)","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"result = run_static(static_environment, static_datastream);","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"static_inference = @animate for i in 1:n\n    estimated = result.history[:x_current]\n    p = plot(1:i, mean.(estimated[1:i]), ribbon = var.(estimated[1:n]), label = \"Estimation\")\n    p = plot!(static_history[1:i], label = \"Real states\")    \n    p = scatter!(static_observations[1:i], ms = 2, label = \"Observations\")\n    p = plot(p, size = (1000, 300), legend = :bottomright)\nend\n\ngif(static_inference, \"./../assets/examples/infinite-data-stream-inference.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"plot(result.free_energy_history, label = \"Bethe Free Energy (averaged)\")","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"examples/Infinite Data Stream/#Filtering-on-realtime-dataset","page":"Infinite Data Stream","title":"Filtering on realtime dataset","text":"","category":"section"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"Next lets create a \"real\" infinite stream. We use timer() observable from Rocket.jlto emulate real-world scenario. In our example we are going to generate a new data point every ~41ms (24 data points per second). For demonstration purposes we force stop after n data points, but there is no principled limitation to run inference indefinite:","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"function run_and_plot(environment, datastream)\n    \n    # `@autoupdates` structure specifies how to update our priors based on new posteriors\n    # For example, every time we have updated a posterior over `x_current` we update our priors\n    # over `x_prev`\n    autoupdates = @autoupdates begin \n        x_prev_mean, x_prev_var = mean_var(q(x_current))\n        τ_shape = shape(q(τ))\n        τ_rate = rate(q(τ))\n    end\n    \n    posteriors = []\n    \n    plotfn = (q_current) -> begin \n        IJulia.clear_output(true)\n        \n        push!(posteriors, q_current)\n\n        p = plot(mean.(posteriors), ribbon = var.(posteriors), label = \"Estimation\")\n        p = plot!(gethistory(environment), label = \"Real states\")    \n        p = scatter!(getobservations(environment), ms = 2, label = \"Observations\")\n        p = plot(p, size = (1000, 300), legend = :bottomright)\n\n        display(p)\n    end\n    \n    engine = rxinference(\n        model         = kalman_filter(),\n        constraints   = filter_constraints(),\n        datastream    = datastream,\n        autoupdates   = autoupdates,\n        returnvars    = (:x_current, ),\n        initmarginals = (x_current = NormalMeanVariance(0.0, 1e3), τ = GammaShapeRate(1.0, 1.0)),\n        iterations    = 10,\n        autostart     = false,\n    )\n    \n    qsubscription = subscribe!(engine.posteriors[:x_current], plotfn)\n    \n    RxInfer.start(engine)\n    \n    return engine\nend","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"run_and_plot (generic function with 1 method)","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"# This example runs in our documentation pipeline, which does not support \"real-time\" execution context\n# We skip this code if run not in Jupyter notebook (see below an example with gif)\nengine = nothing \nif isdefined(Main, :IJulia)\n    timegen      = 41 # 41 ms\n    environment  = Environment(initial_state, observation_precision);\n    observations = timer(timegen, timegen) |> map(Float64, (_) -> getnext!(environment)) |> take(n) # `take!` automatically stops after `n` observations\n    datastream   = observations |> map(NamedTuple{(:y,), Tuple{Float64}}, (d) -> (y = d, ));\n    engine = run_and_plot(environment, datastream)\nend;","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"The plot above is fully interactive and we can stop and unsubscribe from our datastream before it ends:","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"if !isnothing(engine) && isdefined(Main, :IJulia)\n    RxInfer.stop(engine)\n    IJulia.clear_output(true)\nend;","category":"page"},{"location":"examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"page"},{"location":"contributing/new-example/#contributing-new-example","page":"Adding a new example","title":"Contributing: new example","text":"","category":"section"},{"location":"contributing/new-example/","page":"Adding a new example","title":"Adding a new example","text":"We welcome all possible contributors. This page details the some of the guidelines that should be followed when adding a new example (in the examples/ folder) to this package.","category":"page"},{"location":"contributing/new-example/","page":"Adding a new example","title":"Adding a new example","text":"In order to add a new example simply create a new Jupyter notebook with your experiments in the examples/ folder. When creating a new example add a descriptive explanation of your experiments, model specification, inference constraints decisions and add appropriate results analysis. For other people it also would be useful if you write descriptive comments along your code.","category":"page"},{"location":"contributing/new-example/","page":"Adding a new example","title":"Adding a new example","text":"After that it is necessary to modify the examples/.meta.jl file. See the comments in this file for more information.","category":"page"},{"location":"contributing/new-example/","page":"Adding a new example","title":"Adding a new example","text":"Make sure that the very first cell of the notebook contains ONLY # <title> in it and has markdown type. This is important for link generation in the documentation\nPaths must be local and cannot be located in subfolders\nDescription is used to pre-generate an Examples page overview in the documentation\nUse hidden option to not include a certain example in the documentation (build will still run to ensure the example runs)\nName Overview is reserved, please do not use it\nUse $$\\begin{aligned} (note the same line, otherwise formulas will not render correctly in the documentation)                 <latex formulas here>                 \\end{aligned}$$ (on the same line (check other examples if you are not sure)\nNotebooks and plain Julia have different scoping rules for global variables, if it happens so that examples generation fails due to UndefVarError and scoping issues use let ... end blocks to enforce local scoping (see Gaussian Mixtures Multivariate.ipynb as an example)\nAll examples must use and activate local Project.toml in the second cell (see 1.), if you need some package add it to the (examples) project","category":"page"},{"location":"contributing/new-example/","page":"Adding a new example","title":"Adding a new example","text":"note: Note\nUse make examples to run all examples or make examples specific=MyNewCoolNotebook to run any notebook that includes MyNewCoolNotebook in its file name.","category":"page"},{"location":"library/model-specification/#lib-model-specification","page":"Model specification","title":"Model specification in RxInfer","text":"","category":"section"},{"location":"library/model-specification/","page":"Model specification","title":"Model specification","text":"RxInfer.@model\nRxInfer.ModelGenerator\nRxInfer.create_model\nRxInfer.ModelInferenceOptions","category":"page"},{"location":"library/model-specification/#RxInfer.@model","page":"Model specification","title":"RxInfer.@model","text":"@model function model_name(model_arguments...; model_keyword_arguments...)\n    # model description\nend\n\n@model macro generates a function that returns an equivalent graph-representation of the given probabilistic model description.\n\nSupported alias in the model specification\n\na || b: alias for OR(a, b) node (operator precedence between ||, &&, -> and ! is the same as in Julia).\na && b: alias for AND(a, b) node (operator precedence ||, &&, -> and ! is the same as in Julia).\na -> b: alias for IMPLY(a, b) node (operator precedence ||, &&, -> and ! is the same as in Julia).\n¬a and !a: alias for NOT(a) node (Unicode \\neg, operator precedence ||, &&, -> and ! is the same as in Julia).\na + b + c: alias for (a + b) + c\na * b * c: alias for (a * b) * c\nNormal(μ|m|mean = ..., σ²|τ⁻¹|v|var|variance = ...) alias for NormalMeanVariance(..., ...) node. Gaussian could be used instead Normal too.\nNormal(μ|m|mean = ..., τ|γ|σ⁻²|w|p|prec|precision = ...) alias for NormalMeanVariance(..., ...) node. Gaussian could be used instead Normal too.\nMvNormal(μ|m|mean = ..., Σ|V|Λ⁻¹|cov|covariance = ...) alias for MvNormalMeanCovariance(..., ...) node. MvGaussian could be used instead MvNormal too.\nMvNormal(μ|m|mean = ..., Λ|W|Σ⁻¹|prec|precision = ...) alias for MvNormalMeanPrecision(..., ...) node. MvGaussian could be used instead MvNormal too.\nMvNormal(μ|m|mean = ..., τ|γ|σ⁻²|scale_diag_prec|scale_diag_precision = ...) alias for MvNormalMeanScalePrecision(..., ...) node. MvGaussian could be used instead MvNormal too.\nGamma(α|a|shape = ..., θ|β⁻¹|scale = ...) alias for GammaShapeScale(..., ...) node.\nGamma(α|a|shape = ..., β|θ⁻¹|rate = ...) alias for GammaShapeRate(..., ...) node.\n\n\n\n\n\n","category":"macro"},{"location":"library/model-specification/#RxInfer.ModelGenerator","page":"Model specification","title":"RxInfer.ModelGenerator","text":"ModelGenerator\n\nModelGenerator is a special object that is used in the inference function to lazily create model later on given constraints, meta and options.\n\nSee also: inference\n\n\n\n\n\n","category":"type"},{"location":"library/model-specification/#RxInfer.create_model","page":"Model specification","title":"RxInfer.create_model","text":"create_model(::ModelGenerator, constraints = nothing, meta = nothing, options = nothing)\n\nCreates an instance of FactorGraphModel from the given model specification as well as optional constraints, meta and options.\n\nReturns a tuple of 2 values:\n\nan instance of FactorGraphModel\nreturn value from the @model macro function definition\n\n\n\n\n\n","category":"function"},{"location":"library/model-specification/#RxInfer.ModelInferenceOptions","page":"Model specification","title":"RxInfer.ModelInferenceOptions","text":"ModelInferenceOptions(; kwargs...)\n\nCreates model inference options object. The list of available options is present below.\n\nOptions\n\nlimit_stack_depth: limits the stack depth for computing messages, helps with StackOverflowError for some huge models, but reduces the performance of inference backend. Accepts integer as an argument that specifies the maximum number of recursive depth. Lower is better for stack overflow error, but worse for performance.\n\nAdvanced options\n\npipeline: changes the default pipeline for each factor node in the graph\nglobal_reactive_scheduler: changes the scheduler of reactive streams, see Rocket.jl for more info, defaults to no scheduler\n\nSee also: inference, rxinference\n\n\n\n\n\n","category":"type"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Linear Regression/#examples-bayesian-linear-regression","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"","category":"section"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"using RxInfer, Random, Plots, StableRNGs, BenchmarkTools, LinearAlgebra, StatsPlots","category":"page"},{"location":"examples/Linear Regression/#Univariate-regression-with-known-noise","page":"Bayesian Linear Regression","title":"Univariate regression with known noise","text":"","category":"section"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In this example, we are going to perform a simple linear regression problem, but in the Bayesian setting. We specify the model's likelihood as:","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"beginaligned\np(y_i) = mathcalN(y_i  a * x_i + b  10)\nendaligned","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"where x_i and y_i are observed values and a and b are random variables with the following priors:","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"beginaligned\n    p(a) = mathcalN(am_a v_a) \n    p(b) = mathcalN(bm_b v_b) \nendaligned","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function linear_regression(n)\n    a ~ NormalMeanVariance(0.0, 1.0)\n    b ~ NormalMeanVariance(0.0, 100.0)\n    \n    x = datavar(Float64, n)\n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ NormalMeanVariance(a * x[i] + b, 1.0)\n    end\nend","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In order to test our inference procedure we create a test dataset where observations are corrupted with noise. During the inference procedure we, however, do not know the exact magnitude of the noise.","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"reala = 0.5\nrealb = 25.0\nrealv = 1.0\n\nN = 250\n\nrng = StableRNG(1234)\n\nxorig = collect(1:N)\n\nxdata = xorig .+ randn(rng, N)\nydata = rand.(NormalMeanVariance.(realb .+ reala .* xorig, realv))\n\nscatter(xdata, ydata, title = \"Linear regression dataset\", legend=false)","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In order to run inference with the static dataset we use the inference function from RxInfer package.","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"results = inference(\n    model = linear_regression(length(xdata)), \n    data  = (y = ydata, x = xdata), \n    initmessages = (b = NormalMeanVariance(0.0, 100.0), ), \n    returnvars   = (a = KeepLast(), b = KeepLast()), \n    iterations = 20,\n);","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"After the inference has been completed it is interesting to compare prior distribution and posterior distribution against the real values:","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"pra = plot(range(-3, 3, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 1.0), x), title=\"Prior for a parameter\", fillalpha=0.3, fillrange = 0, label=\"Prior P(a)\", c=1,)\npra = vline!(pra, [ reala ], label=\"Real a\", c = 3)\npsa = plot(range(0.45, 0.55, length = 1000), (x) -> pdf(results.posteriors[:a], x), title=\"Posterior for a parameter\", fillalpha=0.3, fillrange = 0, label=\"Posterior P(a)\", c=2,)\npsa = vline!(psa, [ reala ], label=\"Real a\", c = 3)\n\nplot(pra, psa, size = (1000, 200))","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"prb = plot(range(-40, 40, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 100.0), x), title=\"Prior for b parameter\", fillalpha=0.3, fillrange = 0, label=\"Prior P(b)\", c=1, legend = :topleft)\nprb = vline!(prb, [ realb ], label=\"Real b\", c = 3)\npsb = plot(range(23, 28, length = 1000), (x) -> pdf(results.posteriors[:b], x), title=\"Posterior for b parameter\", fillalpha=0.3, fillrange = 0, label=\"Posterior P(b)\", c=2, legend = :topleft)\npsb = vline!(psb, [ realb ], label=\"Real b\", c = 3)\n\nplot(prb, psb, size = (1000, 200))","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"a = results.posteriors[:a]\nb = results.posteriors[:b]\n\nprintln(\"Real a: \", reala, \" | Estimated a: \", mean_var(a), \" | Error: \", abs(mean(a) - reala))\nprintln(\"Real b: \", realb, \" | Estimated b: \", mean_var(b), \" | Error: \", abs(mean(b) - realb))","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Real a: 0.5 | Estimated a: (0.5005968303126298, 1.9152402435219974e-7) | Er\nror: 0.0005968303126298036\nReal b: 25.0 | Estimated b: (24.93529510966175, 0.004015967531215887) | Err\nor: 0.06470489033824833","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"We can see that ReactiveMP.jl estimated real values of linear regression coefficients with high precision. ","category":"page"},{"location":"examples/Linear Regression/#Univariate-regression-with-unknown-noise","page":"Bayesian Linear Regression","title":"Univariate regression with unknown noise","text":"","category":"section"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"For this demo we are going to increase the amount of noise in the dataset, but also instead of using a fixed value for the noise in the model we are going to make it a random variable with its own prior:","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"beginaligned\np(s) = mathcalIG(salpha theta)\nendaligned","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function linear_regression_unknown_noise(n)\n    a ~ NormalMeanVariance(0.0, 1.0)\n    b ~ NormalMeanVariance(0.0, 100.0)\n    s ~ InverseGamma(1.0, 1.0)\n    \n    x = datavar(Float64, n)\n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ NormalMeanVariance(a * x[i] + b, s)\n    end\nend","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"reala_un = 0.5\nrealb_un = 25.0\nrealv_un = 200.0\n\nN_un = 250\n\nrng_un = StableRNG(1234)\n\nxorig_un = collect(1:N)\n\nxdata_un = xorig_un .+ randn(rng_un, N_un)\nydata_un = rand.(NormalMeanVariance.(realb_un .+ reala_un .* xorig_un, realv_un))\n\nscatter(xdata_un, ydata_un, title = \"Linear regression dataset with more noise\", legend=false)","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"To solve this problem analytically we need to set constraints = MeanField() as well as provide initial marginals with the initmarginals argument. We are also going to evaluate the convergency performance of the algorithm with the free_energy = true option:","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"results_unknown_noise = inference(\n    model = linear_regression_unknown_noise(length(xdata_un)), \n    data  = (y = ydata_un, x = xdata_un), \n    initmessages = (b = NormalMeanVariance(0.0, 100.0), ), \n    returnvars   = (a = KeepLast(), b = KeepLast(), s = KeepLast()), \n    iterations = 20,\n    constraints = MeanField(),\n    initmarginals = (s = vague(InverseGamma), ),\n    free_energy = true\n);","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"pra = plot(range(-3, 3, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 1.0), x), title=\"Prior for a parameter\", fillalpha=0.3, fillrange = 0, label=\"Prior P(a)\", c=1,)\npra = vline!(pra, [ reala_un ], label=\"Real a\", c = 3)\npsa = plot(range(0.45, 0.55, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:a], x), title=\"Posterior for a parameter\", fillalpha=0.3, fillrange = 0, label=\"Posterior P(a)\", c=2,)\npsa = vline!(psa, [ reala_un ], label=\"Real a\", c = 3)\n\nplot(pra, psa, size = (1000, 200))","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"prb = plot(range(-40, 40, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 100.0), x), title=\"Prior for b parameter\", fillalpha=0.3, fillrange = 0, label=\"Prior P(b)\", c=1, legend = :topleft)\nprb = vline!(prb, [ realb_un ], label=\"Real b\", c = 3)\npsb = plot(range(23, 28, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:b], x), title=\"Posterior for b parameter\", fillalpha=0.3, fillrange = 0, label=\"Posterior P(b)\", c=2, legend = :topleft)\npsb = vline!(psb, [ realb_un ], label=\"Real b\", c = 3)\n\nplot(prb, psb, size = (1000, 200))","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"prb = plot(range(0.001, 400, length = 1000), (x) -> pdf(InverseGamma(1.0, 1.0), x), title=\"Prior for s parameter\", fillalpha=0.3, fillrange = 0, label=\"Prior P(s)\", c=1, legend = :topleft)\nprb = vline!(prb, [ realv_un ], label=\"Real s\", c = 3)\npsb = plot(range(0.001, 400, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:s], x), title=\"Posterior for s parameter\", fillalpha=0.3, fillrange = 0, label=\"Posterior P(b)\", c=2, legend = :topleft)\npsb = vline!(psb, [ realv_un ], label=\"Real s\", c = 3)\n\nplot(prb, psb, size = (1000, 200))","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"We can see that in the presence of more noise the inference result is more uncertain about the actual values for a and b parameters.","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Lets sample a and b and plot many regression lines on the same plot:","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"as = rand(results_unknown_noise.posteriors[:a], 100)\nbs = rand(results_unknown_noise.posteriors[:b], 100)\n\np = scatter(xdata_un, ydata_un, title = \"Linear regression dataset with more noise\", legend=false)\n\nfor (a, b) in zip(as, bs)\n    global p = plot!(p, xdata_un, a .* xdata_un .+ b, alpha = 0.05, color = :red)\nend\n\nf = plot(results_unknown_noise.free_energy, title = \"Bethe Free Energy convergence\", label = nothing)\n\nplot(p, f, size = (1000, 400))","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"From this plot we can see that many lines do fit the data well and there is no definite \"best\" answer to the regression coefficients. Most of these lines, however, resemble a similar angle and shift. Bethe Free Energy plot on the right hand side indicates that the inference procedure converged normally.","category":"page"},{"location":"examples/Linear Regression/#Multivariate-linear-regression","page":"Bayesian Linear Regression","title":"Multivariate linear regression","text":"","category":"section"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"In this example we are essentially going to perform many linear regression tasks at once, using multiple x data vector and multiple y outputs with different noises. As in the previous example we assume noise to be unknown as well:","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"@model function linear_regression_multivariate(m, n)\n    a ~ MvNormalMeanCovariance(zeros(m), 100 * diageye(m))\n    b ~ MvNormalMeanCovariance(ones(m), 100 * diageye(m))\n    W ~ InverseWishart(m + 2, 100 * diageye(m))\n\n    # Here is a small trick to make the example work\n    # We treat the `x` vector as a Diagonal matrix such that we can multiply it with `a`\n    x = datavar(Diagonal{Float64, Vector{Float64}}, n)\n    y = datavar(Vector{Float64}, n)\n    z = randomvar(n)\n\n    z .~ x .* a .+ b\n    y .~ MvNormalMeanCovariance(z, W)\n\nend","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Lets have the following dataset, where multiple linear regression intersect with each other:","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"\nN_mv = 50\ns_mv = floor(N_mv / 10)\nd_mv = 6\n\nrng_mv = StableRNG(42)\n\na_mv = randn(rng_mv, d_mv)\nb_mv = 10 * randn(rng_mv, d_mv)\nv_mv = 100 * rand(rng_mv, d_mv)\n\nx_mv = []\ny_mv = []\n\np = plot(title = \"Multiple linear regressions\", legend = :topleft)\n\nplt = palette(:tab10)\n\nfor k in 1:d_mv\n    x_mv_k = collect((1 + s_mv * (k - 1)):(N_mv + s_mv * (k - 1))) .+ 10 * randn()\n    y_mv_k = rand.(NormalMeanVariance.(a_mv[k] .* x_mv_k .+ b_mv[k], v_mv[k]))\n\n    global p = scatter!(p, x_mv_k, y_mv_k, label = \"Dataset #$k\", ms = 2, color = plt[k])\n\n    push!(x_mv, x_mv_k)\n    push!(y_mv, y_mv_k)\nend\n\np","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"xdata_mv = map(i -> Diagonal(getindex.(x_mv, i)), 1:N_mv)\nydata_mv = map(i -> getindex.(y_mv, i), 1:N_mv);","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"results_mv = inference(\n    model = linear_regression_multivariate(d_mv, N_mv),\n    data  = (y = ydata_mv, x = xdata_mv),\n    initmarginals = (W = InverseWishart(d_mv + 2, 10 * diageye(d_mv)), ),\n    initmessages = (b = MvNormalMeanCovariance(ones(d_mv), 10 * diageye(d_mv)), ),\n    returnvars   = (a = KeepLast(), b = KeepLast(), W = KeepLast()),\n    free_energy = true,\n    iterations   = 50,\n    constraints = MeanField()\n)","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"Inference results:\n  Posteriors       | available for (a, b, W)\n  Free Energy:     | Real[926.06, 1055.2, 1046.86, 1039.93, 1034.91, 1030.4\n1, 1026.31, 1022.65, 1019.41, 1016.6  …  1003.37, 1003.37, 1003.37, 1003.37\n, 1003.37, 1003.37, 1003.36, 1003.36, 1003.36, 1003.36]","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"p = plot(title = \"Multivariate linear regression inference results\", legend = :topleft)\n\n# how many lines to plot\nr = 50\n\ni_a = collect.(eachcol(rand(results_mv.posteriors[:a], r)))\ni_b = collect.(eachcol(rand(results_mv.posteriors[:b], r)))\n\nplt = palette(:tab10)\n\nfor k in 1:d_mv\n    x_mv_k = x_mv[k]\n    y_mv_k = y_mv[k]\n\n    for i in 1:r\n        global p = plot!(p, x_mv_k, x_mv_k .* i_a[i][k] .+ i_b[i][k], label = nothing, alpha = 0.05, color = plt[k])\n    end\n\n    global p = scatter!(p, x_mv_k, y_mv_k, label = \"Dataset #$k\", ms = 2, color = plt[k])\nend\n\n# truncate the init step\nf = plot(results_mv.free_energy[2:end], title =\"Bethe Free Energy convergence\", label = nothing) \n\nplot(p, f, size = (1000, 400))","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"We needed more iterations to converge, but that is expected since the problem became multivariate and, hence, more difficult.","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"i_a_mv = results_mv.posteriors[:a]\n\nps_a = []\n\nfor k in 1:d_mv\n    \n    local _p = plot(title = \"Estimated a[$k]\")\n    local m_a_mv_k = mean(i_a_mv)[k]\n    local v_a_mv_k = std(i_a_mv)[k, k]\n    \n    _p = plot!(_p, Normal(m_a_mv_k, v_a_mv_k), fillalpha=0.3, fillrange = 0, label=\"Posterior P(a[$k])\", c=2,)\n    _p = vline!(_p, [ a_mv[k] ], label=\"Real a[$k]\", c = 3)\n           \n    push!(ps_a, _p)\nend\n\nplot(ps_a...)","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"i_b_mv = results_mv.posteriors[:b]\n\nps_b = []\n\nfor k in 1:d_mv\n    \n    local _p = plot(title = \"Estimated b[$k]\")\n    local m_b_mv_k = mean(i_b_mv)[k]\n    local v_b_mv_k = std(i_b_mv)[k, k]\n\n    _p = plot!(_p, Normal(m_b_mv_k, v_b_mv_k), fillalpha=0.3, fillrange = 0, label=\"Posterior P(b[$k])\", c=2,)\n    _p = vline!(_p, [ b_mv[k] ], label=\"Real b[$k]\", c = 3)\n           \n    push!(ps_b, _p)\nend\n\nplot(ps_b...)","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"(Image: )","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"As we can also check the noise estimation procedure found the noise components with high precision:","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"v_mv","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"6-element Vector{Float64}:\n 63.75837562301017\n 11.035887151354174\n 59.851445480565424\n 32.21105133173212\n 16.228513601930295\n 64.33685320974074","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"diag(mean(results_mv.posteriors[:W]))","category":"page"},{"location":"examples/Linear Regression/","page":"Bayesian Linear Regression","title":"Bayesian Linear Regression","text":"6-element Vector{Float64}:\n 63.85406996071467\n 10.452781436982868\n 66.92960007521636\n 33.41385641075882\n 10.041837021782927\n 77.78658604599262","category":"page"},{"location":"manuals/getting-started/#user-guide-getting-started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"RxInfer.jl is a Julia package for Bayesian Inference on Factor Graphs by Message Passing. It supports both exact and variational inference algorithms.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"RxInfer.jl package forms an ecosystem around three main packages: ReactiveMP.jl exports a reactive message passing based Bayesian inference engine, Rocket.jl is the core library that enables reactivity and GraphPPL.jl library simplifies model and constraints specification. ReactiveMP.jl engine is a successor of the ForneyLab package. It follows the same ideas and concepts for message-passing based inference, but uses new reactive and efficient message passing implementation under the hood. The API between two packages is different due to a better flexibility, performance and new reactive approach for solving inference problems.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"This page provides the necessary information you need to get started with Rxinfer. We will show the general approach to solving inference problems with RxInfer by means of a running example: inferring the bias of a coin.","category":"page"},{"location":"manuals/getting-started/#Installation","page":"Getting started","title":"Installation","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Install RxInfer through the Julia package manager:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"] add RxInfer","category":"page"},{"location":"manuals/getting-started/#Importing-RxInfer","page":"Getting started","title":"Importing RxInfer","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"To add RxInfer package (and all associated packages) into a running Julia session simply run:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"using RxInfer","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Read more about about using in the Using methods from RxInfer section of the documentation.","category":"page"},{"location":"manuals/getting-started/#Example:-Inferring-the-bias-of-a-coin","page":"Getting started","title":"Example: Inferring the bias of a coin","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"The RxInfer approach to solving inference problems consists of three phases:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Model specification: RxInfer uses GraphPPL package for model specification part. It offers a domain-specific language to specify your probabilistic model.\nInference specification: RxInfer inference API uses ReactiveMP inference engine under the hood and has been designed to be as flexible as possible. It is compatible both with asynchronous infinite data streams and with static datasets. For most of the use cases it consists of the same simple building blocks. In this example we will show one of the many possible ways to infer your quantities of interest.\nInference execution: Given model specification and inference procedure it is pretty straightforward to use reactive API from Rocket to pass data to the inference backend and to run actual inference.","category":"page"},{"location":"manuals/getting-started/#Coin-flip-simulation","page":"Getting started","title":"Coin flip simulation","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Let's start by creating some dataset. One approach could be flipping a coin N times and recording each outcome. For simplicity in this example we will use static pre-generated dataset. Each sample can be thought of as the outcome of single flip which is either heads or tails (1 or 0). We will assume that our virtual coin is biased, and lands heads up on 75% of the trials (on average).","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"First let's setup our environment by importing all needed packages:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"using RxInfer, Distributions, Random","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Next, let's define our dataset:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"rng = MersenneTwister(42)\nn = 10\np = 0.75\ndistribution = Bernoulli(p)\n\ndataset = float.(rand(rng, Bernoulli(p), n))","category":"page"},{"location":"manuals/getting-started/#getting-started-model-specification","page":"Getting started","title":"Model specification","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"In a Bayesian setting, the next step is to specify our probabilistic model. This amounts to specifying the joint probability of the random variables of the system.","category":"page"},{"location":"manuals/getting-started/#Likelihood","page":"Getting started","title":"Likelihood","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"We will assume that the outcome of each coin flip is governed by the Bernoulli distribution, i.e.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"y_i sim mathrmBernoulli(theta)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"where y_i = 1 represents \"heads\", y_i = 0 represents \"tails\". The underlying probability of the coin landing heads up for a single coin flip is theta in 01.","category":"page"},{"location":"manuals/getting-started/#Prior","page":"Getting started","title":"Prior","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"We will choose the conjugate prior of the Bernoulli likelihood function defined above, namely the beta distribution, i.e.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"theta sim Beta(a b)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"where a and b are the hyperparameters that encode our prior beliefs about the possible values of theta. We will assign values to the hyperparameters in a later step.   ","category":"page"},{"location":"manuals/getting-started/#Joint-probability","page":"Getting started","title":"Joint probability","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"The joint probability is given by the multiplication of the likelihood and the prior, i.e.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"P(y_1N θ) = P(θ) prod_i=1^N P(y_i  θ)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Now let's see how to specify this model using GraphPPL's package syntax.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"\n# GraphPPL.jl export `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function coin_model(n)\n\n    # `datavar` creates data 'inputs' in our model\n    # We will pass data later on to these inputs\n    # In this example we create a sequence of inputs that accepts Float64\n    y = datavar(Float64, n)\n    \n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(2.0, 7.0)\n    # or, in this particular case, the `Uniform(0.0, 1.0)` prior also works:\n    # θ ~ Uniform(0.0, 1.0)\n    \n    # We assume that outcome of each coin flip is governed by the Bernoulli distribution\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)\n    end\n    \n    # We return references to our data inputs and θ parameter\n    # We will use these references later on during inference step\n    return y, θ\nend\n","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"As you can see, RxInfer offers a model specification syntax that resembles closely to the mathematical equations defined above. We use datavar function to create \"clamped\" variables that take specific values at a later date. θ ~ Beta(2.0, 7.0) expression creates random variable θ and assigns it as an output of Beta node in the corresponding FFG. ","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"note: Note\nTo quickly check the list of all available factor nodes that can be used in the model specification language call ?make_node or Base.doc(make_node).","category":"page"},{"location":"manuals/getting-started/#getting-started-inference-specification","page":"Getting started","title":"Inference specification","text":"","category":"section"},{"location":"manuals/getting-started/#Automatic-inference-specification","page":"Getting started","title":"Automatic inference specification","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Once we have defined our model, the next step is to use RxInfer API to infer quantities of interests. To do this we can use a generic inference function that supports static datasets.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"result = inference(\n    model = coin_model(length(dataset)),\n    data  = (y = dataset, )\n)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"θestimated = result.posteriors[:θ]","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"println(\"mean: \", mean(θestimated))\nprintln(\"std:  \", std(θestimated))\nnothing #hide","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Read more information about the inference function in the Static Inference documentation section.","category":"page"},{"location":"manuals/getting-started/#Manual-inference-specification","page":"Getting started","title":"Manual inference specification","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"There is a way to manually specify an inference procedure for advanced use-cases. RxInfer API is flexible in terms of inference specification and is compatible both with real-time inference processing and with static datasets. In most of the cases for static datasets, as in our example, it consists of same basic building blocks:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Return variables of interests from model specification\nSubscribe on variables of interests posterior marginal updates\nPass data to the model\nUnsubscribe ","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Here is an example of inference procedure:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"function custom_inference(data)\n    n = length(data)\n\n    # `coin_model` function from `@model` macro returns a reference to the model generator object\n    # we need to use the `create_model` function to get actual model object\n    model, (y, θ) = create_model(coin_model(n))\n    \n    # Reference for future posterior marginal \n    mθ = nothing\n\n    # `getmarginal` function returns an observable of future posterior marginal updates\n    # We use `Rocket.jl` API to subscribe on this observable\n    # As soon as posterior marginal update is available we just save it in `mθ`\n    subscription = subscribe!(getmarginal(θ), (m) -> mθ = m)\n    \n    # `update!` function passes data to our data inputs\n    update!(y, data)\n    \n    # It is always a good practice to unsubscribe and to \n    # free computer resources held by the subscription\n    unsubscribe!(subscription)\n    \n    # Here we return our resulting posterior marginal\n    return mθ\nend","category":"page"},{"location":"manuals/getting-started/#getting-started-inference-execution","page":"Getting started","title":"Inference execution","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Here after everything is ready we just call our inference function to get a posterior marginal distribution over θ parameter in the model.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"θestimated = custom_inference(dataset)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"println(\"mean: \", mean(θestimated))\nprintln(\"std:  \", std(θestimated))\nnothing #hide","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"using Plots\n\nrθ = range(0, 1, length = 1000)\n\np1 = plot(rθ, (x) -> pdf(Beta(2.0, 7.0), x), title=\"Prior\", fillalpha=0.3, fillrange = 0, label=\"P(θ)\", c=1,)\np2 = plot(rθ, (x) -> pdf(θestimated, x), title=\"Posterior\", fillalpha=0.3, fillrange = 0, label=\"P(θ|y)\", c=3)\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"In our dataset we used 10 coin flips to estimate the bias of a coin. It resulted in a vague posterior distribution, however RxInfer scales very well for large models and factor graphs. We may use more coin flips in our dataset for better posterior distribution estimates:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"dataset_100   = float.(rand(rng, Bernoulli(p), 100))\ndataset_1000  = float.(rand(rng, Bernoulli(p), 1000))\ndataset_10000 = float.(rand(rng, Bernoulli(p), 10000))\nnothing # hide","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"θestimated_100   = custom_inference(dataset_100)\nθestimated_1000  = custom_inference(dataset_1000)\nθestimated_10000 = custom_inference(dataset_10000)\nnothing #hide","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"p3 = plot(title = \"Posterior\", legend = :topleft)\n\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_100, x), fillalpha = 0.3, fillrange = 0, label = \"P(θ|y_100)\", c = 4)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_1000, x), fillalpha = 0.3, fillrange = 0, label = \"P(θ|y_1000)\", c = 5)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_10000, x), fillalpha = 0.3, fillrange = 0, label = \"P(θ|y_10000)\", c = 6)\n\nplot(p1, p3, layout = @layout([ a; b ]))","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"With larger dataset our posterior marginal estimate becomes more and more accurate and represents real value of the bias of a coin.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"println(\"mean: \", mean(θestimated_10000))\nprintln(\"std:  \", std(θestimated_10000))\nnothing #hide","category":"page"},{"location":"manuals/getting-started/#Where-to-go-next?","page":"Getting started","title":"Where to go next?","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"There are a set of examples available in RxInfer repository that demonstrate the more advanced features of the package and also Examples section in the documentation. Alternatively, you can head to the Model specification which provides more detailed information of how to use RxInfer to specify probabilistic models. Inference execution section provides a documentation about RxInfer API for running reactive Bayesian inference.","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/#examples-hierarchical-gaussian-filter","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"","category":"section"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this demo the goal is to perform approximate variational Bayesian Inference for Univariate Hierarchical Gaussian Filter (HGF).","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Simple HGF model can be defined as:","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"beginaligned\n  x^(j)_k  sim  mathcalN(x^(j)_k - 1 f_k(x^(j - 1)_k)) \n  y_k  sim  mathcalN(x^(j)_k tau_k)\nendaligned","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"where j is an index of layer in hierarchy, k is a time step and f_k is a variance activation function. RxInfer.jl export Gaussian Controlled Variance (GCV) node with f_k = exp(kappa x + omega) variance activation function. By default the node uses Gauss-Hermite cubature with a prespecified number of approximation points in the cubature. In this demo we also show how we can change the hyperparameters in different approximation methods (iin this case Gauss-Hermite cubature) with the help of metadata structures. Here how our model will look like with the GCV node:","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"beginaligned\n  z_k  sim  mathcalN(z_k - 1 mathcaltau_z) \n  x_k  sim  mathcalN(x_k - 1 exp(kappa z_k + omega)) \n  y_k  sim  mathcalN(x_k mathcaltau_y)\nendaligned","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this experiment we will create a single time step of the graph and perform variational message passing filtering alrogithm to estimate hidden states of the system. For a more rigorous introduction to Hierarchical Gaussian Filter we refer to Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter paper.","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"For simplicity we will consider tau_z, tau_y, kappa and omega known and fixed, but there are no principled limitations to make them random variables too.","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To model this process in RxInfer, first, we start with importing all needed packages:","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"using RxInfer, BenchmarkTools, Random, Plots","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function generate_data(rng, k, w, zv, yv)\n    z_prev = 0.0\n    x_prev = 0.0\n\n    z = Vector{Float64}(undef, n)\n    v = Vector{Float64}(undef, n)\n    x = Vector{Float64}(undef, n)\n    y = Vector{Float64}(undef, n)\n\n    for i in 1:n\n        z[i] = rand(rng, Normal(z_prev, sqrt(zv)))\n        v[i] = exp(k * z[i] + w)\n        x[i] = rand(rng, Normal(x_prev, sqrt(v[i])))\n        y[i] = rand(rng, Normal(x[i], sqrt(yv)))\n\n        z_prev = z[i]\n        x_prev = x[i]\n    end \n    \n    return z, x, y\nend","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# Seed for reproducibility\nseed = 42\n\nrng = MersenneTwister(seed)\n\n# Parameters of HGF process\nreal_k = 1.0\nreal_w = 0.0\nz_variance = abs2(0.2)\ny_variance = abs2(0.1)\n\n# Number of observations\nn = 300\n\nz, x, y = generate_data(rng, real_k, real_w, z_variance, y_variance);","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Let's plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations.","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    scatter!(px, 1:n, y, label = \"y_i\", color = :red, ms = 2, alpha = 0.2)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To create a model we use the @model macro:","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# We create a single-time step of corresponding state-space process to\n# perform online learning (filtering)\n@model function hgf(real_k, real_w, z_variance, y_variance)\n    \n    # Priors from previous time step for `z`\n    zt_min_mean = datavar(Float64)\n    zt_min_var  = datavar(Float64)\n    \n    # Priors from previous time step for `x`\n    xt_min_mean = datavar(Float64)\n    xt_min_var  = datavar(Float64)\n\n    zt_min ~ NormalMeanVariance(zt_min_mean, zt_min_var)\n    xt_min ~ NormalMeanVariance(xt_min_mean, xt_min_var)\n\n    # Higher layer is modelled as a random walk \n    zt ~ NormalMeanVariance(zt_min, z_variance)\n    \n    # Lower layer is modelled with `GCV` node\n    gcvnode, xt ~ GCV(xt_min, zt, real_k, real_w)\n    \n    # Noisy observations \n    y = datavar(Float64)\n    y ~ NormalMeanVariance(xt, y_variance)\n    \n    return gcvnode\nend\n\n@constraints function hgfconstraints() \n    q(xt, zt, xt_min) = q(xt, xt_min)q(zt)\nend\n\n@meta function hgfmeta()\n    # Lets use 31 approximation points in the Gauss Hermite cubature approximation method\n    GCV(xt_min, xt, zt) -> GCVMetadata(GaussHermiteCubature(31)) \nend","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"hgfmeta (generic function with 1 method)","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function run_inference(data, real_k, real_w, z_variance, y_variance)\n\n    autoupdates   = @autoupdates begin\n        zt_min_mean, zt_min_var = mean_var(q(zt))\n        xt_min_mean, xt_min_var = mean_var(q(xt))\n    end\n\n    return rxinference(\n        model         = hgf(real_k, real_w, z_variance, y_variance),\n        constraints   = hgfconstraints(),\n        meta          = hgfmeta(),\n        data          = (y = data, ),\n        autoupdates   = autoupdates,\n        keephistory   = length(data),\n        historyvars    = (\n            xt = KeepLast(),\n            zt = KeepLast()\n        ),\n        initmarginals = (\n            zt = NormalMeanVariance(0.0, 5.0),\n            xt = NormalMeanVariance(0.0, 5.0),\n        ), \n        iterations    = 5,\n        free_energy   = true,\n        autostart     = true,\n        callbacks     = (\n            after_model_creation = (model, returnval) -> begin \n                gcvnode = returnval\n                setmarginal!(gcvnode, :y_x, MvNormalMeanCovariance([ 0.0, 0.0 ], [ 5.0, 5.0 ]))\n            end,\n        )\n    )\nend","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"run_inference (generic function with 1 method)","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"result = run_inference(y, real_k, real_w, z_variance, y_variance);\n\nmz = result.history[:zt];\nmx = result.history[:xt];","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(pz, 1:n, mean.(mz), ribbon = std.(mz), label = \"estimated z_i\", color = :teal)\n    \n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    plot!(px, 1:n, mean.(mx), ribbon = std.(mx), label = \"estimated x_i\", color = :violet)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the values for Bethe Free Energy functional:","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"plot(result.free_energy_history, label = \"Bethe Free Energy\")","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see BetheFreeEnergy converges nicely to a stable point.","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"At final, lets check the overall performance of our resulting Variational Message Passing algorithm:","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"@benchmark run_inference($y, $real_k, $real_w, $z_variance, $y_variance)","category":"page"},{"location":"examples/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"BenchmarkTools.Trial: 36 samples with 1 evaluation.\n Range (min … max):  128.434 ms … 176.775 ms  ┊ GC (min … max): 0.00% … 21.\n62%\n Time  (median):     137.451 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   142.348 ms ±  15.344 ms  ┊ GC (mean ± σ):  5.05% ±  8.\n65%\n\n  ▁▁▁▄█      ▁▄  ▁      ▁                                        \n  █████▆▆▁▁▆▆██▆▆█▆▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▆▁▆▆▆▁▆▁▁▆ ▁\n  128 ms           Histogram: frequency by time          177 ms <\n\n Memory estimate: 27.08 MiB, allocs estimate: 595804.","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/#examples-nonlinear-smoothing:-noisy-pendulum","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"","category":"section"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\".\"); Pkg.instantiate();","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"In this demo, we will look at a realistic dynamical system with nonlinear state transitions: tracking a noisy single pendulum. We translate a differential equation in state-space model form to a probabilistic model.","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"We are tracking a pendulum with noisy observations. You can find this example in Bayesian Filtering & Smoothing by Simon Särkkä (Ex 3.7). Its state transitions are sinusoidal in nature and it hence, qualifies as a nonlinear dynamical system.","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"We can describe the system with the following differential equation:","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"beginalign*\nfracddt beginbmatrix x_1  x_2 endbmatrix = beginbmatrix x_2  -g sin(x_1) endbmatrix\nendalign*","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"where x_1 represents the angle alpha of the pendulum, x_2 represents the change in angle d alpha dt and g is gravitational acceleration.","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"We discretise the equation using a forward finite difference: dxdt = (x_t+1 - x_t)Delta t. This produces the following discrete state transition:","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"beginalign*\nbeginbmatrix x_1t+1  x_2t+1 endbmatrix = beginbmatrix x_1t + x_2tDelta t  x_2t - g sin(x_1t)Delta t endbmatrix\nendalign*","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"We cannot observe the change in angle directly, only the angle itself (i.e. x_1). We can select the first element of the state vector by taking the inner product between the vector 1  0 and the state vector x_t.","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"The observation is corrupted by white noise v_t:","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"y_t = x_1t + v_t","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"where the white noise is sampled from a zero-mean Gaussian with precision tau^-1:","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"v_t sim mathcalN(0 tau^-1)","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"# Import libraries to julia workspace\nusing RxInfer, Plots, Distributions","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/#Generate-data","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Generate data","text":"","category":"section"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"# Dimension: time\nT = 500 # interaction: total time step count == time-series: length\nΔt = 0.01 # time step: duration\nrT = (1:T).*Δt\n\n# Gravitational acceleration (SI: LT⁻²)\nG = 9.81\n\n# Measurement noise precision\nnoise_precision = 10.\n\n# Initial states\nx0 = [1.0, 0.0]\n\n# Initialize data array\nstates = zeros(2,T)\nobservations = zeros(T,)\n\n# Initialize previous state variable\nprev_state = x0\n\nfor t = 1:T\n    # State transition\n    global states[1,t] = prev_state[1] + prev_state[2]*Δt\n    global states[2,t] = prev_state[2] - G*sin(prev_state[1])*Δt\n    \n    # Observation likelihood\n    global observations[t] = states[1,t] + sqrt(inv(noise_precision))*randn(1)[1]\n    \n    # Update \"previous state\"\n    global prev_state = states[:,t]\nend","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"Let's plot our pre-generated data to get some insights about its structure","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"# Inspect data\nplot(rT, states[1,:], xlabel=\"time (Δt = \"*string(Δt)*\")\", ylabel=\"position pendulum\", label=\"true\", legend=:bottomright)\nscatter!(rT, observations, ms = 2,  label=\"observed\")","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"(Image: )","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"Here g defines the nonlinear state-transition function:","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"g(x) = [x[1] + x[2]*Δt, x[2] - G*sin(x[1])*Δt]","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"g (generic function with 1 method)","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"We use the @model macro to define the probabilistic model:","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"# model of the complete system dynamics\n@model function noisy_pendulum(n)\n    y = datavar(Float64, n)\n    x = randomvar(n)\n    \n    τ ~ Gamma(α = 1.0, β = 1.0)\n    x_0 ~ MvNormal(μ = zeros(2), Σ = diageye(2))\n    \n    x_prev = x_0\n    for i in 1:n\n        \n        x[i] ~ g(x_prev) \n        y[i] ~ Normal(μ = dot([1.0, 0.0], x[i]), γ=τ)\n        x_prev = x[i]\n    end\n    \nend","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"# assumed factorization of the recognition model\nconstraints = @constraints begin \n    q(x_0, x, τ) = q(x_0, x)q(τ)\nend","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"Constraints:\n  marginals form:\n  messages form:\n  factorisation:\n    q(x_0, x, τ) = q(x_0, x)q(τ)\nOptions:\n  warn = true","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"meta = @meta begin \n    # `meta` defines the approximation method, which must be used for the nonlinear transformation\n    g() -> Unscented()\nend","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"Meta specification:\n  g() -> Unscented{Float64, Float64, Float64, Nothing}(0.001, 2.0, 0.0, not\nhing)\nOptions:\n  warn = true","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"result = inference(\n    model = noisy_pendulum(T), \n    data = (y = observations,),\n    iterations = 5, \n    options = (limit_stack_depth = 100, ), \n    constraints = constraints, \n    meta = meta,\n    initmarginals = (τ = Gamma(1.0, 1.0), ),\n    free_energy = true, \n)","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"Inference results:\n  Posteriors       | available for (x_0, τ, x)\n  Free Energy:     | Real[140.834, 134.093, 134.092, 134.092, 134.092]","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"Lets plot the results of the inference procedure:","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"m_x_t = mean.(result.posteriors[:x][end]) \nv_x_t = cov.(result.posteriors[:x][end]);","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"# Plot true states and overlay estimates\nplot(rT, states[1,:], xlabel=\"time (Δt = \"*string(Δt)*\")\", ylabel=\"angle (α)\", label=\"true\", legend=:bottomright)\nplot!(rT, first.(m_x_t), ribbon=sqrt.(first.(v_x_t)), alpha=0.6, label=\"estimated\")\ntitle!(\"Angle of the pendulum, over time\")","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"(Image: )","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"The estimated values are very close to the true values of the hidden states, but variance is not zero.","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"# Plot true states and overlay estimates\nplot(rT, states[2,:], xlabel=\"time (Δt = \"*string(Δt)*\")\", ylabel=\"change in angle (dα/dt)\", label=\"true\", legend=:topleft)\nplot!(rT, last.(m_x_t), ribbon=sqrt.(last.(v_x_t)), alpha=0.6, label=\"estimated\")\ntitle!(\"Change in angle of the pendulum, over time\")","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"(Image: )","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"rTp = range(0.0, 100.0, length = 1000)\n\nplot(rTp, (x) -> pdf(Gamma(1.0, 1.0), x), fillalpha=0.3, fillrange = 0, label=\"P(τ)\")\n\nlet τ̂ = last(result.posteriors[:τ])\n    plot!(rTp, (x) -> pdf(Gamma(shape(τ̂),scale(τ̂)), x), fillalpha=0.3, fillrange = 0, label=\"P(τ|y)\")\nend\n\nvline!([ noise_precision ], label = \"real noise precision\")\n\nplot!(xlim = (0, 20), title = \"Inference results\")","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"(Image: )","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"It is important to look at the evolution of free energy. Remember that free energy is a measure of uncertainty-weighted prediction error. ","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"# Plot free energy objective\nplot(result.free_energy, xlabel=\"# iterations\", label=\"Bethe Free Energy\")\ntitle!(\"Free energy by iterations, averaged over time\")","category":"page"},{"location":"examples/Nonlinear Noisy Pendulum/","page":"Nonlinear Smoothing: Noisy Pendulum","title":"Nonlinear Smoothing: Noisy Pendulum","text":"(Image: )","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-free-energy","page":"Bethe Free Energy","title":"Bethe Free Energy implementation in RxInfer","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"RxInfer.AbstractScoreObjective\nRxInfer.BetheFreeEnergy\nRxInfer.BetheFreeEnergyCheckNaNs\nRxInfer.BetheFreeEnergyCheckInfs","category":"page"},{"location":"library/bethe-free-energy/#RxInfer.AbstractScoreObjective","page":"Bethe Free Energy","title":"RxInfer.AbstractScoreObjective","text":"AbstractScoreObjective\n\nAbstract type for functional objectives that can be used in the score function.\n\nSee also: score\n\n\n\n\n\n","category":"type"},{"location":"library/bethe-free-energy/#RxInfer.BetheFreeEnergy","page":"Bethe Free Energy","title":"RxInfer.BetheFreeEnergy","text":"BetheFreeEnergy(marginal_skip_strategy, scheduler, diagnostic_checks)\n\nCreates Bethe Free Energy values stream when passed to the score function. \n\n\n\n\n\n","category":"type"},{"location":"library/bethe-free-energy/#RxInfer.BetheFreeEnergyCheckNaNs","page":"Bethe Free Energy","title":"RxInfer.BetheFreeEnergyCheckNaNs","text":"BetheFreeEnergyCheckNaNs\n\nIf enabled checks that both variable and factor bound score functions in Bethe Free Energy computation do not return NaNs.  Throws an error if finds NaN. \n\nSee also: BetheFreeEnergyCheckInfs\n\n\n\n\n\n","category":"type"},{"location":"library/bethe-free-energy/#RxInfer.BetheFreeEnergyCheckInfs","page":"Bethe Free Energy","title":"RxInfer.BetheFreeEnergyCheckInfs","text":"BetheFreeEnergyCheckInfs\n\nIf enabled checks that both variable and factor bound score functions in Bethe Free Energy computation do not return Infs.  Throws an error if finds Inf. \n\nSee also: BetheFreeEnergyCheckNaNs\n\n\n\n\n\n","category":"type"},{"location":"manuals/background/#intro-background-variational-inference","page":"Background: variational inference","title":"Background: variational inference","text":"","category":"section"}]
}
