<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Bayesian Linear Regression · RxInfer.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://biaslab.github.io/RxInfer.jl/examples/Linear Regression/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/theme.css" rel="stylesheet" type="text/css"/><link href="../../assets/header.css" rel="stylesheet" type="text/css"/><script src="../../assets/header.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="RxInfer.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="RxInfer.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">RxInfer.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">User guide</span><ul><li><a class="tocitem" href="../../manuals/getting-started/">Getting started</a></li><li><a class="tocitem" href="../../manuals/model-specification/">Model specification</a></li><li><a class="tocitem" href="../../manuals/constraints-specification/">Constraints specification</a></li><li><a class="tocitem" href="../../manuals/meta-specification/">Meta specification</a></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox"/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">Inference specification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../manuals/inference/overview/">Overview</a></li><li><a class="tocitem" href="../../manuals/inference/inference/">Static dataset</a></li><li><a class="tocitem" href="../../manuals/inference/rxinference/">Real-time dataset / reactive inference</a></li><li><a class="tocitem" href="../../manuals/inference/postprocess/">Inference results postprocessing</a></li><li><a class="tocitem" href="../../manuals/inference/manual/">Manual inference specification</a></li></ul></li></ul></li><li><span class="tocitem">Library</span><ul><li><a class="tocitem" href="../../library/functional-forms/">Built-in functional form constraints</a></li><li><a class="tocitem" href="../../library/model-specification/">Model specification</a></li><li><a class="tocitem" href="../../library/bethe-free-energy/">Bethe Free Energy</a></li><li><a class="tocitem" href="../../library/exported-methods/">Exported methods</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../Coin Toss Model/">Coin toss model (Beta-Bernoulli)</a></li><li class="is-active"><a class="tocitem" href>Bayesian Linear Regression</a><ul class="internal"><li><a class="tocitem" href="#Univariate-regression-with-known-noise"><span>Univariate regression with known noise</span></a></li><li><a class="tocitem" href="#Univariate-regression-with-unknown-noise"><span>Univariate regression with unknown noise</span></a></li><li><a class="tocitem" href="#Multivariate-linear-regression"><span>Multivariate linear regression</span></a></li></ul></li><li><a class="tocitem" href="../Active Inference Mountain car/">Active Inference Mountain car</a></li><li><a class="tocitem" href="../Assessing People Skills/">Assessing People’s Skills</a></li><li><a class="tocitem" href="../Gaussian Linear Dynamical System/">Gaussian Linear Dynamical System</a></li><li><a class="tocitem" href="../Hidden Markov Model/">Ensemble Learning of a Hidden Markov Model</a></li><li><a class="tocitem" href="../Autoregressive Model/">Autoregressive Model</a></li><li><a class="tocitem" href="../Hierarchical Gaussian Filter/">Hierarchical Gaussian Filter</a></li><li><a class="tocitem" href="../Bayesian ARMA/">Bayesian ARMA model</a></li><li><a class="tocitem" href="../Infinite Data Stream/">Infinite Data Stream</a></li><li><a class="tocitem" href="../Identification Problem/">System Identification Problem</a></li><li><a class="tocitem" href="../Gaussian Mixture Univariate/">Univariate Gaussian Mixture Model</a></li><li><a class="tocitem" href="../Gaussian Mixtures Multivariate/">Multivariate Gaussian Mixture Model</a></li><li><a class="tocitem" href="../Gamma Mixture/">Gamma Mixture Model</a></li><li><a class="tocitem" href="../Universal Mixtures/">Universal Mixtures</a></li><li><a class="tocitem" href="../Global Parameter Optimisation/">Global Parameter Optimisation</a></li><li><a class="tocitem" href="../Invertible Neural Network Tutorial/">Invertible neural networks: a tutorial</a></li><li><a class="tocitem" href="../Conjugate-Computational Variational Message Passing/">Conjugate-Computational Variational Message Passing (CVI)</a></li><li><a class="tocitem" href="../GPRegression by SSM/">Solve GP regression by SDE</a></li><li><a class="tocitem" href="../Nonlinear Noisy Pendulum/">Nonlinear Smoothing: Noisy Pendulum</a></li><li><a class="tocitem" href="../Nonlinear Rabbit Population/">Nonlinear Smoothing: Rabbit Population</a></li><li><a class="tocitem" href="../Nonlinear Virus Spread/">Nonlinear Virus Spread</a></li><li><a class="tocitem" href="../Nonlinear Sensor Fusion/">Nonlinear Sensor Fusion</a></li><li><a class="tocitem" href="../Kalman filter with LSTM network driven dynamic/">Kalman filter with LSTM network driven dynamic</a></li><li><a class="tocitem" href="../Handling Missing Data/">Handling Missing Data</a></li><li><a class="tocitem" href="../Custom nonlinear node/">Custom Nonlinear Node</a></li><li><a class="tocitem" href="../Probit Model (EP)/">Probit Model (EP)</a></li><li><a class="tocitem" href="../RTS vs BIFM Smoothing/">RTS vs BIFM Smoothing</a></li><li><a class="tocitem" href="../Advanced Tutorial/">Advanced Tutorial</a></li></ul></li><li><span class="tocitem">Contributing</span><ul><li><a class="tocitem" href="../../contributing/overview/">Overview</a></li><li><a class="tocitem" href="../../contributing/new-example/">Adding a new example</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Bayesian Linear Regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Bayesian Linear Regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/biaslab/RxInfer.jl/blob/main/docs/src/examples/Linear Regression.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><p>This example has been auto-generated from the <a href="https://github.com/biaslab/RxInfer.jl/tree/main/examples"><code>examples/</code></a> folder at GitHub repository.</p><h1 id="examples-bayesian-linear-regression"><a class="docs-heading-anchor" href="#examples-bayesian-linear-regression">Bayesian Linear Regression</a><a id="examples-bayesian-linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#examples-bayesian-linear-regression" title="Permalink"></a></h1><pre><code class="language-julia hljs"># Activate local environment, see `Project.toml`
import Pkg; Pkg.activate(&quot;.&quot;); Pkg.instantiate();</code></pre><pre><code class="language-julia hljs">using RxInfer, Random, Plots, StableRNGs, BenchmarkTools, LinearAlgebra, StatsPlots</code></pre><h2 id="Univariate-regression-with-known-noise"><a class="docs-heading-anchor" href="#Univariate-regression-with-known-noise">Univariate regression with known noise</a><a id="Univariate-regression-with-known-noise-1"></a><a class="docs-heading-anchor-permalink" href="#Univariate-regression-with-known-noise" title="Permalink"></a></h2><p>In this example, we are going to perform a simple linear regression problem, but in the Bayesian setting. We specify the model&#39;s likelihood as:</p><p class="math-container">\[\begin{aligned}
p(y_i) = \mathcal{N}(y_i | a * x_i + b , 1.0)
\end{aligned}\]</p><p>where <span>$x_i$</span> and <span>$y_i$</span> are observed values and <span>$a$</span> and <span>$b$</span> are random variables with the following priors:</p><p class="math-container">\[\begin{aligned}
    p(a) &amp;= \mathcal{N}(a|m_a, v_a) \\
    p(b) &amp;= \mathcal{N}(b|m_b, v_b) 
\end{aligned}\]</p><pre><code class="language-julia hljs">@model function linear_regression(n)
    a ~ NormalMeanVariance(0.0, 1.0)
    b ~ NormalMeanVariance(0.0, 100.0)
    
    x = datavar(Float64, n)
    y = datavar(Float64, n)
    
    for i in 1:n
        y[i] ~ NormalMeanVariance(a * x[i] + b, 1.0)
    end
end</code></pre><p>In order to test our inference procedure we create a test dataset where observations are corrupted with noise. During the inference procedure we, however, do not know the exact magnitude of the noise.</p><pre><code class="language-julia hljs">reala = 0.5
realb = 25.0
realv = 1.0

N = 250

rng = StableRNG(1234)

xorig = collect(1:N)

xdata = xorig .+ randn(rng, N)
ydata = rand.(NormalMeanVariance.(realb .+ reala .* xorig, realv))

scatter(xdata, ydata, title = &quot;Linear regression dataset&quot;, legend=false)</code></pre><p><img src="../../assets/examples/Linear Regression_4_1.png" alt/></p><p>In order to run inference with the static dataset we use the <code>inference</code> function from <code>RxInfer</code> package.</p><pre><code class="language-julia hljs">results = inference(
    model = linear_regression(length(xdata)), 
    data  = (y = ydata, x = xdata), 
    initmessages = (b = NormalMeanVariance(0.0, 100.0), ), 
    returnvars   = (a = KeepLast(), b = KeepLast()), 
    iterations = 20,
);</code></pre><p>After the inference has been completed it is interesting to compare prior distribution and posterior distribution against the real values:</p><pre><code class="language-julia hljs">pra = plot(range(-3, 3, length = 1000), (x) -&gt; pdf(NormalMeanVariance(0.0, 1.0), x), title=&quot;Prior for a parameter&quot;, fillalpha=0.3, fillrange = 0, label=&quot;Prior P(a)&quot;, c=1,)
pra = vline!(pra, [ reala ], label=&quot;Real a&quot;, c = 3)
psa = plot(range(0.45, 0.55, length = 1000), (x) -&gt; pdf(results.posteriors[:a], x), title=&quot;Posterior for a parameter&quot;, fillalpha=0.3, fillrange = 0, label=&quot;Posterior P(a)&quot;, c=2,)
psa = vline!(psa, [ reala ], label=&quot;Real a&quot;, c = 3)

plot(pra, psa, size = (1000, 200))</code></pre><p><img src="../../assets/examples/Linear Regression_6_1.png" alt/></p><pre><code class="language-julia hljs">prb = plot(range(-40, 40, length = 1000), (x) -&gt; pdf(NormalMeanVariance(0.0, 100.0), x), title=&quot;Prior for b parameter&quot;, fillalpha=0.3, fillrange = 0, label=&quot;Prior P(b)&quot;, c=1, legend = :topleft)
prb = vline!(prb, [ realb ], label=&quot;Real b&quot;, c = 3)
psb = plot(range(23, 28, length = 1000), (x) -&gt; pdf(results.posteriors[:b], x), title=&quot;Posterior for b parameter&quot;, fillalpha=0.3, fillrange = 0, label=&quot;Posterior P(b)&quot;, c=2, legend = :topleft)
psb = vline!(psb, [ realb ], label=&quot;Real b&quot;, c = 3)

plot(prb, psb, size = (1000, 200))</code></pre><p><img src="../../assets/examples/Linear Regression_7_1.png" alt/></p><pre><code class="language-julia hljs">a = results.posteriors[:a]
b = results.posteriors[:b]

println(&quot;Real a: &quot;, reala, &quot; | Estimated a: &quot;, mean_var(a), &quot; | Error: &quot;, abs(mean(a) - reala))
println(&quot;Real b: &quot;, realb, &quot; | Estimated b: &quot;, mean_var(b), &quot; | Error: &quot;, abs(mean(b) - realb))</code></pre><pre><code class="nohighlight hljs">Real a: 0.5 | Estimated a: (0.500516360224855, 1.9152402435219974e-7) | Err
or: 0.000516360224855017
Real b: 25.0 | Estimated b: (24.939602693014116, 0.004015967531215887) | Er
ror: 0.06039730698588386</code></pre><p>We can see that ReactiveMP.jl estimated real values of linear regression coefficients with high precision. </p><h2 id="Univariate-regression-with-unknown-noise"><a class="docs-heading-anchor" href="#Univariate-regression-with-unknown-noise">Univariate regression with unknown noise</a><a id="Univariate-regression-with-unknown-noise-1"></a><a class="docs-heading-anchor-permalink" href="#Univariate-regression-with-unknown-noise" title="Permalink"></a></h2><p>For this demo we are going to increase the amount of noise in the dataset, but also instead of using a fixed value for the noise in the model we are going to make it a random variable with its own prior:</p><p class="math-container">\[\begin{aligned}
p(s) = \mathcal{IG}(s|\alpha, \theta)
\end{aligned}\]</p><pre><code class="language-julia hljs">@model function linear_regression_unknown_noise(n)
    a ~ NormalMeanVariance(0.0, 1.0)
    b ~ NormalMeanVariance(0.0, 100.0)
    s ~ InverseGamma(1.0, 1.0)
    
    x = datavar(Float64, n)
    y = datavar(Float64, n)
    
    for i in 1:n
        y[i] ~ NormalMeanVariance(a * x[i] + b, s)
    end
end</code></pre><pre><code class="language-julia hljs">reala_un = 0.5
realb_un = 25.0
realv_un = 200.0

N_un = 250

rng_un = StableRNG(1234)

xorig_un = collect(1:N)

xdata_un = xorig_un .+ randn(rng_un, N_un)
ydata_un = rand.(NormalMeanVariance.(realb_un .+ reala_un .* xorig_un, realv_un))

scatter(xdata_un, ydata_un, title = &quot;Linear regression dataset with more noise&quot;, legend=false)</code></pre><p><img src="../../assets/examples/Linear Regression_10_1.png" alt/></p><p>To solve this problem analytically we need to set <code>constraints = MeanField()</code> as well as provide initial marginals with the <code>initmarginals</code> argument. We are also going to evaluate the convergency performance of the algorithm with the <code>free_energy = true</code> option:</p><pre><code class="language-julia hljs">results_unknown_noise = inference(
    model = linear_regression_unknown_noise(length(xdata_un)), 
    data  = (y = ydata_un, x = xdata_un), 
    initmessages = (b = NormalMeanVariance(0.0, 100.0), ), 
    returnvars   = (a = KeepLast(), b = KeepLast(), s = KeepLast()), 
    iterations = 20,
    constraints = MeanField(),
    initmarginals = (s = vague(InverseGamma), ),
    free_energy = true
);</code></pre><pre><code class="language-julia hljs">pra = plot(range(-3, 3, length = 1000), (x) -&gt; pdf(NormalMeanVariance(0.0, 1.0), x), title=&quot;Prior for a parameter&quot;, fillalpha=0.3, fillrange = 0, label=&quot;Prior P(a)&quot;, c=1,)
pra = vline!(pra, [ reala_un ], label=&quot;Real a&quot;, c = 3)
psa = plot(range(0.45, 0.55, length = 1000), (x) -&gt; pdf(results_unknown_noise.posteriors[:a], x), title=&quot;Posterior for a parameter&quot;, fillalpha=0.3, fillrange = 0, label=&quot;Posterior P(a)&quot;, c=2,)
psa = vline!(psa, [ reala_un ], label=&quot;Real a&quot;, c = 3)

plot(pra, psa, size = (1000, 200))</code></pre><p><img src="../../assets/examples/Linear Regression_12_1.png" alt/></p><pre><code class="language-julia hljs">prb = plot(range(-40, 40, length = 1000), (x) -&gt; pdf(NormalMeanVariance(0.0, 100.0), x), title=&quot;Prior for b parameter&quot;, fillalpha=0.3, fillrange = 0, label=&quot;Prior P(b)&quot;, c=1, legend = :topleft)
prb = vline!(prb, [ realb_un ], label=&quot;Real b&quot;, c = 3)
psb = plot(range(23, 28, length = 1000), (x) -&gt; pdf(results_unknown_noise.posteriors[:b], x), title=&quot;Posterior for b parameter&quot;, fillalpha=0.3, fillrange = 0, label=&quot;Posterior P(b)&quot;, c=2, legend = :topleft)
psb = vline!(psb, [ realb_un ], label=&quot;Real b&quot;, c = 3)

plot(prb, psb, size = (1000, 200))</code></pre><p><img src="../../assets/examples/Linear Regression_13_1.png" alt/></p><pre><code class="language-julia hljs">prb = plot(range(0.001, 400, length = 1000), (x) -&gt; pdf(InverseGamma(1.0, 1.0), x), title=&quot;Prior for s parameter&quot;, fillalpha=0.3, fillrange = 0, label=&quot;Prior P(s)&quot;, c=1, legend = :topleft)
prb = vline!(prb, [ realv_un ], label=&quot;Real s&quot;, c = 3)
psb = plot(range(0.001, 400, length = 1000), (x) -&gt; pdf(results_unknown_noise.posteriors[:s], x), title=&quot;Posterior for s parameter&quot;, fillalpha=0.3, fillrange = 0, label=&quot;Posterior P(b)&quot;, c=2, legend = :topleft)
psb = vline!(psb, [ realv_un ], label=&quot;Real s&quot;, c = 3)

plot(prb, psb, size = (1000, 200))</code></pre><p><img src="../../assets/examples/Linear Regression_14_1.png" alt/></p><p>We can see that in the presence of more noise the inference result is more uncertain about the actual values for <span>$a$</span> and <span>$b$</span> parameters.</p><p>Lets sample <span>$a$</span> and <span>$b$</span> and plot many regression lines on the same plot:</p><pre><code class="language-julia hljs">as = rand(results_unknown_noise.posteriors[:a], 100)
bs = rand(results_unknown_noise.posteriors[:b], 100)

p = scatter(xdata_un, ydata_un, title = &quot;Linear regression dataset with more noise&quot;, legend=false)

for (a, b) in zip(as, bs)
    global p = plot!(p, xdata_un, a .* xdata_un .+ b, alpha = 0.05, color = :red)
end

f = plot(results_unknown_noise.free_energy, title = &quot;Bethe Free Energy convergence&quot;, label = nothing)

plot(p, f, size = (1000, 400))</code></pre><p><img src="../../assets/examples/Linear Regression_15_1.png" alt/></p><p>From this plot we can see that many lines do fit the data well and there is no definite &quot;best&quot; answer to the regression coefficients. Most of these lines, however, resemble a similar angle and shift. Bethe Free Energy plot on the right hand side indicates that the inference procedure converged normally.</p><h2 id="Multivariate-linear-regression"><a class="docs-heading-anchor" href="#Multivariate-linear-regression">Multivariate linear regression</a><a id="Multivariate-linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Multivariate-linear-regression" title="Permalink"></a></h2><p>In this example we are essentially going to perform many linear regression tasks at once, using multiple <code>x</code> data vector and multiple <code>y</code> outputs with different noises. As in the previous example we assume noise to be unknown as well:</p><pre><code class="language-julia hljs">@model function linear_regression_multivariate(m, n)
    a ~ MvNormalMeanCovariance(zeros(m), 100 * diageye(m))
    b ~ MvNormalMeanCovariance(ones(m), 100 * diageye(m))
    W ~ InverseWishart(m + 2, 100 * diageye(m))

    # Here is a small trick to make the example work
    # We treat the `x` vector as a Diagonal matrix such that we can multiply it with `a`
    x = datavar(Diagonal{Float64, Vector{Float64}}, n)
    y = datavar(Vector{Float64}, n)
    z = randomvar(n)

    z .~ x .* a .+ b
    y .~ MvNormalMeanCovariance(z, W)

end</code></pre><p>Lets have the following dataset, where multiple linear regression intersect with each other:</p><pre><code class="language-julia hljs">
N_mv = 50
s_mv = floor(N_mv / 10)
d_mv = 6

rng_mv = StableRNG(42)

a_mv = randn(rng_mv, d_mv)
b_mv = 10 * randn(rng_mv, d_mv)
v_mv = 100 * rand(rng_mv, d_mv)

x_mv = []
y_mv = []

p = plot(title = &quot;Multiple linear regressions&quot;, legend = :topleft)

plt = palette(:tab10)

for k in 1:d_mv
    x_mv_k = collect((1 + s_mv * (k - 1)):(N_mv + s_mv * (k - 1))) .+ 10 * randn()
    y_mv_k = rand.(NormalMeanVariance.(a_mv[k] .* x_mv_k .+ b_mv[k], v_mv[k]))

    global p = scatter!(p, x_mv_k, y_mv_k, label = &quot;Dataset #$k&quot;, ms = 2, color = plt[k])

    push!(x_mv, x_mv_k)
    push!(y_mv, y_mv_k)
end

p</code></pre><p><img src="../../assets/examples/Linear Regression_17_1.png" alt/></p><pre><code class="language-julia hljs">xdata_mv = map(i -&gt; Diagonal(getindex.(x_mv, i)), 1:N_mv)
ydata_mv = map(i -&gt; getindex.(y_mv, i), 1:N_mv);</code></pre><pre><code class="language-julia hljs">results_mv = inference(
    model = linear_regression_multivariate(d_mv, N_mv),
    data  = (y = ydata_mv, x = xdata_mv),
    initmarginals = (W = InverseWishart(d_mv + 2, 10 * diageye(d_mv)), ),
    initmessages = (b = MvNormalMeanCovariance(ones(d_mv), 10 * diageye(d_mv)), ),
    returnvars   = (a = KeepLast(), b = KeepLast(), W = KeepLast()),
    free_energy = true,
    iterations   = 50,
    constraints = MeanField()
)</code></pre><pre><code class="nohighlight hljs">Inference results:
  Posteriors       | available for (a, b, W)
  Free Energy:     | Real[939.239, 1080.99, 1071.86, 1063.92, 1058.2, 1053.
21, 1048.82, 1045.04, 1041.86, 1039.22  …  1029.4, 1029.4, 1029.4, 1029.4, 
1029.39, 1029.39, 1029.39, 1029.39, 1029.39, 1029.39]</code></pre><pre><code class="language-julia hljs">p = plot(title = &quot;Multivariate linear regression inference results&quot;, legend = :topleft)

# how many lines to plot
r = 50

i_a = collect.(eachcol(rand(results_mv.posteriors[:a], r)))
i_b = collect.(eachcol(rand(results_mv.posteriors[:b], r)))

plt = palette(:tab10)

for k in 1:d_mv
    x_mv_k = x_mv[k]
    y_mv_k = y_mv[k]

    for i in 1:r
        global p = plot!(p, x_mv_k, x_mv_k .* i_a[i][k] .+ i_b[i][k], label = nothing, alpha = 0.05, color = plt[k])
    end

    global p = scatter!(p, x_mv_k, y_mv_k, label = &quot;Dataset #$k&quot;, ms = 2, color = plt[k])
end

# truncate the init step
f = plot(results_mv.free_energy[2:end], title =&quot;Bethe Free Energy convergence&quot;, label = nothing) 

plot(p, f, size = (1000, 400))</code></pre><p><img src="../../assets/examples/Linear Regression_20_1.png" alt/></p><p>We needed more iterations to converge, but that is expected since the problem became multivariate and, hence, more difficult.</p><pre><code class="language-julia hljs">i_a_mv = results_mv.posteriors[:a]

ps_a = []

for k in 1:d_mv
    
    local _p = plot(title = &quot;Estimated a[$k]&quot;)
    local m_a_mv_k = mean(i_a_mv)[k]
    local v_a_mv_k = std(i_a_mv)[k, k]
    
    _p = plot!(_p, Normal(m_a_mv_k, v_a_mv_k), fillalpha=0.3, fillrange = 0, label=&quot;Posterior P(a[$k])&quot;, c=2,)
    _p = vline!(_p, [ a_mv[k] ], label=&quot;Real a[$k]&quot;, c = 3)
           
    push!(ps_a, _p)
end

plot(ps_a...)</code></pre><p><img src="../../assets/examples/Linear Regression_21_1.png" alt/></p><pre><code class="language-julia hljs">i_b_mv = results_mv.posteriors[:b]

ps_b = []

for k in 1:d_mv
    
    local _p = plot(title = &quot;Estimated b[$k]&quot;)
    local m_b_mv_k = mean(i_b_mv)[k]
    local v_b_mv_k = std(i_b_mv)[k, k]

    _p = plot!(_p, Normal(m_b_mv_k, v_b_mv_k), fillalpha=0.3, fillrange = 0, label=&quot;Posterior P(b[$k])&quot;, c=2,)
    _p = vline!(_p, [ b_mv[k] ], label=&quot;Real b[$k]&quot;, c = 3)
           
    push!(ps_b, _p)
end

plot(ps_b...)</code></pre><p><img src="../../assets/examples/Linear Regression_22_1.png" alt/></p><p>As we can also check the noise estimation procedure found the noise components with high precision:</p><pre><code class="language-julia hljs">v_mv</code></pre><pre><code class="nohighlight hljs">6-element Vector{Float64}:
 63.75837562301017
 11.035887151354174
 59.851445480565424
 32.21105133173212
 16.228513601930295
 64.33685320974074</code></pre><pre><code class="language-julia hljs">diag(mean(results_mv.posteriors[:W]))</code></pre><pre><code class="nohighlight hljs">6-element Vector{Float64}:
 46.64761070497661
 12.583702255465932
 69.16236187045124
 51.55154620526491
 20.786598854905954
 72.43736861496203</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Coin Toss Model/">« Coin toss model (Beta-Bernoulli)</a><a class="docs-footer-nextpage" href="../Active Inference Mountain car/">Active Inference Mountain car »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 21 December 2022 12:33">Wednesday 21 December 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
